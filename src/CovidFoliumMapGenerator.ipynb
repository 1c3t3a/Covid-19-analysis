{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0p31pajYJ8D"
   },
   "source": [
    "# Utility functions\n",
    "\n",
    "Some functions required by this and other modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1MFeKfTtYJ8H"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def ensure_path_exists(thePath):\n",
    "    \"\"\" The function checks of the given relative or absolute path exists and if not it will try to create it. If that fails\n",
    "    the function will throw an exception\n",
    "\n",
    "    Args:\n",
    "        thePath (string): the given relative or absolute path\n",
    "\n",
    "    Returns:\n",
    "        string: The absolute path that will exists or an empty string if it failed to create it\n",
    "    \"\"\"\n",
    "    # if the data directory is given as an absolute path it's all fine\n",
    "    if Path(thePath).is_absolute():\n",
    "        result = thePath\n",
    "    else:\n",
    "        # get path to the directory of this file\n",
    "        try:\n",
    "            # check if it is running in jupyter, it will throw if not running in jupyter\n",
    "            get_ipython\n",
    "            # the absolute directory of this python file\n",
    "            currentDirectory = os.path.dirname(os.path.abspath(os.path.abspath('')))\n",
    "        except:\n",
    "            # the absolute directory of this python file\n",
    "            currentDirectory = os.path.dirname(os.path.abspath(__file__))\n",
    "        # the directory is not given as an absolute path so add it to the current directory\n",
    "        result = currentDirectory + '/' + thePath\n",
    "    if not os.path.exists(result):\n",
    "        try:\n",
    "            os.makedirs(result)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return ''\n",
    "    return result\n",
    "    \n",
    "def download_JSON_file(endpoint, filename):\n",
    "    \"\"\" The function downloads a JSON file from the given endpoint and stores it in a file \n",
    "    of the given filename. If the directory doesn't exist it will be created. The function \n",
    "    throws an exception in case of an error\n",
    "\n",
    "    Args:\n",
    "        endpoint (string): the full endpoint that is referring to a JSON file\n",
    "        filename (string): the full filename of the file to be created\n",
    "\n",
    "    Raises:\n",
    "        IOError: In case it can't save the data\n",
    "    \"\"\"\n",
    "    # contact the server\n",
    "    res = requests.get(endpoint)\n",
    "    # check if there was a response\n",
    "    if res.ok:\n",
    "        # get the json\n",
    "        res = res.json()\n",
    "    else:\n",
    "        # raise an exception\n",
    "        res.raise_for_status()\n",
    "    try:\n",
    "        # create the directory if it doesn't exist \n",
    "        path = os.path.dirname(filename)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(filename)\n",
    "        # write it to the file\n",
    "        with open (filename, 'w', encoding='utf-8') as f:\n",
    "            # use dumps as we don't care about formatting\n",
    "            f.write(json.dumps(res) + \"\\n\")\n",
    "    except:\n",
    "        msg = 'Error writing file ' + filename\n",
    "        raise IOError(msg)      \n",
    "\n",
    "def this_or_last_weekday(the_date, the_weekday):\n",
    "    \"\"\" Retruns the given date of the last weekday or the given date if that has the right weekday.\n",
    "        Example: the_date = 2022.01.19 that was a Wednesday (weekday=2), \n",
    "                 if being called with the_weekday=4 (Friday) the function will return 2022.01.14\n",
    "                 if being called with the_weekday=2 (Wednesday) the function will return 2022.01.19\n",
    "\n",
    "    Args:\n",
    "        the_date (Date): the date to be checked\n",
    "        the_weekday (int): the day of the week to get the date for ranging from 0 (Monday) to 6 (Sunday)\n",
    "\n",
    "    Returns:\n",
    "        DateTime: The date of the weekday a week ago or at the given date if it is already the proper weekday\n",
    "    \"\"\"\n",
    "    # maybe it is the_date that is the right weekday\n",
    "    if the_date.weekday() == the_weekday:\n",
    "        return the_date\n",
    "    # 9:00 on that date\n",
    "    the_time = datetime.datetime(the_date.year, the_date.month, the_date.day, 9, 0)\n",
    "    # get the same day one week ago at 9:00\n",
    "    last_weekday = (the_time.date() -\n",
    "                    datetime.timedelta(days=the_time.weekday()) +\n",
    "                    datetime.timedelta(days=the_weekday, weeks=-1))\n",
    "    last_weekday_at_9 = datetime.datetime.combine(last_weekday, datetime.time(9))\n",
    "\n",
    "    # if today is also the_weekday but after 9:00 change to the current date\n",
    "    one_week = datetime.timedelta(weeks=1)\n",
    "    if the_time - last_weekday_at_9 >= one_week:\n",
    "        last_weekday_at_9 += one_week\n",
    "    return last_weekday_at_9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zJqa4_YYJ8K"
   },
   "source": [
    "# The GeoInformationWorld class\n",
    "\n",
    "A class to handle ISO 3166 country codes and names including basic inormation about the population of the countries of the world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "umrZiEtaYJ8K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "class GeoInformationWorld():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"The constructor loads a CSV with the geo information of the countries of the world.  \n",
    "            ATTENTION: The GeoID and alpha-2 of Nambia would be 'NA' but panadas csv reader makes a NaN out of it.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: In case it couldn't download the file\n",
    "\n",
    "        \"\"\"\n",
    "        # load the geo information for the world via GitHub\n",
    "        targetFilename = 'https://raw.githubusercontent.com/1c3t3a/Covid-19-analysis/master/data/GeoInformationWorld.csv'\n",
    "        self.__dfGeoInformationWorld = pd.read_csv(targetFilename, keep_default_na=False)\n",
    "        \n",
    "    def get_geo_information_world(self):\n",
    "        \"\"\"Return the dataframe of information of all countries such as country name, continent, population etc..\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A data frame holding the information of all countries\n",
    "        \"\"\"\n",
    "        return self.__dfGeoInformationWorld\n",
    "\n",
    "    def geo_name_from_geoid (self, geoID):\n",
    "        \"\"\"Return the name of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n",
    "        \n",
    "        Args:\n",
    "            geoID (str):  a string of a ISO-3166-alpha_2 geoid\n",
    "\n",
    "        Returns:\n",
    "            str: the country name\n",
    "        \"\"\"\n",
    "        # get the world info\n",
    "        dfInfo = self.get_geo_information_world()\n",
    "        # ISO-3166-alpha_3\n",
    "        # find the row in our internal listin the GeoID column\n",
    "        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n",
    "        # the name used in our internal list\n",
    "        return dfTheOne['GeoName'].values[0]\n",
    "        \n",
    "    def geo_name_from_ISO3166_alpha_3 (self, geoID):\n",
    "        \"\"\"Return the name of a country of the internal geo information from a given ISO-3166-alpha_3 geoid.\n",
    "\n",
    "        Args:\n",
    "            geoID (str):  a string of a ISO-3166-alpha_3 geoid\n",
    "\n",
    "        Returns:\n",
    "            str: the country name\n",
    "        \"\"\"\n",
    "        # get the world info\n",
    "        dfInfo = self.get_geo_information_world()\n",
    "        # find the row in our internal listin the GeoID column\n",
    "        dfTheOne = dfInfo.loc[dfInfo['ISO-3166-alpha_3'] == geoID]\n",
    "        if dfTheOne.empty:\n",
    "            # print the geoid that is not in the database\n",
    "            #print('Unknown GeoId: ' + geoID)\n",
    "            return 'Unknown'\n",
    "        # the name used in our internal list\n",
    "        return dfTheOne['GeoName'].values[0]\n",
    "\n",
    "    def geoID_from_ISO3166_alpha_3 (self, geoID):\n",
    "        \"\"\"Return the name of a country of the internal geo information from a given ISO-3166-alpha_3 geoid.\n",
    "\n",
    "        Args:\n",
    "            geoID (str):  a string of a ISO-3166-alpha_3 geoid\n",
    "\n",
    "        Returns:\n",
    "            str: ISO-3166-alpha_2 geoid\n",
    "        \"\"\"\n",
    "        # get the world info\n",
    "        dfInfo = self.get_geo_information_world()\n",
    "        # find the row in our internal listin the GeoID column\n",
    "        dfTheOne = dfInfo.loc[dfInfo['ISO-3166-alpha_3'] == geoID]\n",
    "        # check if it is empty\n",
    "        if dfTheOne.empty:\n",
    "            # print the geoid that is not in the database\n",
    "            #print('Unknown GeoId: ' + geoID)\n",
    "            return 'Unknown'\n",
    "        # the name used in our internal list\n",
    "        return dfTheOne['GeoID'].values[0]\n",
    "\n",
    "    def ISO3166_alpha_3_from_geoID (self, geoID):\n",
    "        \"\"\"Return the ISO-3166-alpha_2 geoid of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n",
    "\n",
    "        Args:\n",
    "            geoID (str):  a string of a ISO-3166-alpha_2 geoid\n",
    "\n",
    "        Returns:\n",
    "            str: the ISO-3166-alpha_3 geoid\n",
    "        \"\"\"\n",
    "        # get the world info\n",
    "        dfInfo = self.get_geo_information_world()\n",
    "        # find the row in our internal list in the GeoID column\n",
    "        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n",
    "        # the name used in our internal list\n",
    "        return dfTheOne['ISO-3166-alpha_3'].values[0]\n",
    "\n",
    "    def population_from_geoid(self, geoID):\n",
    "        \"\"\"Return the population of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n",
    "\n",
    "        Args:\n",
    "            geoID (str):  a string of a ISO 3166 alpha_2 geoid\n",
    "\n",
    "        Returns:\n",
    "            int: the population of the country\n",
    "        \"\"\"\n",
    "        # get the world info\n",
    "        dfInfo = self.get_geo_information_world()\n",
    "         # find the row in our internal list in the GeoID column\n",
    "        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n",
    "        # the name used in our internal list\n",
    "        pop = int(dfTheOne['Population2019'].values[0])\n",
    "        return pop\n",
    "\n",
    "    def continent_from_geoid(self, geoID):\n",
    "        \"\"\"Return the continent of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n",
    "\n",
    "        Args:\n",
    "            geoID (str):  a string of a ISO 3166 alpha_2 geoid\n",
    "\n",
    "        Returns:\n",
    "            str: the continent of the country\n",
    "        \"\"\"\n",
    "        # get the world info\n",
    "        dfInfo = self.get_geo_information_world()\n",
    "         # find the row in our internal list in the GeoID column\n",
    "        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n",
    "        # the name used in our internal list\n",
    "        return dfTheOne['Continent'].values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlx1lJ-XZO44"
   },
   "source": [
    "# The CovidCases class and its subclasses\n",
    "\n",
    "This abstract base class will expose data attributes in form of a DataFrame. It also provides methods to process the data which will end up in additional columns in the DataFrame. Please refer to [http://mb.cmbt.de/python-class-documentation/the-covidcases-class52/](http://mb.cmbt.de/python-class-documentation/the-covidcases-class52/) for a complete documentation of the class.  \n",
    "\n",
    "So far there are three sub-classes handling three different data sources: \n",
    "  \n",
    "```CovidCasesWHO```  \n",
    "the data is provided by the [WHO website](https://covid19.who.int/WHO-COVID-19-global-data.csv).   \n",
    "\n",
    "```CovidCasesOWID```  \n",
    "gets data from [Our World In Data](https://covid.ourworldindata.org/data/owid-covid-data.csv). The quality of this data, especially the fact that the data is not for all countries generated by official agencies is somehow a drawback. On the other side OWID generates much more data such as vaccination numbers.  \n",
    "\n",
    "```CovidCasesECDC```  \n",
    "handles the data provided by the [European Center of Disease Control](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) until December, 14th. 2020.  \n",
    "\n",
    "The documentation for this classes is available on [http://mb.cmbt.de/python-class-documentation/the-covidcases-world-sub-classes52/](http://mb.cmbt.de/python-class-documentation/the-covidcases-world-sub-classes52/). \n",
    "Please note: The ECDC subclass is a legacy class and can't be used anymore as the ECDC is not publishing daily updated data since December 2020.  \n",
    "\n",
    "The [blog post in this link](http://mb.cmbt.de/covid-19-analysis/data-source-comparison/) compares the quality of the different datasets.\n",
    "\n",
    "**ATTENTION**  \n",
    "These classes have been modified compared to the files on GitHub to be able\n",
    "to be executed in colab and they store the data in **/content/data/**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b-IFuGaRZR01"
   },
   "outputs": [],
   "source": [
    "class CovidCases(ABC):\n",
    "    \"\"\"This abstract base class will expose data attributes in form of a DataFrame. It also provides methods to process \n",
    "    the data which will end up in additional columns in the DataFrame.  \n",
    "    These are the names of seven columns that have to be generated by ALL subclasses.\n",
    "\n",
    "    Date\n",
    "    The date of the data \n",
    "    \n",
    "    GeoID\n",
    "    The ISO-3166-alpha_3 GeoID of the area such as 'FR' for France or 'DE' for Germany\n",
    "\n",
    "    GeoName\n",
    "    The name of the area such as 'England' or 'Italy'\n",
    "\n",
    "    Population\n",
    "    The population of the country\n",
    "\n",
    "    Continent\n",
    "    E.g. The continent of the country. But it also may be grouping value for e.g. the states of a federal republic such as Bavaria\n",
    "    \n",
    "    DailyCases\n",
    "    The number of new cases on a given day\n",
    "\n",
    "    DailyDeaths\n",
    "    The number of new deaths on the given date\n",
    "\n",
    "    Beside these fields a subclass might also define additional columns such as 'Continent'\n",
    "    Based on the six mandatory columns the class will generate the following additional columns (attributes):\n",
    "    \n",
    "    Cases\n",
    "    The accumulated number of cases since the 31.12.2019\n",
    "\n",
    "    Deaths\n",
    "    The accumulated number of deaths since the 31.12.2019\n",
    "\n",
    "    CasesPerMillionPopulation\n",
    "    The number of cumulative cases divided by the population of the country in million\n",
    "\n",
    "    DeathsPerMillionPopulation\n",
    "    The number of cumulative deaths divided by the population of the country in million\n",
    "\n",
    "    PercentDeaths\n",
    "    The number of deaths in % of the cases. This is the Case Fatality Rate (CFR), an approximation for the\n",
    "    Infection Fatality Rate (IFR) that includes also 'hidden' cases.\n",
    "\n",
    "    Incidence7DayPer100Kpopulation\n",
    "    The accumulated 7 day incidence. That is the sum of daily cases of the last 7 days divided by the \n",
    "    population in 100000\n",
    "\n",
    "    DoublingTime\n",
    "    The number of days in which the number of cases will be doubled\n",
    "\n",
    "    R0\n",
    "    This is an estimation of the reproduction number R0. As the calculation takes some time it is \n",
    "    generated on demand by calling add_r0 method.\n",
    "\n",
    "    Beside that sub-class may add additional attributes. Please refer to the documentation of the \n",
    "    specific sub-class that you want to use.\n",
    "\n",
    "    Returns:\n",
    "        You can't create an instance of this class. Instead create an instance of a subclass\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"The constructor takes a dataframe loaded by any sub-class containing the data published by the\n",
    "        website that is handled in the sub-classes individually.  \n",
    "        To retrieve the data for an individual country you can use the public methods\n",
    "        GetCountryDataByGeoID or GetCountryDataByCountryName. These functions take ISO 3166 alpha_2 \n",
    "        (2 characters long) GeoIDs.\n",
    "\n",
    "        Args:\n",
    "            df (dataframe): The dataframe containing information about individual countries such as\n",
    "                            GeoID, CountryName, Cases and Deaths. \n",
    "        \"\"\"\n",
    "        # keep the data frame\n",
    "        self.__df = df\n",
    "        # load the geo information for the world via GitHub\n",
    "        targetFilename = 'https://raw.githubusercontent.com/1c3t3a/Covid-19-analysis/master/data/GeoInformationWorld.csv'\n",
    "        self.__dfGeoInformationWorld = pd.read_csv(targetFilename, keep_default_na=False)\n",
    " \n",
    "    @staticmethod\n",
    "    def __compute_doubling_time(dfSingleCountry):\n",
    "        \"\"\"Computes the doubling time for everyday day with the formula:\n",
    "                ln(2) / ln(Conf[n] / Conf[n - 1])\n",
    "        \n",
    "        Args:\n",
    "            dfSingleCountry (DataFrame): A dataframe holding only one country\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A data frame holding only one column to be appended to another data frame\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        quotient = []\n",
    "        for index, value in dfSingleCountry['Cases'].iteritems():\n",
    "            #  calculating the quotient conf[n] / conf[n-1]\n",
    "            if index > 0 and index - 1 != 0:\n",
    "                quotient.append(value / dfSingleCountry['Cases'][index - 1])\n",
    "            else:\n",
    "                quotient.append(math.nan)\n",
    "            # calculates the doubling time (can't be calculated when there's \n",
    "            # no change from one day to the other)\n",
    "            if quotient[index] != 1 and quotient[index] != math.nan and quotient[index] != 0:\n",
    "                result.append(math.log(2) / math.log(quotient[index]))\n",
    "            else:\n",
    "                result.append(math.nan)\n",
    "        # return the dataframe\n",
    "        return pd.DataFrame(np.asarray(result))\n",
    "\n",
    "    @staticmethod\n",
    "    def create_combined_dataframe_by_geoid_string_list(dfList, geoIDs, lastNdays=0, sinceNcases=0): \n",
    "        \"\"\"Creates a combined dataframe from a list of individual datafames. To avoid\n",
    "        duplicate country names the method will add a '-DATASOURCE' string behind the \n",
    "        country name (e.g. 'Germany-OWID'). \n",
    "\n",
    "        Args:\n",
    "            dfList (tuple of DataFrame objects): A list of data frames\n",
    "            geoIDs (str): A string of comma separated GeoIds that have to be included in all given data frames\n",
    "            lastNdays (int, optional): Get the data only for the last N days. Defaults to 0.\n",
    "            sinceNcases (int, optional): Get the data since the Nth. case has been exceeded. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            [DataFrame]: The combined data frame\n",
    "        \"\"\"         \n",
    "        # a final array of dataframes containing all three data\n",
    "        dfs = []\n",
    "        # loop through all classes / geoIDs\n",
    "        for obj in dfList:\n",
    "            # get the data frame\n",
    "            df = obj.get_data_by_geoid_string_list(geoIDs, lastNdays, sinceNcases)\n",
    "            # rename the country and add the source info to the name\n",
    "            for name in df['GeoName'].unique():\n",
    "                df.replace(name, name + '-' + obj.get_data_source_info()[1], inplace=True)\n",
    "            # add it to the list\n",
    "            dfs.append(df)  \n",
    "        # finally concatenate all dfs together\n",
    "        df = pd.concat(dfs)  \n",
    "        # ...and return it\n",
    "        return df\n",
    "\n",
    "    def __add_additional_attributes(self, dfSingleCountry):\n",
    "        \"\"\"Adds additional attributes to a dataframe of a single country.  \n",
    "\n",
    "        Args:\n",
    "            dfSingleCountry (DataFrame): A dataframe holding only one country\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The modified data frame of the country\n",
    "        \"\"\"\n",
    "        if dfSingleCountry.empty == True:\n",
    "            return\n",
    "        # reset the index on the dataframe (if the argument is just a slice)\n",
    "        dfSingleCountry.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # the cumulative cases\n",
    "        dfSingleCountry['Cases'] = dfSingleCountry['DailyCases'].cumsum()\n",
    "        # the cumulative cases\n",
    "        dfSingleCountry['Deaths'] = dfSingleCountry['DailyDeaths'].cumsum()\n",
    "        # the percentage of deaths of the cumulative cases\n",
    "        dfSingleCountry['PercentDeaths'] = pd.DataFrame({'PercentDeaths': dfSingleCountry['Deaths'] * 100 / dfSingleCountry['Cases']})\n",
    "        # the percentage of cumulative cases of the 1 million population\n",
    "        dfSingleCountry['CasesPerMillionPopulation'] = pd.DataFrame({'CasesPerMillionPopulation': dfSingleCountry['Cases'].div(dfSingleCountry['Population'].iloc[0] / 1000000)})\n",
    "        # the percantage of cumulative deaths of 1 million population\n",
    "        dfSingleCountry['DeathsPerMillionPopulation'] = pd.DataFrame({'DeathsPerMillionPopulation': dfSingleCountry['Deaths'].div(dfSingleCountry['Population'].iloc[0] / 1000000)})\n",
    "        \n",
    "        if self.get_data_source_info()[1] == 'OWID':\n",
    "            # the percantage of people that received the first vaccination dose\n",
    "            dfSingleCountry['PercentPeopleReceivedFirstDose'] = pd.DataFrame({'PercentPeopleReceivedFirstDose': dfSingleCountry['PeopleReceivedFirstDose'] * 100 / dfSingleCountry['Population'].iloc[0]})\n",
    "            # the percantage of people that are fully vaccinated\n",
    "            dfSingleCountry['PercentPeopleReceivedAllDoses'] = pd.DataFrame({'PercentPeopleReceivedAllDoses': dfSingleCountry['PeopleReceivedAllDoses'] * 100 / dfSingleCountry['Population'].iloc[0]})\n",
    "        \n",
    "        # adds the extra attributes\n",
    "        dfSingleCountry['DoublingTime'] = self.__compute_doubling_time(dfSingleCountry)\n",
    "        # now apply the country names from our internal list\n",
    "        dfInfo = self.__dfGeoInformationWorld\n",
    "        # return the manipulated dataframe\n",
    "        return dfSingleCountry\n",
    "\n",
    "    def __apply_lowpass_filter(self, dfAttribute, n):\n",
    "        \"\"\"Returns a dataframe containing the lowpass filtered (with depth n)\n",
    "        data of the given dataframe.\n",
    "\n",
    "        Args:\n",
    "            dfAttribute (DataFrame): The data frame to be filtered\n",
    "            n (int): Width of the lowpass filter\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A data frame holding only one column to be appended to another data frame\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # iterate the attribute\n",
    "        for index, value in dfAttribute.iteritems():\n",
    "            # if the dataframe contains NaN, leave it untouched\n",
    "            if math.isnan(value):\n",
    "                result.append(math.nan)\n",
    "                continue\n",
    "            if index == 0:\n",
    "                result.append(value)\n",
    "            # for all rows below the nth row, calculate the lowpass filter up to this point\n",
    "            elif index < n:\n",
    "                result.append(sum(dfAttribute[0:index + 1]) / (index + 1))\n",
    "            else:\n",
    "                start = index - n + 1\n",
    "                result.append(sum(dfAttribute[start:start + n]) / n)\n",
    "        # return the calculated data as an array\n",
    "        return pd.DataFrame(np.asarray(result))\n",
    "\n",
    "    def add_lowpass_filter_for_attribute(self, df, attribute, n):\n",
    "        \"\"\"Adds a attribute to the df of each country that is the lowpass filtered\n",
    "        data of the given attribute. The width of the lowpass is given by then\n",
    "        number n. The name of the newly created attribute is the given name\n",
    "        with a tailing number n. E.g. 'DailyCases' with n = 7 will add to a newly\n",
    "        added attribute named 'Cases7'.\n",
    "        If the attribute already exists the function will return the given df.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The data frame holding all countries and all columns\n",
    "            attribute (str): The name of the column to be processed\n",
    "            n (int): The width of the lowpass filter\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A data frame that includes the newly generated column\n",
    "        \"\"\" \n",
    "        # check if the attribute already exists\n",
    "        requestedAttribute = attribute + str(n)\n",
    "        for col in df.columns:\n",
    "            if col == requestedAttribute:\n",
    "                return df\n",
    "        # get all GeoIDs in the df\n",
    "        geoIDs = df['GeoID'].unique()\n",
    "        # our result data frame\n",
    "        dfs = []\n",
    "        for geoID in geoIDs:\n",
    "            # get the country dataframe\n",
    "            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n",
    "            # reset the index to start from index = 0\n",
    "            dfSingleCountry.reset_index(inplace=True, drop=True)\n",
    "            # add the lowpass filtered attribute\n",
    "            dfSingleCountry[requestedAttribute] = self.__apply_lowpass_filter(dfSingleCountry[attribute], 7)\n",
    "            # add the country to the result\n",
    "            dfs.append(dfSingleCountry)\n",
    "        return pd.concat(dfs)\n",
    "\n",
    "    def __apply_r0(self, dfCases):\n",
    "        \"\"\"Returns a dataframe containing an estimation for the reproduction\n",
    "        number R0 of the dataframe given. The given dataframe has to contain\n",
    "        'DailyCases'.\n",
    "\n",
    "        Args:\n",
    "            dfCases (DataFrame): The data frame to be processed\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: A data frame holding only one column to be appended to another data frame\n",
    "        \"\"\"\n",
    "        # add the r0 attribute\n",
    "        result = []\n",
    "        # we will create 2 blocks and sum the data of each block\n",
    "        blockSize = 4\n",
    "        # iterate the cases\n",
    "        for index, value in dfCases.iteritems():\n",
    "            if index < 2 * blockSize - 1:\n",
    "                result.append(math.nan)\n",
    "            else:\n",
    "                # the sum of block 0\n",
    "                start = index - (2 * blockSize - 1)\n",
    "                sum0 = sum(dfCases[start: start + blockSize])\n",
    "                # the sum of block 1\n",
    "                start = index - (blockSize - 1)\n",
    "                sum1 = sum(dfCases[start: start + blockSize])\n",
    "                # and R\n",
    "                if sum0 == 0:\n",
    "                    R = math.nan\n",
    "                else:\n",
    "                    R = sum1 / sum0\n",
    "                result.append(R)\n",
    "        # return the calculated data as an array\n",
    "        return pd.DataFrame(np.asarray(result))\n",
    "\n",
    "    def add_r0(self, df):\n",
    "        \"\"\"Adds a attribute to the df of each country that is an estimation of the\n",
    "        reproduction number R0. Here the number is called 'R'. The returned\n",
    "        dataframe should finally lowpassed filtered with a kernel size of 1x7.\n",
    "        If the attribute already exists the function will return the given df.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The data frame holding all countries and all columns\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A data frame that includes the newly generated column\n",
    "        \"\"\" \n",
    "        # check if the attribute already exists\n",
    "        requestedAttribute = 'R'\n",
    "        for col in df.columns:\n",
    "            if col == requestedAttribute:\n",
    "                return df\n",
    "        # get all GeoIDs in the df\n",
    "        geoIDs = df['GeoID'].unique()\n",
    "        # our result data frame\n",
    "        dfs = []\n",
    "        for geoID in geoIDs:\n",
    "            # get the country dataframe\n",
    "            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n",
    "            # reset the index to start from index = 0\n",
    "            dfSingleCountry.reset_index(inplace=True, drop=True)\n",
    "            # add the lowpass filtered attribute\n",
    "            dfSingleCountry[requestedAttribute] = self.__apply_r0(dfSingleCountry['DailyCases'])\n",
    "            # add the country to the result\n",
    "            dfs.append(dfSingleCountry)\n",
    "        return pd.concat(dfs)\n",
    "\n",
    "    def __apply_incidence_7day_per_100Kpopulation(self, dfAttribute, dfPopulation):\n",
    "        \"\"\"Returns a dataframe containing the accumulated 7 day incidence\n",
    "        of the given dataframe containing only one country.\n",
    "        \n",
    "        Args:\n",
    "            dfAttribute (DataFrame): The data frame holding the daily ne cases\n",
    "            dfPopulation (DataFrame): A data frame holding the population\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: A data frame holding only one column to be appended to another data frame\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # iterate the attribute\n",
    "        for index, value in dfAttribute.iteritems():\n",
    "            # for all rows below the nth row, calculate the lowpass filter up to this point\n",
    "            if index < 7:\n",
    "                daysSum7 = sum(dfAttribute[0:index + 1]) * 7 / (index + 1)\n",
    "                result.append(daysSum7  / (dfPopulation[index] / 100000))\n",
    "            else:\n",
    "                start = index - 7 + 1\n",
    "                daysSum7 = sum(dfAttribute[start:start + 7])\n",
    "                result.append(daysSum7 / (dfPopulation[index] / 100000))\n",
    "        # return the calculated data as an array\n",
    "        return pd.DataFrame(np.asarray(result))\n",
    "\n",
    "    def add_incidence_7day_per_100Kpopulation(self, df):\n",
    "        \"\"\"Adds a attribute to the df of each country that is representing the\n",
    "        accumulated 7-day incidence. That is the sum of the daily cases of \n",
    "        the last 7 days divided by the population in 100000 people.\n",
    "        If the attribute already exists the function will return the given df.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The data frame holding all countries and all columns\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A data frame that includes the newly generated column\n",
    "        \"\"\" \n",
    "        # check if the attribute exists\n",
    "        requestedAttribute = 'Incidence7DayPer100Kpopulation'\n",
    "        for col in df.columns:\n",
    "            if col == requestedAttribute:\n",
    "                return df\n",
    "        # get all GeoIDs in the df\n",
    "        geoIDs = df['GeoID'].unique()\n",
    "        # our result data frame\n",
    "        dfs = []\n",
    "        for geoID in geoIDs:\n",
    "            # get the country dataframe\n",
    "            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n",
    "            # reset the index to start from index = 0\n",
    "            dfSingleCountry.reset_index(inplace=True, drop=True)\n",
    "            # add the lowpass filtered attribute\n",
    "            dfSingleCountry[requestedAttribute] = self.__apply_incidence_7day_per_100Kpopulation(dfSingleCountry['DailyCases'], dfSingleCountry['Population'])\n",
    "            # add the country to the result\n",
    "            dfs.append(dfSingleCountry)\n",
    "        return pd.concat(dfs)\n",
    "\n",
    "    def save_df_to_csv(self, df, filename):\n",
    "        \"\"\"Saves a df to a CSV file\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The data frame holding all countries and all columns\n",
    "            filename (str): The name of the output file\n",
    "        \"\"\"       \n",
    "        df.to_csv(filename)\n",
    "\n",
    "    def get_data_by_geoid_list(self, geoIDs, lastNdays=0, sinceNcases=0):\n",
    "        \"\"\"Return the dataframe by a list of geoIDs. Refer to the CSV\n",
    "        file for a list of available GeoIDs and CountryNames.\n",
    "\n",
    "        Args:\n",
    "            geoIDs (list): A list of strings holding the GeoIds\n",
    "            lastNdays (int, optional): Get the data only for the last N days. Defaults to 0.\n",
    "            sinceNcases (int, optional): Get the data since the Nth. case has been exceeded. Defaults to 0.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: In case that both optional arguments have been used (>0) \n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A data frame holding the information of the selected countries\n",
    "        \"\"\"\n",
    "        # correct potentially incorrect lists\n",
    "        geoIDs = self.review_geoid_list(geoIDs)\n",
    "        # check if only one optional parameter is used\n",
    "        if lastNdays > 0 and sinceNcases > 0:\n",
    "            raise ValueError(\"Only one optional parameter allowed!\")\n",
    "        # our result data frame\n",
    "        dfs = []\n",
    "        # get data for each country\n",
    "        for geoID in geoIDs:\n",
    "            # get the data for a country and add the additional rows\n",
    "            df = self.__df.loc[self.__df['GeoID'] == geoID].copy()\n",
    "            # reverse the data frame to the newest date in the bottom\n",
    "            df = df.reindex(index=df.index[::-1])\n",
    "            df.head()\n",
    "            df = self.__add_additional_attributes(df)\n",
    "            # if lastNdays is specified just return these last n days\n",
    "            if lastNdays > 0:\n",
    "                df = df.tail(lastNdays)\n",
    "            # if sinceNcases is specified calculate the start index\n",
    "            if sinceNcases > 0:\n",
    "                start = -1\n",
    "                for index, val in df['Cases'].iteritems():\n",
    "                    if val >= sinceNcases:\n",
    "                        start = index\n",
    "                        break\n",
    "                # an illegal input will cause an exception\n",
    "                if start == -1:\n",
    "                    raise ValueError(\"Number of cases wasn't that high!\")\n",
    "                # copy the data\n",
    "                df = df.iloc[start:].copy()\n",
    "                # reset the index on the remaining data points so that they\n",
    "                # start at zero\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "            # append this dataframe to our result\n",
    "            dfs.append(df)\n",
    "        # return the concatenated dataframe\n",
    "        return pd.concat(dfs)\n",
    "\n",
    "    def get_data_by_geoid_string_list(self, geoIDstringList, lastNdays=0, sinceNcases=0):\n",
    "        \"\"\"Return the dataframe by a comma separated list of geoIDs. Refer to the CSV\n",
    "        file for a list of available GeoIDs and CountryNames.\n",
    "\n",
    "        Args:\n",
    "            geoIDs (str): A string of comma separated GeoIds\n",
    "            lastNdays (int, optional): Get the data only for the last N days. Defaults to 0.\n",
    "            sinceNcases (int, optional): Get the data since the Nth. case has been exceeded. Defaults to 0.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: In case that both optional arguments have been used (>0) \n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A data frame holding the information of the selected countries\n",
    "        \"\"\"\n",
    "        # split the string\n",
    "        geoIDs = re.split(r',\\s*', geoIDstringList.upper())\n",
    "        # return the concatenated dataframe\n",
    "        return self.get_data_by_geoid_list(geoIDs, lastNdays, sinceNcases)\n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"Return the dataframe of all countries in the database.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A data frame holding the information of all countries in the file\n",
    "        \"\"\"\n",
    "        # return all countries, but first add the extra columns\n",
    "        return self.get_data_by_geoid_list(self.__df['GeoID'].unique())\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_available_GeoID_list(self):\n",
    "        \"\"\"\n",
    "        Returns a dataframe having just two columns for the GeoID and region/country or whatever name.  \n",
    "        Needs to be implemented by all sub-classes derived from this.\n",
    "\n",
    "        Returns:\n",
    "            Dataframe: A dataframe having two columns: The country name and GeoID\n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_source_info(self):\n",
    "        \"\"\"\n",
    "        Returns a dataframe containing information about the data source. The dataframe holds 3 columns:\n",
    "        InfoFullName: The full name of the data source\n",
    "        InfoShortName: A shortname for the data source\n",
    "        InfoLink: The link to get the data\n",
    "\n",
    "        Returns:\n",
    "            Dataframe: A dataframe holding the information\n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def review_geoid_list(self, geoIDs):\n",
    "        \"\"\"\n",
    "        Returns a corrected version of the given geoID list to ensure that cases of mismatches like UK-GB are corrected by the sub-class.  \n",
    "        geoIDs: The list holding the geoIDs as requested such as ['DE', 'UK']\n",
    "\n",
    "        Returns:\n",
    "            list: A corrected list such as ['DE', 'GB'] that translates incorrect country codes to corrected codes \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    \n",
    "class CovidCasesWHO(CovidCases):\n",
    "    \"\"\"The class will expose data attributes in form of a DataFrame. Its base class also provides methods to process \n",
    "    the data which will end up in additional columns in the DataFrame. These are the name sof the columns\n",
    "    that are generated. Notice: The 'Continent' column is additionally and specific to this sub class.\n",
    "\n",
    "    ATTENTION: The CovidCasesWHOv1 class is a older version of this class and it will load 50% slower. Both classes\n",
    "               produce the same results\n",
    "\n",
    "    Date\n",
    "    The date of the data \n",
    "    \n",
    "    GeoID\n",
    "    The GeoID of the country such as FR for France or DE for Germany\n",
    "\n",
    "    GeoName\n",
    "    The name of the country\n",
    "\n",
    "    Continent\n",
    "    The continent of the country\n",
    "\n",
    "    Population\n",
    "    The population of the country\n",
    "\n",
    "    DailyCases\n",
    "    The number of new cases on a given day\n",
    "\n",
    "    DailyDeaths\n",
    "    The number of new deaths on the given date\n",
    "\n",
    "    Continent\n",
    "    The continent of the country as an additional column\n",
    "    \n",
    "    Returns:\n",
    "        CovidCasesWHO: A class to provide access to some data based on the WHO file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"The constructor takes a string containing the full filename of a CSV\n",
    "        database you can download from the WHO website:\n",
    "        https://covid19.who.int/WHO-COVID-19-global-data.csv\n",
    "        The database will be loaded and kept as a private member. To retrieve the\n",
    "        data for an individual country you can use the public methods\n",
    "        GetCountryDataByGeoID or GetCountryDataByCountryName. These functions take \n",
    "        ISO 3166 alpha_2 (2 characters long) GeoIDs.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The full path and name of the csv file. \n",
    "        \"\"\"\n",
    "        # some benchmarking\n",
    "        start = time.time()\n",
    "        # open the file\n",
    "        self.__df = pd.read_csv(filename, keep_default_na=False)\n",
    "        # drop some columns\n",
    "        self.__df = self.__df.drop(columns=['WHO_region',\n",
    "                                            'Cumulative_cases',\n",
    "                                            'Cumulative_deaths'])\n",
    "        # rename the columns to be more readable\n",
    "        self.__df.columns = ['Date',\n",
    "                             'GeoID',\n",
    "                             'GeoName',\n",
    "                             'DailyCases',\n",
    "                             'DailyDeaths']\n",
    "        \n",
    "        # now apply the country names from our internal list\n",
    "        giw = GeoInformationWorld()\n",
    "        # get all country info\n",
    "        dfInfo = giw.get_geo_information_world()\n",
    "        # our result data frame\n",
    "        dfs = []\n",
    "        for geoID in self.__df['GeoID'].unique():\n",
    "            # 'other' fix\n",
    "            if geoID == ' ':\n",
    "                continue\n",
    "            # 'Saba' fix\n",
    "            if geoID == 'XC':\n",
    "                continue\n",
    "            # Sint Eustatius\n",
    "            if geoID == 'XB':\n",
    "                continue\n",
    "            # American Samoa\n",
    "            if geoID == 'AS':\n",
    "                continue\n",
    "            # Korea, People's Republic\n",
    "            if geoID == 'KP':\n",
    "                continue\n",
    "            # French Guinea\n",
    "            if geoID == 'GF':\n",
    "                continue\n",
    "            # Guadeloupe\n",
    "            if geoID == 'GP':\n",
    "                continue\n",
    "            # Kiribati\n",
    "            if geoID == 'KI':\n",
    "                continue\n",
    "            # Martinique\n",
    "            if geoID == 'MQ':\n",
    "                continue\n",
    "            # Mayotte\n",
    "            if geoID == 'YT':\n",
    "                continue\n",
    "            # Micronesia \n",
    "            if geoID == 'FM':\n",
    "                continue\n",
    "            # Nauru\n",
    "            if geoID == 'NR':\n",
    "                continue\n",
    "            # Niue\n",
    "            if geoID == 'NU':\n",
    "                continue\n",
    "            # Palau\n",
    "            if geoID == 'PW':\n",
    "                continue\n",
    "            # Pitcairn Islands\n",
    "            if geoID == 'PN':\n",
    "                continue\n",
    "            # Réunion\n",
    "            if geoID == 'RE':\n",
    "                continue\n",
    "            # Saint Barthélemy\n",
    "            if geoID == 'BL':\n",
    "                continue\n",
    "            # Saint Helena\n",
    "            if geoID == 'SH':\n",
    "                continue\n",
    "            # Saint Martin\n",
    "            if geoID == 'MF':\n",
    "                continue\n",
    "            # Saint Pierre and Miquelon\n",
    "            if geoID == 'PM':\n",
    "                continue\n",
    "            # Turkmenistan\n",
    "            if geoID == 'TM':\n",
    "                continue\n",
    "            # Tokelau\n",
    "            if geoID == 'TK':\n",
    "                continue\n",
    "            # Tonga\n",
    "            if geoID == 'TO':\n",
    "                continue\n",
    "            # Tuvalu\n",
    "            if geoID == 'TV':\n",
    "                continue\n",
    "\n",
    "            # get the data for a country and add the additional rows\n",
    "            dfSingleCountry = self.__df.loc[self.__df['GeoID'] == geoID].copy()\n",
    "            # reset the index\n",
    "            dfSingleCountry.reset_index(inplace=True, drop=True)\n",
    "            dfSingleCountry.head()\n",
    "            # Bonaire workaround\n",
    "            if geoID == 'XA':\n",
    "                geoID = 'BQ'\n",
    "                dfSingleCountry['GeoID'] = [geoID for _ in range(0, len(dfSingleCountry['GeoID']))]    \n",
    "            # get the geoName for this geoID from our internal list\n",
    "            geoName = giw.geo_name_from_geoid(geoID)\n",
    "            # the current name         \n",
    "            curName = dfSingleCountry['GeoName'][0]\n",
    "            # replace it if necessary\n",
    "            if geoName != curName:\n",
    "                dfSingleCountry['GeoName'] = [geoName for _ in range(0, len(dfSingleCountry['GeoID']))]\n",
    "            # get the continent for this geoID from our internal list\n",
    "            continent = giw.continent_from_geoid(geoID)\n",
    "            # apply it to this country\n",
    "            dfSingleCountry['Continent'] = [continent for _ in range(0, len(dfSingleCountry['GeoID']))]\n",
    "            # get the population for this geoID from our internal list\n",
    "            population = giw.population_from_geoid(geoID)\n",
    "            # apply it to this country\n",
    "            dfSingleCountry['Population'] = [population for _ in range(0, len(dfSingleCountry['GeoID']))]\n",
    "            # re-order it from newest to olders (top-bottom)\n",
    "            dfSingleCountry = dfSingleCountry.reindex(index=dfSingleCountry.index[::-1])\n",
    "            # append this dataframe to our result\n",
    "            dfs.append(dfSingleCountry)\n",
    "        # keep the concatenated dataframe\n",
    "        self.__df = pd.concat(dfs)\n",
    "        # re-order the columns to be similar for all sub-classes                                   \n",
    "        self.__df = self.__df[['Date', \n",
    "                              'GeoName', \n",
    "                              'GeoID', \n",
    "                              'Population', \n",
    "                              'Continent', \n",
    "                              'DailyCases',\n",
    "                              'DailyDeaths']]\n",
    "        # change the type of the 'date' field to a pandas date\n",
    "        self.__df['Date'] = pd.to_datetime(self.__df['Date'],\n",
    "                                           format='%Y-%m-%d')\n",
    "        # some benchmarking\n",
    "        end = time.time()\n",
    "        print('Pandas loading the WHO CSV: ' + str(end - start) + 's')\n",
    "        # pass the dataframe to the base class\n",
    "        super().__init__(self.__df)\n",
    "\n",
    "    @staticmethod\n",
    "    def download_CSV_file():\n",
    "        \"\"\"automatically downloads the database file if it doesn't exists. Need\n",
    "        to be called in a try-catch block as it may throw FileNotFoundError or\n",
    "        IOError errors\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: In case it couldn't download the file\n",
    "\n",
    "        Returns:\n",
    "            str: The filename of the database wether it has been downloaded or not.\n",
    "        \"\"\"\n",
    "        # todays date\n",
    "        today = date.today()\n",
    "        # the prefix of the CSV file is Y-m-d\n",
    "        preFix = today.strftime('%Y-%m-%d') + \"-WHO\"\n",
    "        # the target file\n",
    "        targetFilename = '/content/data/' + preFix + '-db.csv'\n",
    "        # check if it exist already\n",
    "        if os.path.exists(targetFilename):\n",
    "            print('using existing file: ' + targetFilename)\n",
    "        else:\n",
    "            # download the file from the ecdc server\n",
    "            url = 'https://covid19.who.int/WHO-COVID-19-global-data.csv'\n",
    "            r = requests.get(url, timeout=1.0)\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(targetFilename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "            else:\n",
    "                raise FileNotFoundError('Error getting CSV file. Error code: ' + str(r.status_code))\n",
    "        return targetFilename\n",
    "\n",
    "    def get_available_GeoID_list(self):\n",
    "        \"\"\"Returns a dataframe having just two columns for the GeoID and Country name\n",
    "\n",
    "        Returns:\n",
    "            Dataframe: A dataframe having two columns: The country name and GeoID\n",
    "        \"\"\" \n",
    "        # the list of GeoIDs in the dataframe\n",
    "        geoIDs = self.__df['GeoID'].unique()\n",
    "        # the list of country names in the dataframe\n",
    "        countries = self.__df['GeoName'].unique()\n",
    "        # merge them together\n",
    "        list_of_tuples = list(zip(geoIDs, countries))\n",
    "        # create a dataframe out of the list\n",
    "        dfResult = pd.DataFrame(list_of_tuples, columns=['GeoID', 'GeoName'])\n",
    "        return dfResult\n",
    "\n",
    "    def get_data_source_info(self):\n",
    "        \"\"\"\n",
    "        Returns a list containing information about the data source. The list holds 3 strings:\n",
    "        InfoFullName: The full name of the data source\n",
    "        InfoShortName: A shortname for the data source\n",
    "        InfoLink: The link to get the data\n",
    "\n",
    "        Returns:\n",
    "            Dataframe: A dataframe holding the information\n",
    "        \"\"\"\n",
    "        info = [\"World Health Organization\", \n",
    "                \"WHO\",\n",
    "                \"https://covid19.who.int/WHO-COVID-19-global-data.csv\"]\n",
    "        return info\n",
    "\n",
    "    def review_geoid_list(self, geoIDs):\n",
    "        \"\"\"\n",
    "        Returns a corrected version of the given geoID list to ensure that cases of mismatches like UK-GB are corrected by the sub-class.  \n",
    "        geoIDs: The list holding the geoIDs as requested such as ['DE', 'UK']\n",
    "\n",
    "        Returns:\n",
    "            list: A corrected list such as ['DE', 'GB'] that translates incorrect country codes to corrected codes \n",
    "        \"\"\"\n",
    "        # fix the ECDC mistakes and map e.g. UK to GB \n",
    "        corrected = []\n",
    "        for geoID in geoIDs:\n",
    "            if geoID == 'UK':\n",
    "                corrected.append('GB')\n",
    "            elif geoID == 'EL':\n",
    "                corrected.append('GR')\n",
    "            elif geoID == 'TW':\n",
    "                corrected.append('CN')\n",
    "            else:\n",
    "                corrected.append(geoID)\n",
    "        return corrected\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_european_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of European countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_european_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_european_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of European countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main european countries for a map, pygal doesn't contain e.g. \n",
    "        # Andorra, Kosovo (XK)\n",
    "        geoIdList = 'AM, AL, AZ, AT, BA, BE, BG, BY, CH, CY, CZ, ' + \\\n",
    "                    'DE, DK, EE, GR, ES, FI, FR, GE, GL, '  + \\\n",
    "                    'HU, HR, IE, IS, IT, LV, LI, LT, ' + \\\n",
    "                    'MD, ME, MK, MT, NL, NO, PL, PT, ' + \\\n",
    "                    'RU, SE, SI, SK, RO, UA, GB, RS'\n",
    "        return geoIdList\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_american_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of American countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_american_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_pygal_american_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of American countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main american countries for a map, pygal doesn't contain e.g. \n",
    "        # Bahamas (BS), Barbados (BB), Bermuda (BM), Falkland Island (FK)\n",
    "        # 2022-01-22 added BZ\n",
    "        geoIdList = 'AR, BB, BM, BO, BR, BS, CA, CL, CO, ' + \\\n",
    "                    'CR, CU, DO, EC, SV, GT, GY, HN, HT, ' + \\\n",
    "                    'JM, MX, NI, PA, PE, PR, PY, SR, US, UY, VE, BZ'\n",
    "        return geoIdList\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_asian_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of Asian countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_asian_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_pygal_asian_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of Asian countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main asian countries for a map, pygal doesn't contain e.g. \n",
    "        # Qatar (QA)\n",
    "        geoIdList = 'AF, BH, BD, BT, BN, KH, CN, IR, IQ, IL, JP, JO, '  + \\\n",
    "                    'KZ, KW, KG, LA, LB, MY, MV, MN, MM, NP, OM, PK, PS, PH, '  + \\\n",
    "                    'QA, SA, SG, KR, LK, SY, TJ, TH, TL, TR, AE, UZ, VN, YE, IN, ID'\n",
    "        return geoIdList\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_pygal_african_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of African countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_african_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_pygal_african_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of African countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main african countries for a map, pygal doesn't contain e.g. \n",
    "        # Comoros (KM)\n",
    "        # 2022-01-22 added NA\n",
    "        geoIdList = 'DZ, AO, BJ, BW, BF, BI, CM, CV, CF, TD, KM, CG, CI, CD, '  + \\\n",
    "                    'DJ, EG, GQ, ER, SZ, ET, GA, GM, GH, GN, GW, KE, LS, LR, '  + \\\n",
    "                    'LY, MG, MW, ML, MR, MU, MA, MZ, NE, NG, RW, ST, SN, SC, '  + \\\n",
    "                    'SL, SO, ZA, SS, SD, TG, TN, UG, TZ, EH, ZM, ZW, NA'\n",
    "        return geoIdList\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_oceania_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of Oceanian countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_oceania_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_oceania_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of Oceanian countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main oceania countries for a map, pygal doesn't contain e.g. \n",
    "        # Comoros (KM)\n",
    "        geoIdList = 'AU, FJ, PF, GU, NC, NZ, MP, PG'\n",
    "        return geoIdList\n",
    "\n",
    "    \n",
    "class CovidCasesOWID(CovidCases):\n",
    "    \"\"\"The class will expose data attributes in form of a DataFrame. Its base class also provides methods to process \n",
    "    the data which will end up in additional columns in the DataFrame. These are the name sof the columns\n",
    "    that are generated. Notice: The 'Continent' column is additionally and specific to this sub class\n",
    "\n",
    "    Date\n",
    "    The date of the data \n",
    "    \n",
    "    GeoID\n",
    "    The GeoID of the country such as FR for France or DE for Germany\n",
    "\n",
    "    GeoName\n",
    "    The name of the country\n",
    "\n",
    "    Population\n",
    "    The population of the country\n",
    "\n",
    "    Continent\n",
    "    The continent of the country\n",
    "\n",
    "    DailyCases\n",
    "    The number of new cases on a given day\n",
    "\n",
    "    DailyDeaths\n",
    "    The number of new deaths on the given date\n",
    "\n",
    "    DailyVaccineDosesAdministered7DayAverage\n",
    "    New COVID-19 vaccination doses administered (7-day smoothed). For countries that \n",
    "    don't report vaccination data on a daily basis, we assume that vaccination \n",
    "    changed equally on a daily basis over any periods in which no data was reported. \n",
    "    This produces a complete series of daily figures, which is then averaged over a \n",
    "    rolling 7-day window. \n",
    "    In OWID words this is the new_vaccinations_smoothed value.\n",
    "                              \n",
    "    PeopleReceivedFirstDose\n",
    "    Total number of people who received at least one vaccine dose.\n",
    "    In OWID words this is the people_vaccinated value.\n",
    "\n",
    "    PeopleReceivedAllDoses\n",
    "    Total number of people who received all doses prescribed by the vaccination protocol.\n",
    "    In OWID words this is the people_fully_vaccinated value.\n",
    "\n",
    "    VaccineDosesAdministered\n",
    "    Total number of COVID-19 vaccination doses administered. It's the sum of \n",
    "    PeopleReceivedFirstDose and PeopleReceivedAllDoses.\n",
    "    In OWID words this is the total_vaccinations value.\n",
    "\n",
    "    Continent\n",
    "    The continent of the country as an additional column.\n",
    "    \n",
    "    Returns:\n",
    "        CovidCasesOWID: A class to provide access to some data based on the OWID file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"The constructor takes a string containing the full filename of a CSV\n",
    "        database you can download from the OWID website:\n",
    "        https://covid.ourworldindata.org/data/owid-covid-data.csv\n",
    "        The database will be loaded and kept as a private member. To retrieve the\n",
    "        data for an individual country you can use the public methods\n",
    "        GetCountryDataByGeoID or GetCountryDataByCountryName. These functions take \n",
    "        ISO 3166 alpha_2 (2 characters long) GeoIDs.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The full path and name of the csv file. \n",
    "        \"\"\"\n",
    "        # some benchmarking\n",
    "        start = time.time()\n",
    "        # open the file\n",
    "        self.__df = pd.read_csv(filename)\n",
    "        # remove columns that we don't need\n",
    "        self.__df = self.__df.drop(columns=['total_cases', \n",
    "                                            'new_cases_smoothed', \n",
    "                                            'total_deaths', \n",
    "                                            'new_deaths_smoothed', \n",
    "                                            'total_cases_per_million',\n",
    "                                            'new_cases_per_million',\n",
    "                                            'new_cases_smoothed_per_million',\n",
    "                                            'total_deaths_per_million',\n",
    "                                            'new_deaths_per_million',\n",
    "                                            'new_deaths_smoothed_per_million',\n",
    "                                            'reproduction_rate',\n",
    "                                            'icu_patients',\n",
    "                                            'icu_patients_per_million',\n",
    "                                            'hosp_patients',\n",
    "                                            'hosp_patients_per_million',\n",
    "                                            'weekly_icu_admissions',\n",
    "                                            'weekly_icu_admissions_per_million',\n",
    "                                            'weekly_hosp_admissions',\n",
    "                                            'weekly_hosp_admissions_per_million',\n",
    "                                            'new_tests',\n",
    "                                            'total_tests',\n",
    "                                            'total_tests_per_thousand',\n",
    "                                            'new_tests_per_thousand',\n",
    "                                            'new_tests_smoothed',\n",
    "                                            'new_tests_smoothed_per_thousand',\n",
    "                                            'positive_rate',\n",
    "                                            'tests_per_case',\n",
    "                                            'tests_units',\n",
    "                                            #'total_vaccinations',\n",
    "                                            'total_vaccinations_per_hundred',\n",
    "                                            'stringency_index',\n",
    "                                            'population_density',\n",
    "                                            'median_age',\n",
    "                                            'aged_65_older',\n",
    "                                            'aged_70_older',\n",
    "                                            'gdp_per_capita',\n",
    "                                            'extreme_poverty',\n",
    "                                            'cardiovasc_death_rate',\n",
    "                                            'diabetes_prevalence',\n",
    "                                            'female_smokers',\n",
    "                                            'male_smokers',\n",
    "                                            'handwashing_facilities',\n",
    "                                            'hospital_beds_per_thousand',\n",
    "                                            'life_expectancy',\n",
    "                                            'human_development_index',\n",
    "                                            # three more columns have been introduced\n",
    "                                            'new_vaccinations',\n",
    "                                            #'new_vaccinations_smoothed',\n",
    "                                            'new_vaccinations_smoothed_per_million',\n",
    "                                            #'people_fully_vaccinated',\n",
    "                                            'people_fully_vaccinated_per_hundred',\n",
    "                                            #'people_vaccinated',\n",
    "                                            'people_vaccinated_per_hundred',\n",
    "                                            # again a new field\n",
    "                                            'excess_mortality',\n",
    "                                            # and of course some new fields\n",
    "                                            'total_boosters',\n",
    "                                            'total_boosters_per_hundred',\n",
    "                                            # some more\n",
    "                                            'excess_mortality_cumulative_absolute',\n",
    "                                            'excess_mortality_cumulative',\n",
    "                                            'excess_mortality_cumulative_per_million',\n",
    "                                            'excess_mortality',\n",
    "                                            'new_people_vaccinated_smoothed',\n",
    "                                            'new_people_vaccinated_smoothed_per_hundred'])\n",
    "        if self.__df.columns.size != 11:\n",
    "            # oops, there are some new columns in the csv\n",
    "            print('Detecting new cols in OWID CSV: ' + self.__df.columns)\n",
    "            # add the new cols to a list\n",
    "            cols = [self.__df.columns[col] for col in range (11, self.__df.columns.size)]\n",
    "            # ...and drop them\n",
    "            self.__df = self.__df.drop(columns=cols)\n",
    "            print('Accepting cols in OWID CSV: ' + self.__df.columns)\n",
    "        # rename the columns to be more readable\n",
    "        self.__df.columns = ['GeoID',\n",
    "                             'Continent',\n",
    "                             'GeoName',\n",
    "                             'Date',\n",
    "                             'DailyCases',\n",
    "                             'DailyDeaths',\n",
    "                             'VaccineDosesAdministered',\n",
    "                             'PeopleReceivedFirstDose',\n",
    "                             'PeopleReceivedAllDoses',\n",
    "                             'DailyVaccineDosesAdministered7DayAverage',\n",
    "                             'Population']\n",
    "        #print(self.__df.columns)\n",
    "        # change the type of the 'date' field to a pandas date\n",
    "        self.__df['Date'] = pd.to_datetime(self.__df['Date'],\n",
    "                                           format='%Y/%m/%d')\n",
    "        # re-order the columns to be similar for all sub-classes                                   \n",
    "        self.__df = self.__df[['Date', \n",
    "                              'GeoName', \n",
    "                              'GeoID', \n",
    "                              'Population', \n",
    "                              'Continent', \n",
    "                              'DailyCases',\n",
    "                              'DailyDeaths',\n",
    "                              'DailyVaccineDosesAdministered7DayAverage',\n",
    "                              'PeopleReceivedFirstDose',\n",
    "                              'PeopleReceivedAllDoses',\n",
    "                              'VaccineDosesAdministered']]\n",
    "        #print(self.__df)\n",
    "        df = self.__df\n",
    "        # to apply the country names from our internal list\n",
    "        giw = GeoInformationWorld()\n",
    "        # get all country info\n",
    "        dfInfo = giw.get_geo_information_world()\n",
    "        # we need the newest date being on top, get all GeoIDs in the df\n",
    "        geoIDs = df['GeoID'].unique()\n",
    "        # our result data frame\n",
    "        dfs = []\n",
    "        for geoID in geoIDs:\n",
    "            # 'nan' workaround\n",
    "            if str(geoID) == 'nan':\n",
    "                # nothing else worked to detect this nan (it's the 'international' line in the file that doesn't have any GeoIds)\n",
    "                continue\n",
    "            # get the country dataframe\n",
    "            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n",
    "            # reset the index to start from index = 0\n",
    "            dfSingleCountry.reset_index(inplace=True, drop=True)\n",
    "            dfSingleCountry = dfSingleCountry.reindex(index=dfSingleCountry.index[::-1])  \n",
    "            # 'Kosovo' workaround\n",
    "            if geoID == 'OWID_KOS':\n",
    "                geoID = 'KOS'\n",
    "            # 'OWID World' workaround\n",
    "            if geoID == 'OWID_WRL':\n",
    "                continue\n",
    "            # get the geoName for this geoID from our internal list\n",
    "            geoName = giw.geo_name_from_ISO3166_alpha_3(geoID)\n",
    "            # get the alpha-2 geoID from the alpha-3 geoID\n",
    "            geoID2 = giw.geoID_from_ISO3166_alpha_3(geoID)\n",
    "            # the current name         \n",
    "            curName = dfSingleCountry['GeoName'][0]\n",
    "            # replace it if necessary\n",
    "            if geoName != curName:\n",
    "                dfSingleCountry['GeoName'] = [geoName for _ in range(0, len(dfSingleCountry['GeoID']))]\n",
    "            # now overwrite the alpha-3 geoID with the alpha-2 geoID so all sublasses can use the same geoIDs\n",
    "            dfSingleCountry['GeoID'] = [geoID2 for _ in range(0, len(dfSingleCountry['GeoID']))]    \n",
    "            # add the country to the result\n",
    "            dfs.append(dfSingleCountry)\n",
    "        # done, keep the list\n",
    "        self.__df = pd.concat(dfs)\n",
    "        # some benchmarking\n",
    "        end = time.time()\n",
    "        print('Pandas loading the OWID CSV: ' + str(end - start) + 's')\n",
    "        # pass the dataframe to the base class\n",
    "        super().__init__(self.__df)\n",
    "\n",
    "    @staticmethod\n",
    "    def download_CSV_file():\n",
    "        \"\"\"automatically downloads the database file if it doesn't exists. Need\n",
    "        to be called in a try-catch block as it may throw FileNotFoundError or\n",
    "        IOError errors\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: In case it couldn't download the file\n",
    "\n",
    "        Returns:\n",
    "            str: The filename of the database wether it has been downloaded or not.\n",
    "        \"\"\"\n",
    "        # todays date\n",
    "        today = date.today()\n",
    "        # the prefix of the CSV file is Y-m-d\n",
    "        preFix = today.strftime('%Y-%m-%d') + \"-OWID\"\n",
    "        # the target file\n",
    "        targetFilename = '/content/data/' + preFix + '-db.csv'\n",
    "        # check if it exist already\n",
    "        if os.path.exists(targetFilename):\n",
    "            print('using existing file: ' + targetFilename)\n",
    "        else:\n",
    "            # download the file from the ecdc server\n",
    "            url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'\n",
    "            r = requests.get(url, timeout=1.0)\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(targetFilename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "            else:\n",
    "                raise FileNotFoundError('Error getting CSV file. Error code: ' + str(r.status_code))\n",
    "        return targetFilename\n",
    "\n",
    "    def get_available_GeoID_list(self):\n",
    "        \"\"\"Returns a dataframe having just two columns for the GeoID and Country name\n",
    "\n",
    "        Returns:\n",
    "            Dataframe: A dataframe having two columns: The country name and GeoID\n",
    "        \"\"\" \n",
    "        # the list of GeoIDs in the dataframe\n",
    "        geoIDs = self.__df['GeoID'].unique()\n",
    "        # the list of country names in the dataframe\n",
    "        countries = self.__df['GeoName'].unique()\n",
    "        # merge them together\n",
    "        list_of_tuples = list(zip(geoIDs, countries))\n",
    "        # create a dataframe out of the list\n",
    "        dfResult = pd.DataFrame(list_of_tuples, columns=['GeoID', 'GeoName'])\n",
    "        return dfResult\n",
    "\n",
    "    def get_data_source_info(self):\n",
    "        \"\"\"\n",
    "        Returns a list containing information about the data source. The list holds 3 strings:\n",
    "        InfoFullName: The full name of the data source\n",
    "        InfoShortName: A shortname for the data source\n",
    "        InfoLink: The link to get the data\n",
    "\n",
    "        Returns:\n",
    "            Dataframe: A dataframe holding the information\n",
    "        \"\"\"\n",
    "        info = [\"Our World In Data\", \n",
    "                \"OWID\",\n",
    "                \"https://covid.ourworldindata.org/data/owid-covid-data.csv\"]\n",
    "        return info\n",
    "\n",
    "    def review_geoid_list(self, geoIDs):\n",
    "        \"\"\"\n",
    "        Returns a corrected version of the given geoID list to ensure that cases of mismatches like UK-GB are corrected by the sub-class.  \n",
    "        geoIDs: The list holding the geoIDs as requested such as ['DE', 'UK']\n",
    "\n",
    "        Returns:\n",
    "            list: A corrected list such as ['DE', 'GB'] that translates incorrect country codes to corrected codes \n",
    "        \"\"\"\n",
    "        # fix the ECDC mistakes and map e.g. UK to GB \n",
    "        corrected = []\n",
    "        for geoID in geoIDs:\n",
    "            if geoID == 'UK':\n",
    "                corrected.append('GB')\n",
    "            elif geoID == 'EL':\n",
    "                corrected.append('GR')\n",
    "            else:\n",
    "                corrected.append(geoID)\n",
    "        return corrected\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_european_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of European countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_european_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_european_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of European countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main european countries for a map, pygal doesn't contain e.g. \n",
    "        # Andorra, Kosovo (XK)\n",
    "        geoIdList = 'AM, AL, AZ, AT, BA, BE, BG, BY, CH, CY, CZ, ' + \\\n",
    "                    'DE, DK, EE, GR, ES, FI, FR, GE, GL, '  + \\\n",
    "                    'HU, HR, IE, IS, IT, LV, LI, LT, ' + \\\n",
    "                    'MD, ME, MK, MT, NL, NO, PL, PT, ' + \\\n",
    "                    'RU, SE, SI, SK, RO, UA, GB, RS'\n",
    "        return geoIdList\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_american_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of American countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_american_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_pygal_american_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of American countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main american countries for a map, pygal doesn't contain e.g. \n",
    "        # Bahamas (BS), Barbados (BB), Bermuda (BM), Falkland Island (FK)\n",
    "        geoIdList = 'AR, BB, BM, BO, BR, BS, CA, CL, CO, ' + \\\n",
    "                    'CR, CU, DO, EC, SV, GT, GY, HN, HT, ' + \\\n",
    "                    'JM, MX, NI, PA, PE, PR, PY, SR, US, UY, VE'\n",
    "        return geoIdList\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_asian_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of Asian countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_asian_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_pygal_asian_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of Asian countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main asian countries for a map, pygal doesn't contain e.g. \n",
    "        # Qatar (QA)\n",
    "        geoIdList = 'AF, BH, BD, BT, BN, KH, CN, IR, IQ, IL, JP, JO, '  + \\\n",
    "                    'KZ, KW, KG, LA, LB, MY, MV, MN, MM, NP, OM, PK, PS, PH, '  + \\\n",
    "                    'QA, SA, SG, KR, LK, SY, TW, TJ, TH, TL, TR, AE, UZ, VN, YE, IN, ID'\n",
    "        return geoIdList\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_pygal_african_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of African countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_african_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_pygal_african_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of African countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main african countries for a map, pygal doesn't contain e.g. \n",
    "        # Comoros (KM)\n",
    "        # 2022-01-20 added NA\n",
    "        geoIdList = 'DZ, AO, BJ, BW, BF, BI, CM, CV, CF, TD, KM, CG, CI, CD, '  + \\\n",
    "                    'DJ, EG, GQ, ER, SZ, ET, GA, GM, GH, GN, GW, KE, LS, LR, '  + \\\n",
    "                    'LY, MG, MW, ML, MR, MU, MA, MZ, NE, NG, RW, ST, SN, SC, '  + \\\n",
    "                    'SL, SO, ZA, SS, SD, TG, TN, UG, TZ, EH, ZM, ZW, NA'\n",
    "        return geoIdList\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_oceania_geoid_list():\n",
    "        \"\"\"Returns a list of GeoIDs of Oceanian countries that are available in PayGal and \n",
    "        the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            list: List of strings of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main countries for a map\n",
    "        geoIDs = re.split(r',\\s*', CovidCases.get_pygal_oceania_geoid_string_list().upper())\n",
    "        return geoIDs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pygal_oceania_geoid_string_list():\n",
    "        \"\"\"\n",
    "        Returns a comma separated list of GeoIDs of Oceanian countries that are available in \n",
    "        PayGal and the WHO data. \n",
    "        Be aware:\n",
    "        Not all countries of the WHO are available in PayGal and some names are different \n",
    "        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n",
    "        upper case. \n",
    "\n",
    "        Returns:\n",
    "            str: A comma separate list of GeoID's\n",
    "        \"\"\"\n",
    "        # just the main oceania countries for a map, pygal doesn't contain e.g. \n",
    "        # Comoros (KM)\n",
    "        geoIdList = 'AU, FJ, PF, GU, NC, NZ, MP, PG'\n",
    "        return geoIdList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqycnVSZYJ8U"
   },
   "source": [
    "# CovidFoliumMap class\n",
    "\n",
    "This abstract class acts as a base class for other classes that implement different folium maps based on different data \n",
    "sources. Here are some usefull links:\n",
    "\n",
    "- Geodata visualization   \n",
    "  Folium: The documentation is available on https://python-visualization.github.io/folium/  \n",
    "   Different basemaps are available on https://leaflet-extras.github.io/leaflet-providers/preview/\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnOZ7EdfYJ8U",
    "outputId": "e6ed8078-f67e-4a1d-f50e-22cfa7f06fda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (0.10.2)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from geopandas) (3.2.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from geopandas) (1.8.20)\n",
      "Requirement already satisfied: shapely>=1.6 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from geopandas) (1.8.0)\n",
      "Requirement already satisfied: pandas>=0.25.0 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from geopandas) (1.1.1)\n",
      "Requirement already satisfied: certifi in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from pyproj>=2.2.0->geopandas) (2020.6.20)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: attrs>=17 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (20.1.0)\n",
      "Requirement already satisfied: munch in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six>=1.7 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (41.2.0)\n",
      "Requirement already satisfied: click>=4.0 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from pandas>=0.25.0->geopandas) (1.19.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from pandas>=0.25.0->geopandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from pandas>=0.25.0->geopandas) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/cmbt/.pyenv/versions/3.7.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: folium in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (0.12.1.post1)\n",
      "Requirement already satisfied: requests in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from folium) (2.24.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from folium) (2.11.2)\n",
      "Requirement already satisfied: branca>=0.3.0 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from folium) (0.4.2)\n",
      "Requirement already satisfied: numpy in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from folium) (1.19.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from requests->folium) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from requests->folium) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from requests->folium) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from requests->folium) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/cmbt/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from jinja2>=2.9->folium) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/cmbt/.pyenv/versions/3.7.7/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas\n",
    "!pip install folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "\n",
    "class FoliumCovid19Map(ABC):\n",
    "    \"\"\"\n",
    "    This abstract base class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes. It does this  \n",
    "    by providing access to a pandas geoJSON dataframe and a data dataframe. It also also provides methods to generate a default map.\n",
    "    \"\"\"\n",
    "    def __init__(self, dfGeo, dfData, dataDirectory):\n",
    "        \"\"\"\n",
    "        The constructor takes two dataframes. One containing geoJSON information and a second containing CoVid-19 data. \n",
    "\n",
    "        Args:\n",
    "            dfGeo (dataframe): The geoPandas dataframe containing geometry information of the countries and regions of the world.  \n",
    "            dfData (dataframe): The 'regular' Pandas dataframe containing Covid-19 data to be shown on the map\n",
    "        \"\"\"\n",
    "        # keep the data frames\n",
    "        self.__dfGeo = dfGeo\n",
    "        self.__dfData = dfData,\n",
    "        self.__dataDirectory = dataDirectory\n",
    "\n",
    "    def get_geo_df(self):\n",
    "        \"\"\"Returns the geoPandas data frame\n",
    "        \n",
    "        Args:\n",
    "            -\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The geoPandas data frame containing geoJSON geometries\n",
    "        \"\"\"\n",
    "        return self.__dfGeo\n",
    "    \n",
    "    def get_data_df(self):\n",
    "        \"\"\"Returns the geoPandas data frame\n",
    "        \n",
    "        Args:\n",
    "            -\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The Pandas data frame containing Covid-19 data\n",
    "        \"\"\"\n",
    "        return self.__dfData\n",
    "\n",
    "    def get_data_directory(self):\n",
    "        \"\"\"Returns the data directory as a string\n",
    "        \n",
    "        Args:\n",
    "            -\n",
    "\n",
    "        Returns:\n",
    "            DataDirectory: A string pointing to the absolute data directory path\n",
    "        \"\"\"\n",
    "        return self.__dataDirectory\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it \n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7F5ylrtYJ8V"
   },
   "source": [
    "# CovidFoliumMapXXX classs\n",
    "\n",
    "This classes implement different folium maps based on the data of the WHO using the CovidCases, CovidCasesWHO and in case of the World and Asia maps also the CovidCasesOWID class to map the Taiwan cases as well.  \n",
    "The class inherits from the CovidFoliumMap class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HlNMzuDgYJ8V"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FoliumCovid19MapEurope(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for German states. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapEurope'\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data()\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'WorldCountries.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'\n",
    "            # the manual download link is\n",
    "            # 'https://github.com/datasets/geo-countries/blob/master/data/countries.geojson'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "        # adjust column names\n",
    "        geoDf.columns = ['Name', 'ISO-3166-alpha_3', 'GeoID', 'geometry']\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        try:\n",
    "            # get the latests database file as a CSV\n",
    "            dataFile = CovidCasesWHO.download_CSV_file()\n",
    "            # get the data for the countryList\n",
    "            whoData = CovidCasesWHO(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the list of comma separated geoIDs\n",
    "        countryList = whoData.get_pygal_european_geoid_string_list()\n",
    "        # get the data for the country list\n",
    "        df = whoData.get_data_by_geoid_string_list(countryList)\n",
    "        # add the incidence\n",
    "        df = whoData.add_incidence_7day_per_100Kpopulation(df)\n",
    "        # get the data for last friday, on days reporting will not be good\n",
    "        today = date.today()\n",
    "        # take care of weekends as the data is often not available on weekends\n",
    "        if (today.weekday() == 0) or (today.weekday() == 6):\n",
    "            last_friday = this_or_last_weekday(date.today(), 4)\n",
    "            self.__generationDate = date(last_friday.year, last_friday.month, last_friday.day)\n",
    "        else:\n",
    "            self.__generationDate = today - timedelta(1)\n",
    "        # get the data for that date\n",
    "        dfDate = df.loc[df['Date'] == pd.to_datetime(self.__generationDate)]     \n",
    "        #print(dfDate.head())\n",
    "        # ...and return df\n",
    "        return dfDate\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoName', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'PercentDeaths',\n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'Incidence7DayPer100Kpopulation',\n",
    "                                                    'CasesPerMillionPopulation',\n",
    "                                                    'DeathsPerMillionPopulation']],\n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='WHO data. Map generated by CMBT, 2022', location=[51.3, 10.5], tiles=basemap, zoom_start=4)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        #bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoName', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'PercentDeaths',\n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'Incidence7DayPer100Kpopulation',\n",
    "                                            'CasesPerMillionPopulation',\n",
    "                                            'DeathsPerMillionPopulation'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # a legend\n",
    "        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n",
    "        #map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'GeoID'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias    \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'cartodbpositron',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n",
    "\n",
    "\n",
    "class FoliumCovid19MapAsia(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for German states. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapAsia'\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data()\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'WorldCountries.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'\n",
    "            # the manual download link is\n",
    "            # 'https://github.com/datasets/geo-countries/blob/master/data/countries.geojson'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "        # adjust column names\n",
    "        geoDf.columns = ['Name', 'ISO-3166-alpha_3', 'GeoID', 'geometry']\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        try:\n",
    "            # get the latests WHO database file as a CSV\n",
    "            dataFile = CovidCasesWHO.download_CSV_file()\n",
    "            # get the data for the WHO countryList\n",
    "            whoData = CovidCasesWHO(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the list of comma separated geoIDs\n",
    "        countryList = whoData.get_pygal_asian_geoid_string_list()\n",
    "        # get the data for the country list\n",
    "        df = whoData.get_data_by_geoid_string_list(countryList)\n",
    "        # add the incidence\n",
    "        df = whoData.add_incidence_7day_per_100Kpopulation(df)\n",
    "        try:\n",
    "            # get the OWID database as well\n",
    "            dataFile = CovidCasesOWID.download_CSV_file()\n",
    "            # get the OWID data\n",
    "            owidData = CovidCasesOWID(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the taiwan data\n",
    "        dfTW = owidData.get_data_by_geoid_string_list('TW')\n",
    "        # add the incidence\n",
    "        dfTW = owidData.add_incidence_7day_per_100Kpopulation(dfTW)  \n",
    "        # append it\n",
    "        df = pd.concat([df, dfTW])  \n",
    "        # get the data for last friday, on days reporting will not be good\n",
    "        today = date.today()\n",
    "        # take care of weekends as the data is often not available on weekends\n",
    "        if (today.weekday() == 0) or (today.weekday() == 6):\n",
    "            last_friday = this_or_last_weekday(date.today(), 4)\n",
    "            self.__generationDate = date(last_friday.year, last_friday.month, last_friday.day)\n",
    "        else:\n",
    "            self.__generationDate = today - timedelta(1)\n",
    "        # get the data for that date\n",
    "        dfDate = df.loc[df['Date'] == pd.to_datetime(self.__generationDate)]      \n",
    "        #print(dfDate.head())\n",
    "        # ...and return df\n",
    "        return dfDate\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoName', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'PercentDeaths',\n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'Incidence7DayPer100Kpopulation',\n",
    "                                                    'CasesPerMillionPopulation',\n",
    "                                                    'DeathsPerMillionPopulation']],\n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='WHO data. Map genrated by CMBT, 2022', location=[23, 92], tiles=basemap, zoom_start=4)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        #bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoName', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'PercentDeaths',\n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'Incidence7DayPer100Kpopulation',\n",
    "                                            'CasesPerMillionPopulation',\n",
    "                                            'DeathsPerMillionPopulation'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # a legend\n",
    "        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n",
    "        #map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'GeoID'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias  \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'cartodbpositron',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n",
    "\n",
    "class FoliumCovid19MapAmerica(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for German states. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapAmerica'\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data()\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'WorldCountries.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'\n",
    "            # the manual download link is\n",
    "            # 'https://github.com/datasets/geo-countries/blob/master/data/countries.geojson'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "        # adjust column names\n",
    "        geoDf.columns = ['Name', 'ISO-3166-alpha_3', 'GeoID', 'geometry']\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        try:\n",
    "            # get the latests database file as a CSV\n",
    "            dataFile = CovidCasesWHO.download_CSV_file()\n",
    "            # get the data for the countryList\n",
    "            whoData = CovidCasesWHO(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the list of comma separated geoIDs\n",
    "        countryList = whoData.get_pygal_american_geoid_string_list()\n",
    "        # get the data for the country list\n",
    "        df = whoData.get_data_by_geoid_string_list(countryList)\n",
    "        # add the incidence\n",
    "        df = whoData.add_incidence_7day_per_100Kpopulation(df)\n",
    "        # get the data for last friday, on days reporting will not be good\n",
    "        today = date.today()\n",
    "        # take care of weekends as the data is often not available on weekends\n",
    "        if (today.weekday() == 0) or (today.weekday() == 6):\n",
    "            last_friday = this_or_last_weekday(date.today(), 4)\n",
    "            self.__generationDate = date(last_friday.year, last_friday.month, last_friday.day)\n",
    "        else:\n",
    "            self.__generationDate = today - timedelta(1)\n",
    "        # get the data for that date\n",
    "        dfDate = df.loc[df['Date'] == pd.to_datetime(self.__generationDate)]       \n",
    "        #print(dfDate.head())\n",
    "        # ...and return df\n",
    "        return dfDate\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoName', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'PercentDeaths',\n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'Incidence7DayPer100Kpopulation',\n",
    "                                                    'CasesPerMillionPopulation',\n",
    "                                                    'DeathsPerMillionPopulation']],\n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='WHO data. Map generated by CMBT, 2022', location=[16, -86], tiles=basemap, zoom_start=3)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        #bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoName', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'PercentDeaths',\n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'Incidence7DayPer100Kpopulation',\n",
    "                                            'CasesPerMillionPopulation',\n",
    "                                            'DeathsPerMillionPopulation'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # a legend\n",
    "        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n",
    "        #map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'GeoID'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias  \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'cartodbpositron',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n",
    "\n",
    "class FoliumCovid19MapOceania(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for German states. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapOceania'\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data()\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'WorldCountries.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'\n",
    "            # the manual download link is\n",
    "            # 'https://github.com/datasets/geo-countries/blob/master/data/countries.geojson'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "        # adjust column names\n",
    "        geoDf.columns = ['Name', 'ISO-3166-alpha_3', 'GeoID', 'geometry']\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        try:\n",
    "            # get the latests database file as a CSV\n",
    "            dataFile = CovidCasesWHO.download_CSV_file()\n",
    "            # get the data for the countryList\n",
    "            whoData = CovidCasesWHO(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the list of comma separated geoIDs\n",
    "        countryList = whoData.get_pygal_oceania_geoid_string_list()\n",
    "        # get the data for the country list\n",
    "        df = whoData.get_data_by_geoid_string_list(countryList)\n",
    "        # add the incidence\n",
    "        df = whoData.add_incidence_7day_per_100Kpopulation(df)\n",
    "        # get the data for last friday, on days reporting will not be good\n",
    "        today = date.today()\n",
    "        # take care of weekends as the data is often not available on weekends\n",
    "        if (today.weekday() == 0) or (today.weekday() == 6):\n",
    "            last_friday = this_or_last_weekday(date.today(), 4)\n",
    "            self.__generationDate = date(last_friday.year, last_friday.month, last_friday.day)\n",
    "        else:\n",
    "            self.__generationDate = today - timedelta(1)\n",
    "        # get the data for that date\n",
    "        dfDate = df.loc[df['Date'] == pd.to_datetime(self.__generationDate)]       \n",
    "        #print(dfDate.head())\n",
    "        # ...and return df\n",
    "        return dfDate\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoName', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'PercentDeaths',\n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'Incidence7DayPer100Kpopulation',\n",
    "                                                    'CasesPerMillionPopulation',\n",
    "                                                    'DeathsPerMillionPopulation']],\n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='WHO data. Map generated by CMBT, 2022', location=[-26, 147], tiles=basemap, zoom_start=4)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        #bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoName', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'PercentDeaths',\n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'Incidence7DayPer100Kpopulation',\n",
    "                                            'CasesPerMillionPopulation',\n",
    "                                            'DeathsPerMillionPopulation'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # a legend\n",
    "        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n",
    "        #map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'GeoID'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias  \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'cartodbpositron',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n",
    "\n",
    "class FoliumCovid19MapAfrica(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for German states. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapAfrica'\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data()\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'WorldCountries.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'\n",
    "            # the manual download link is\n",
    "            # 'https://github.com/datasets/geo-countries/blob/master/data/countries.geojson'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "        # adjust column names\n",
    "        geoDf.columns = ['Name', 'ISO-3166-alpha_3', 'GeoID', 'geometry']\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        try:\n",
    "            # get the latests database file as a CSV\n",
    "            dataFile = CovidCasesWHO.download_CSV_file()\n",
    "            # get the data for the countryList\n",
    "            whoData = CovidCasesWHO(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the list of comma separated geoIDs\n",
    "        countryList = whoData.get_pygal_african_geoid_string_list()\n",
    "        # get the data for the country list\n",
    "        df = whoData.get_data_by_geoid_string_list(countryList)\n",
    "        # add the incidence\n",
    "        df = whoData.add_incidence_7day_per_100Kpopulation(df)\n",
    "        # get the data for last friday, on days reporting will not be good\n",
    "        today = date.today()\n",
    "        # take care of weekends as the data is often not available on weekends\n",
    "        if (today.weekday() == 0) or (today.weekday() == 6):\n",
    "            last_friday = this_or_last_weekday(date.today(), 4)\n",
    "            self.__generationDate = date(last_friday.year, last_friday.month, last_friday.day)\n",
    "        else:\n",
    "            self.__generationDate = today - timedelta(1)\n",
    "        # get the data for that date\n",
    "        dfDate = df.loc[df['Date'] == pd.to_datetime(self.__generationDate)]      \n",
    "        #print(dfDate.head())\n",
    "        # ...and return df\n",
    "        return dfDate\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoName', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'PercentDeaths',\n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'Incidence7DayPer100Kpopulation',\n",
    "                                                    'CasesPerMillionPopulation',\n",
    "                                                    'DeathsPerMillionPopulation']],\n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='WHO data. Map generated by CMBT, 2022', location=[5, 19], tiles=basemap, zoom_start=4)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        #bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoName', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'PercentDeaths',\n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'Incidence7DayPer100Kpopulation',\n",
    "                                            'CasesPerMillionPopulation',\n",
    "                                            'DeathsPerMillionPopulation'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # a legend\n",
    "        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n",
    "        #map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'GeoID'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias  \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'cartodbpositron',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n",
    "\n",
    "class FoliumCovid19MapWorld(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for German states. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapWorld'\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data()\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'WorldCountries.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'\n",
    "            # the manual download link is\n",
    "            # 'https://github.com/datasets/geo-countries/blob/master/data/countries.geojson'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "        # adjust column names\n",
    "        geoDf.columns = ['Name', 'ISO-3166-alpha_3', 'GeoID', 'geometry']\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        try:\n",
    "            # get the latests WHO database file as a CSV\n",
    "            dataFile = CovidCasesWHO.download_CSV_file()\n",
    "            # get the data for the WHO countryList\n",
    "            whoData = CovidCasesWHO(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the list of comma separated geoIDs\n",
    "        countryList = whoData.get_pygal_asian_geoid_string_list()  + ',' + \\\n",
    "                      whoData.get_pygal_european_geoid_string_list()  + ',' + \\\n",
    "                      whoData.get_pygal_american_geoid_string_list()  + ',' + \\\n",
    "                      whoData.get_pygal_african_geoid_string_list()  + ',' + \\\n",
    "                      whoData.get_pygal_oceania_geoid_string_list()\n",
    "        # get the data for the country list\n",
    "        df = whoData.get_data_by_geoid_string_list(countryList)\n",
    "        # add the incidence\n",
    "        df = whoData.add_incidence_7day_per_100Kpopulation(df)\n",
    "        try:\n",
    "            # get the OWID database as well\n",
    "            dataFile = CovidCasesOWID.download_CSV_file()\n",
    "            # get the OWID data\n",
    "            owidData = CovidCasesOWID(dataFile)\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print(e)  \n",
    "            return df\n",
    "        # the taiwan data\n",
    "        dfTW = owidData.get_data_by_geoid_string_list('TW')\n",
    "        # add the incidence\n",
    "        dfTW = owidData.add_incidence_7day_per_100Kpopulation(dfTW)  \n",
    "        # append it\n",
    "        df = pd.concat([df, dfTW])  \n",
    "        # get the data for last friday, on days reporting will not be good\n",
    "        today = date.today()\n",
    "        # take care of weekends as the data is often not available on weekends\n",
    "        if (today.weekday() == 0) or (today.weekday() == 6):\n",
    "            last_friday = this_or_last_weekday(date.today(), 4)\n",
    "            self.__generationDate = date(last_friday.year, last_friday.month, last_friday.day)\n",
    "        else:\n",
    "            self.__generationDate = today - timedelta(1)\n",
    "        # get the data for that date\n",
    "        dfDate = df.loc[df['Date'] == pd.to_datetime(self.__generationDate)]      \n",
    "        #print(dfDate.head())\n",
    "        # ...and return df\n",
    "        return dfDate\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoName', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'PercentDeaths',\n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'Incidence7DayPer100Kpopulation',\n",
    "                                                    'CasesPerMillionPopulation',\n",
    "                                                    'DeathsPerMillionPopulation']],\n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='WHO data. Map generated by CMBT, 2022', location=[15, 0], tiles=basemap, zoom_start=2)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        #bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoName', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'PercentDeaths',\n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'Incidence7DayPer100Kpopulation',\n",
    "                                            'CasesPerMillionPopulation',\n",
    "                                            'DeathsPerMillionPopulation'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # a legend\n",
    "        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n",
    "        #map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'GeoID'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias  \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'cartodbpositron',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOlkwCX8nu5u"
   },
   "source": [
    "# CovidFoliumMapRKIXXX classs\n",
    "\n",
    "This classes implement different folium maps based on the data of the Robert Koch Institute. There (so far) two maps available. One showing the 7-day incidence data for German Stated, the other for German Cities and Counties.  \n",
    "The class inherits from the CovidFoliumMap class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hQMMZ9mjoZiF"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FoliumCovid19MapDEcounties(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for counties and cities in Germany. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapDEcounty'\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data(self.__dfGeo)\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German counties and cities or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'RKI_Corona_Landkreise.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/RKI_Landkreisdaten/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json'\n",
    "            # the manual download link is\n",
    "            # 'https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/917fc37a709542548cc3be077a786c17_0/explore?location=51.282342%2C10.714458%2C6.71'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "            #print(geoDf.head())\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self, geoDf):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German counties and cities or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        # get the date\n",
    "        today = date.today()\n",
    "        # the prefix of the CSV file is Y-m-d\n",
    "        preFix = today.strftime('%Y-%m-%d') + \"-RKIcounty\"\n",
    "        # the target filename of the csv to be downloaded\n",
    "        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n",
    "        # check if it exist already\n",
    "        if os.path.exists(targetFilename):\n",
    "            print('using existing file: ' + targetFilename)\n",
    "            # read the file\n",
    "            df = pd.read_csv(targetFilename)\n",
    "        else:\n",
    "            print('Downloading data, that might take some time...')\n",
    "            # build a result df\n",
    "            dfs = []\n",
    "            for id in geoDf['RS']:\n",
    "                try:\n",
    "                    # get the data for the county\n",
    "                    df = self.__get_county_data_from_web(id)\n",
    "                    # add it to the list\n",
    "                    dfs.append(df)\n",
    "                except:\n",
    "                    msg = 'Error getting the data for ' + id + '!'\n",
    "                    print(msg) \n",
    "            # finally concatenate all dfs together\n",
    "            df = pd.concat(dfs)  \n",
    "            # save it to file\n",
    "            df.to_csv(targetFilename)\n",
    "            print('Download finished.')\n",
    "        # ensure RS length is 5\n",
    "        if not df is None:\n",
    "            df['RS'] = df['RS'].astype(str).str.zfill(5)\n",
    "        # ...and return df\n",
    "        return df\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoID', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'WeeklyCases', \n",
    "                                                    'WeeklyDeaths', \n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'DailyRecovered', \n",
    "                                                    'Incidence7DayPer100Kpopulation']], \n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='Robert Koch-Institut (RKI), dl-de/by-2-0, CMBT 2022', location=[51.3, 10.5], tiles=basemap, zoom_start=6)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        #bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoID', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'WeeklyCases', \n",
    "                                            'WeeklyDeaths', \n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'DailyRecovered', \n",
    "                                            'Incidence7DayPer100Kpopulation'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def __get_county_data_from_web(self, county_ID):\n",
    "        \"\"\" Downloads the covid-19 data for the given county-ID\n",
    "\n",
    "        Args:\n",
    "            county_ID string: the county-ID for which we want the data\n",
    "\n",
    "        Raises:\n",
    "            ValueError: In case the data is empty\n",
    "\n",
    "        Returns:\n",
    "            dataframe: A dataframe of the county data\n",
    "        \"\"\"\n",
    "        # the endpoint of the request\n",
    "        endpoint = 'https://api.corona-zahlen.org/districts/' + county_ID\n",
    "        # contact the server\n",
    "        res = requests.get(endpoint)\n",
    "        # check if there was a response\n",
    "        if res.ok:\n",
    "            # get the json\n",
    "            res = res.json()\n",
    "        else:\n",
    "            # raise an exception\n",
    "            res.raise_for_status()\n",
    "        # check if the data is not empty\n",
    "        if not bool(res['data']):\n",
    "            raise ValueError(\"Empty response! County ID might be invalid.\")\n",
    "        df = pd.json_normalize(res['data'])\n",
    "        df.columns = ['RS', \n",
    "                    'GeoName', \n",
    "                    'GeoID', \n",
    "                    'State', \n",
    "                    'Population', \n",
    "                    'Cases',\n",
    "                    'Deaths',\n",
    "                    'WeeklyCases',\n",
    "                    'WeeklyDeaths',\n",
    "                    'StateID',\n",
    "                    'Recovered',\n",
    "                    'Incidence7DayPer100Kpopulation', \n",
    "                    'CasesPer100kPopulation', \n",
    "                    'DailyCases', \n",
    "                    'DailyDeaths', \n",
    "                    'DailyRecovered']\n",
    "        return df\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'RS'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias     \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['cartodbpositron',\n",
    "                    'https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n",
    "\n",
    "class FoliumCovid19MapDEstates(FoliumCovid19Map):\n",
    "    \"\"\"\n",
    "    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for German states. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataDirectory = '../data'):\n",
    "        \"\"\" Constructor\n",
    "\n",
    "        Args:\n",
    "            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n",
    "        \"\"\"\n",
    "        # init members\n",
    "        self.__dataDirectory = dataDirectory + '/'\n",
    "        self.__dfGeo = None\n",
    "        self.__dfData = None\n",
    "        self.__alias = 'MapDEstate'\n",
    "        self.__statelist = [['Schleswig-Holstein', 'SH'],\n",
    "                            ['Hamburg', 'HH'],\n",
    "                            ['Niedersachsen', 'NI'],\n",
    "                            ['Bremen', 'HB'],\n",
    "                            ['Nordrhein-Westfalen', 'NW'],\n",
    "                            ['Hessen', 'HE'],\n",
    "                            ['Rheinland-Pfalz', 'RP'],\n",
    "                            ['Baden-Württemberg', 'BW'],\n",
    "                            ['Bayern', 'BY'],\n",
    "                            ['Saarland', 'SL'],\n",
    "                            ['Berlin', 'BE'],\n",
    "                            ['Brandenburg', 'BB'],\n",
    "                            ['Mecklenburg-Vorpommern', 'MV'],\n",
    "                            ['Sachsen', 'SN'],\n",
    "                            ['Sachsen-Anhalt', 'ST'],\n",
    "                            ['Thüringen', 'TH']]\n",
    "        # ensure that the data directory exists, meaning to create it if it is not available\n",
    "        self.__dataDirectory = ensure_path_exists(dataDirectory)\n",
    "        # check if it really exists\n",
    "        if self.__dataDirectory != '':\n",
    "            # get the geo JSON data frame\n",
    "            self.__dfGeo = self.__get_geo_data()\n",
    "            # get the covid data for all counties/cities in the geo dataframe\n",
    "            if not self.get_geo_df is None:\n",
    "                self.__dfData = self.__get_covid_data()\n",
    "        # pass the everything to the base class\n",
    "        super().__init__(self.__dfGeo, self.__dfData, self.__dataDirectory)\n",
    "\n",
    "    def __get_geo_data(self):\n",
    "        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n",
    "        exception in case of an error\n",
    "\n",
    "        Returns:\n",
    "            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init return\n",
    "        geoDf = None\n",
    "        # the filename of the geoJSON that is used\n",
    "        targetFilename = self.__dataDirectory + '/' + 'RKI_Corona_Bundeslaender.geojson'\n",
    "        # check if it exist already\n",
    "        if not os.path.exists(targetFilename):\n",
    "            # download the file\n",
    "            print('Downloading data, that might take some time...')\n",
    "            endpoint = 'https://opendata.arcgis.com/api/v3/datasets/ef4b445a53c1406892257fe63129a8ea_0/downloads/data?format=geojson&spatialRefId=4326'\n",
    "            # the manual download link is\n",
    "            # 'https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/ef4b445a53c1406892257fe63129a8ea_0/explore'\n",
    "            try:\n",
    "                # try to download the file \n",
    "                download_JSON_file(endpoint, targetFilename)\n",
    "                print('Download finished.')\n",
    "            except Exception as e:\n",
    "                if hasattr(e, 'message'):\n",
    "                    print(e.message)\n",
    "                else:\n",
    "                    print(e)    \n",
    "        # now the file should exist\n",
    "        if os.path.exists(targetFilename):\n",
    "            # load the file\n",
    "            geoDf = gpd.read_file(targetFilename)\n",
    "        # finally return the geo df\n",
    "        return geoDf\n",
    "\n",
    "    def __get_covid_data(self):\n",
    "        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n",
    "        \n",
    "        Returns:\n",
    "            covid dataframe: the covid data for the German states or None if it can't load the file\n",
    "        \"\"\"\n",
    "        # init the result\n",
    "        df = None\n",
    "        # get the date\n",
    "        today = date.today()\n",
    "        # the prefix of the CSV file is Y-m-d\n",
    "        preFix = today.strftime('%Y-%m-%d') + \"-RKIstates\"\n",
    "        # the target filename of the csv to be downloaded\n",
    "        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n",
    "        # check if it exist already\n",
    "        if os.path.exists(targetFilename):\n",
    "            print('using existing file: ' + targetFilename)\n",
    "            # read the file\n",
    "            df = pd.read_csv(targetFilename)\n",
    "        else:\n",
    "            print('Downloading data, that might take some time...')\n",
    "            # build a result df\n",
    "            dfs = []\n",
    "            for id in self.__statelist:\n",
    "                try:\n",
    "                    # get the data for the county\n",
    "                    df = self.__get_state_data_from_web(id[1])\n",
    "                    # add it to the list\n",
    "                    dfs.append(df)\n",
    "                except:\n",
    "                    msg = 'Error getting the data for ' + id + '!'\n",
    "                    print(msg) \n",
    "            # finally concatenate all dfs together\n",
    "            df = pd.concat(dfs)  \n",
    "            # save it to file\n",
    "            df.to_csv(targetFilename)\n",
    "            print('Download finished.')\n",
    "            #print(df.head())\n",
    "        # ensure AGS length is 2\n",
    "        if not df is None:\n",
    "            df['AGS_TXT'] = df['AGS_TXT'].astype(str).str.zfill(2)\n",
    "        # ...and return df\n",
    "        return df\n",
    "    \n",
    "    def create_default_map(self, \n",
    "                           basemap, coloredAttribute = 'Incidence7DayPer100Kpopulation', \n",
    "                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n",
    "        \"\"\" Returns a default folium map\n",
    "\n",
    "        Args:\n",
    "            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n",
    "            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n",
    "            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n",
    "        \"\"\"\n",
    "        # check if we have every<thing that we need\n",
    "        if (self.__dfGeo is None) or (self.__dfData is None):\n",
    "            return None\n",
    "        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n",
    "        combined = self.__dfGeo.merge(self.__dfData[[self.get_merge_UID(), \n",
    "                                                    'GeoName', \n",
    "                                                    'Cases', \n",
    "                                                    'Deaths', \n",
    "                                                    'WeeklyCases', \n",
    "                                                    'WeeklyDeaths', \n",
    "                                                    'DailyCases', \n",
    "                                                    'DailyDeaths', \n",
    "                                                    'DailyRecovered', \n",
    "                                                    'Incidence7DayPer100Kpopulation',\n",
    "                                                    'HospitalizationCases7']], \n",
    "                                                    on=self.get_merge_UID(), \n",
    "                                                    how='left')\n",
    "        # create the map\n",
    "        map = folium.Map(attr='Robert Koch-Institut (RKI), dl-de/by-2-0, CMBT 2022', location=[51.3, 10.5], tiles=basemap, zoom_start=6)\n",
    "        # the alias incl. the date\n",
    "        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + date.today().strftime('%Y-%m-%d')\n",
    "        # the bins for the colored values\n",
    "        #bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n",
    "        # the maximum in the coloredAttribute column\n",
    "        max = self.__dfData[coloredAttribute].max()\n",
    "        # fixed bins\n",
    "        bins = [0, 150, 300, 450, 600, 750, 900, 1050, 1200, max]\n",
    "        # build the choropleth\n",
    "        cp = folium.Choropleth (geo_data=combined,\n",
    "                                data=combined,\n",
    "                                #data=df,\n",
    "                                columns=[self.get_merge_UID(), coloredAttribute],\n",
    "                                key_on='feature.properties.' + self.get_merge_UID(),\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.4,\n",
    "                                line_opacity=0.4,\n",
    "                                nan_fill_color='#f5f5f3',\n",
    "                                legend_name=coloredAttributeAlias,\n",
    "                                bins=[float(x) for x in bins],\n",
    "                                highlight=True,\n",
    "                                smooth_factor = 0.1)\n",
    "        # give it a name\n",
    "        cp.layer_name = \"Covid-19 data\"  \n",
    "        # add it to the map\n",
    "        cp.add_to(map)\n",
    "        # create a tooltip for hovering\n",
    "        tt = folium.GeoJsonTooltip(fields= ['GeoName', \n",
    "                                            'Cases', \n",
    "                                            'Deaths', \n",
    "                                            'WeeklyCases', \n",
    "                                            'WeeklyDeaths', \n",
    "                                            'DailyCases', \n",
    "                                            'DailyDeaths', \n",
    "                                            'DailyRecovered', \n",
    "                                            'Incidence7DayPer100Kpopulation',\n",
    "                                            'HospitalizationCases7'])\n",
    "        # add it to the json\n",
    "        tt.add_to(cp.geojson)\n",
    "        # numbers and dates in the system local\n",
    "        tt.localize = True\n",
    "        # add a layer control to the map\n",
    "        folium.LayerControl().add_to(map)\n",
    "        # a legend\n",
    "        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n",
    "        #map.get_root().html.add_child(folium.Element(legend_html))\n",
    "        # return the map\n",
    "        return map\n",
    "\n",
    "    def __get_state_data_from_web(self, state_ID):\n",
    "        \"\"\" Downloads the covid-19 data for the given county-ID\n",
    "\n",
    "        Args:\n",
    "            county_ID string: the county-ID for which we want the data\n",
    "\n",
    "        Raises:\n",
    "            ValueError: In case the data is empty\n",
    "\n",
    "        Returns:\n",
    "            dataframe: A dataframe of the county data\n",
    "        \"\"\"\n",
    "        # the endpoint of the request\n",
    "        endpoint = 'https://api.corona-zahlen.org/states/' + state_ID\n",
    "        # contact the server\n",
    "        res = requests.get(endpoint)\n",
    "        # check if there was a response\n",
    "        if res.ok:\n",
    "            # get the json\n",
    "            res = res.json()\n",
    "        else:\n",
    "            # raise an exception\n",
    "            res.raise_for_status()\n",
    "        # check if the data is not empty\n",
    "        if not bool(res['data']):\n",
    "            raise ValueError(\"Empty response! County ID might be invalid.\")\n",
    "        df = pd.json_normalize(res['data'])\n",
    "        # adjust column names\n",
    "        df.columns = ['AGS_TXT', \n",
    "                    'GeoName', \n",
    "                    'Population', \n",
    "                    'Cases',\n",
    "                    'Deaths',\n",
    "                    'WeeklyCases',\n",
    "                    'WeeklyDeaths',\n",
    "                    'Recovered',\n",
    "                    'GeoID',\n",
    "                    'Incidence7DayPer100Kpopulation', \n",
    "                    'CasesPer100kPopulation', \n",
    "                    'DailyCases', \n",
    "                    'DailyDeaths', \n",
    "                    'DailyRecovered',\n",
    "                    'HospitalizationCases7',\n",
    "                    'HospitalizationIncidence7',\n",
    "                    'HospitalizationDate',\n",
    "                    'HospitalizationUpdate']\n",
    "        return df\n",
    "\n",
    "    def get_merge_UID(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the data dataframe \n",
    "        \"\"\"\n",
    "        return 'AGS_TXT'\n",
    "\n",
    "    def get_map_alias(self):\n",
    "        \"\"\"\n",
    "        Returns the string holding the name of the map that can be used to save it\n",
    "\n",
    "        Returns:\n",
    "            string: A string holding the name of the unique ID of the geo dataframe \n",
    "        \"\"\"\n",
    "        return self.__alias     \n",
    "\n",
    "    def get_nice_basemaps(self):\n",
    "        \"\"\"\n",
    "        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n",
    "        the preferred basemap should be basemap[0]\n",
    "\n",
    "        Returns:\n",
    "            string: A array of strings referring to nice basemaps \n",
    "        \"\"\"\n",
    "        mapArray = ['cartodbpositron',\n",
    "                    'https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n",
    "                    'Stamen Terrain']\n",
    "        return mapArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lghJh3eIYJ8a"
   },
   "source": [
    "# The main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2lCDJkmYJ8a",
    "outputId": "cdc364ad-3932-444e-9413-e36500a664ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on colab. Using /Users/cmbt/Documents/GitHub/Covid-19-analysis/./data/ as the data directory\n",
      "Downloading data, that might take some time...\n",
      "Download finished.\n",
      "using existing file: /Users/cmbt/Documents/GitHub/Covid-19-analysis/./data//2022-01-21-RKIstates-db.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def main():\n",
    "    # the directory for temp. data as well as for the output\n",
    "    if 'google.colab' in str(get_ipython()):    \n",
    "        outputDir = '/content/data/'\n",
    "        print('Running on colab. Using ' + outputDir + ' as the data directory')\n",
    "    else:\n",
    "        # the absolute directory of this python file\n",
    "        absDirectory = os.path.dirname(os.path.abspath(os.path.abspath('')))\n",
    "        # the target filename\n",
    "        outputDir = os.path.join(absDirectory, './data/')\n",
    "        print('Not running on colab. Using ' + outputDir + ' as the data directory')  \n",
    "    \n",
    "    # an array of instances\n",
    "    mapObjects = []\n",
    "    #\"\"\"\n",
    "    # world\n",
    "    mapObjects.append(FoliumCovid19MapWorld(outputDir))\n",
    "    # africa\n",
    "    mapObjects.append(FoliumCovid19MapAfrica(outputDir))\n",
    "    # oceania\n",
    "    mapObjects.append(FoliumCovid19MapOceania(outputDir))\n",
    "    # america\n",
    "    mapObjects.append(FoliumCovid19MapAmerica(outputDir))\n",
    "    # asia\n",
    "    mapObjects.append(FoliumCovid19MapAsia(outputDir))\n",
    "    # europe\n",
    "    mapObjects.append(FoliumCovid19MapEurope(outputDir))\n",
    "    #\"\"\"\n",
    "    #\"\"\"\n",
    "    # de states\n",
    "    mapObjects.append(FoliumCovid19MapDEstates(outputDir))\n",
    "    # de counties\n",
    "    mapObjects.append(FoliumCovid19MapDEcounties(outputDir))\n",
    "    #\"\"\"\n",
    "    \n",
    "    # process the maps\n",
    "    for mapObject in mapObjects:\n",
    "        if mapObject.get_geo_df() is None:\n",
    "            return\n",
    "        # get the data directory\n",
    "        dir = mapObject.get_data_directory()\n",
    "        # select a basemap\n",
    "        basemap = mapObject.get_nice_basemaps()[0]\n",
    "        # build the default map\n",
    "        map = mapObject.create_default_map(basemap)\n",
    "        # save the map\n",
    "        map.save(outputDir + mapObject.get_map_alias() + '.html')  \n",
    "        if mapObject.get_map_alias().find('World') > 0:\n",
    "            # build another map of the world\n",
    "            map = mapObject.create_default_map(basemap, 'PercentDeaths', 'Case Fatality Rate (CFR)')\n",
    "            # save that as well\n",
    "            map.save(outputDir + mapObject.get_map_alias() + 'CFR.html')  \n",
    "            # build another map of the world\n",
    "            map = mapObject.create_default_map(basemap, 'CasesPerMillionPopulation', 'Cases per million population')\n",
    "            # save that as well\n",
    "            map.save(outputDir + mapObject.get_map_alias() + 'CasesPerMillionPopulation.html')  \n",
    "            map\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ufJEHchYJ8a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "t0p31pajYJ8D",
    "_zJqa4_YYJ8K",
    "zlx1lJ-XZO44",
    "JqycnVSZYJ8U"
   ],
   "name": "CovidFoliumMapGenerator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
