{"cells":[{"cell_type":"markdown","metadata":{"id":"t0p31pajYJ8D"},"source":["# Utility functions\n","\n","Some functions required by this and other modules.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655282889176,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"1MFeKfTtYJ8H"},"outputs":[],"source":["import requests\n","import json\n","import datetime\n","from datetime import date, timedelta\n","\n","\n","def ensure_path_exists(thePath):\n","    \"\"\" The function checks of the given relative or absolute path exists and if not it will try to create it. If that fails\n","    the function will throw an exception\n","\n","    Args:\n","        thePath (string): the given relative or absolute path\n","\n","    Returns:\n","        string: The absolute path that will exists or an empty string if it failed to create it\n","    \"\"\"\n","    # if the data directory is given as an absolute path it's all fine\n","    if Path(thePath).is_absolute():\n","        result = thePath\n","    else:\n","        # get path to the directory of this file\n","        try:\n","            # check if it is running in jupyter, it will throw if not running in jupyter\n","            get_ipython\n","            # the absolute directory of this python file\n","            currentDirectory = os.path.dirname(os.path.abspath(os.path.abspath('')))\n","        except:\n","            # the absolute directory of this python file\n","            currentDirectory = os.path.dirname(os.path.abspath(__file__))\n","        # the directory is not given as an absolute path so add it to the current directory\n","        result = currentDirectory + '/' + thePath\n","    if not os.path.exists(result):\n","        try:\n","            os.makedirs(result)\n","        except Exception as e:\n","            if hasattr(e, 'message'):\n","                print(e.message)\n","            else:\n","                print(e)  \n","            return ''\n","    return result\n","    \n","def download_JSON_file(endpoint, filename):\n","    \"\"\" The function downloads a JSON file from the given endpoint and stores it in a file \n","    of the given filename. If the directory doesn't exist it will be created. The function \n","    throws an exception in case of an error\n","\n","    Args:\n","        endpoint (string): the full endpoint that is referring to a JSON file\n","        filename (string): the full filename of the file to be created\n","\n","    Raises:\n","        IOError: In case it can't save the data\n","    \"\"\"\n","    # contact the server\n","    res = requests.get(endpoint)\n","    # check if there was a response\n","    if res.ok:\n","        # get the json\n","        res = res.json()\n","    else:\n","        # raise an exception\n","        res.raise_for_status()\n","    try:\n","        # create the directory if it doesn't exist \n","        path = os.path.dirname(filename)\n","        if not os.path.exists(path):\n","            os.makedirs(filename)\n","        # write it to the file\n","        with open (filename, 'w', encoding='utf-8') as f:\n","            # use dumps as we don't care about formatting\n","            f.write(json.dumps(res) + \"\\n\")\n","    except:\n","        msg = 'Error writing file ' + filename\n","        raise IOError(msg)      \n","\n","def this_or_last_weekday(the_date, the_weekday):\n","    \"\"\" Retruns the given date of the last weekday or the given date if that has the right weekday.\n","        Example: the_date = 2022.01.19 that was a Wednesday (weekday=2), \n","                 if being called with the_weekday=4 (Friday) the function will return 2022.01.14\n","                 if being called with the_weekday=2 (Wednesday) the function will return 2022.01.19\n","\n","    Args:\n","        the_date (Date): the date to be checked\n","        the_weekday (int): the day of the week to get the date for ranging from 0 (Monday) to 6 (Sunday)\n","\n","    Returns:\n","        DateTime: The date of the weekday a week ago or at the given date if it is already the proper weekday\n","    \"\"\"\n","    # maybe it is the_date that is the right weekday\n","    if the_date.weekday() == the_weekday:\n","        return the_date\n","    # 9:00 on that date\n","    the_time = datetime.datetime(the_date.year, the_date.month, the_date.day, 9, 0)\n","    # get the same day one week ago at 9:00\n","    last_weekday = (the_time.date() -\n","                    datetime.timedelta(days=the_time.weekday()) +\n","                    datetime.timedelta(days=the_weekday, weeks=-1))\n","    last_weekday_at_9 = datetime.datetime.combine(last_weekday, datetime.time(9))\n","\n","    # if today is also the_weekday but after 9:00 change to the current date\n","    one_week = datetime.timedelta(weeks=1)\n","    if the_time - last_weekday_at_9 >= one_week:\n","        last_weekday_at_9 += one_week\n","    return last_weekday_at_9\n"," "]},{"cell_type":"markdown","metadata":{"id":"_zJqa4_YYJ8K"},"source":["# The GeoInformationWorld class\n","\n","A class to handle ISO 3166 country codes and names including basic inormation about the population of the countries of the world.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":550,"status":"ok","timestamp":1655282889722,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"umrZiEtaYJ8K"},"outputs":[],"source":["from abc import ABC, abstractmethod\n","import pandas as pd\n","import numpy as np\n","import math\n","import re\n","\n","class GeoInformationWorld():\n","    \n","    def __init__(self):\n","        \"\"\"The constructor loads a CSV with the geo information of the countries of the world.  \n","            ATTENTION: The GeoID and alpha-2 of Nambia would be 'NA' but panadas csv reader makes a NaN out of it.\n","\n","        Raises:\n","            FileNotFoundError: In case it couldn't download the file\n","\n","        \"\"\"\n","        # load the geo information for the world via GitHub\n","        targetFilename = 'https://raw.githubusercontent.com/1c3t3a/Covid-19-analysis/master/data/GeoInformationWorld.csv'\n","        self.__dfGeoInformationWorld = pd.read_csv(targetFilename, keep_default_na=False)\n","        \n","    def get_geo_information_world(self):\n","        \"\"\"Return the dataframe of information of all countries such as country name, continent, population etc..\n","        \n","        Returns:\n","            DataFrame: A data frame holding the information of all countries\n","        \"\"\"\n","        return self.__dfGeoInformationWorld\n","\n","    def geo_name_from_geoid (self, geoID):\n","        \"\"\"Return the name of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n","        \n","        Args:\n","            geoID (str):  a string of a ISO-3166-alpha_2 geoid\n","\n","        Returns:\n","            str: the country name\n","        \"\"\"\n","        # get the world info\n","        dfInfo = self.get_geo_information_world()\n","        # ISO-3166-alpha_3\n","        # find the row in our internal listin the GeoID column\n","        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n","        # the name used in our internal list\n","        return dfTheOne['GeoName'].values[0]\n","        \n","    def geo_name_from_ISO3166_alpha_3 (self, geoID):\n","        \"\"\"Return the name of a country of the internal geo information from a given ISO-3166-alpha_3 geoid.\n","\n","        Args:\n","            geoID (str):  a string of a ISO-3166-alpha_3 geoid\n","\n","        Returns:\n","            str: the country name\n","        \"\"\"\n","        # get the world info\n","        dfInfo = self.get_geo_information_world()\n","        # find the row in our internal listin the GeoID column\n","        dfTheOne = dfInfo.loc[dfInfo['ISO-3166-alpha_3'] == geoID]\n","        if dfTheOne.empty:\n","            # print the geoid that is not in the database\n","            #print('Unknown GeoId: ' + geoID)\n","            return 'Unknown'\n","        # the name used in our internal list\n","        return dfTheOne['GeoName'].values[0]\n","\n","    def geoID_from_ISO3166_alpha_3 (self, geoID):\n","        \"\"\"Return the name of a country of the internal geo information from a given ISO-3166-alpha_3 geoid.\n","\n","        Args:\n","            geoID (str):  a string of a ISO-3166-alpha_3 geoid\n","\n","        Returns:\n","            str: ISO-3166-alpha_2 geoid\n","        \"\"\"\n","        # get the world info\n","        dfInfo = self.get_geo_information_world()\n","        # find the row in our internal listin the GeoID column\n","        dfTheOne = dfInfo.loc[dfInfo['ISO-3166-alpha_3'] == geoID]\n","        # check if it is empty\n","        if dfTheOne.empty:\n","            # print the geoid that is not in the database\n","            #print('Unknown GeoId: ' + geoID)\n","            return 'Unknown'\n","        # the name used in our internal list\n","        return dfTheOne['GeoID'].values[0]\n","\n","    def ISO3166_alpha_3_from_geoID (self, geoID):\n","        \"\"\"Return the ISO-3166-alpha_2 geoid of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n","\n","        Args:\n","            geoID (str):  a string of a ISO-3166-alpha_2 geoid\n","\n","        Returns:\n","            str: the ISO-3166-alpha_3 geoid\n","        \"\"\"\n","        # get the world info\n","        dfInfo = self.get_geo_information_world()\n","        # find the row in our internal list in the GeoID column\n","        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n","        # the name used in our internal list\n","        return dfTheOne['ISO-3166-alpha_3'].values[0]\n","\n","    def population_from_geoid(self, geoID):\n","        \"\"\"Return the population of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n","\n","        Args:\n","            geoID (str):  a string of a ISO 3166 alpha_2 geoid\n","\n","        Returns:\n","            int: the population of the country\n","        \"\"\"\n","        # get the world info\n","        dfInfo = self.get_geo_information_world()\n","         # find the row in our internal list in the GeoID column\n","        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n","        # the name used in our internal list\n","        pop = int(dfTheOne['Population2019'].values[0])\n","        return pop\n","\n","    def continent_from_geoid(self, geoID):\n","        \"\"\"Return the continent of a country of the internal geo information from a given ISO-3166-alpha_2 geoid.\n","\n","        Args:\n","            geoID (str):  a string of a ISO 3166 alpha_2 geoid\n","\n","        Returns:\n","            str: the continent of the country\n","        \"\"\"\n","        # get the world info\n","        dfInfo = self.get_geo_information_world()\n","         # find the row in our internal list in the GeoID column\n","        dfTheOne = dfInfo.loc[dfInfo['GeoID'] == geoID]\n","        # the name used in our internal list\n","        return dfTheOne['Continent'].values[0]\n"]},{"cell_type":"markdown","metadata":{"id":"zlx1lJ-XZO44"},"source":["# The CovidCases class and its subclasses\n","\n","This abstract base class will expose data attributes in form of a DataFrame. It also provides methods to process the data which will end up in additional columns in the DataFrame. Please refer to [http://mb.cmbt.de/python-class-documentation/the-covidcases-class52/](http://mb.cmbt.de/python-class-documentation/the-covidcases-class52/) for a complete documentation of the class.  \n","\n","So far there are three sub-classes handling three different data sources: \n","  \n","```CovidCasesWHO```  \n","the data is provided by the [WHO website](https://covid19.who.int/WHO-COVID-19-global-data.csv).   \n","\n","```CovidCasesOWID```  \n","gets data from [Our World In Data](https://covid.ourworldindata.org/data/owid-covid-data.csv). The quality of this data, especially the fact that the data is not for all countries generated by official agencies is somehow a drawback. On the other side OWID generates much more data such as vaccination numbers.  \n","\n","```CovidCasesECDC```  \n","handles the data provided by the [European Center of Disease Control](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) until December, 14th. 2020.  \n","\n","The documentation for this classes is available on [http://mb.cmbt.de/python-class-documentation/the-covidcases-world-sub-classes52/](http://mb.cmbt.de/python-class-documentation/the-covidcases-world-sub-classes52/). \n","Please note: The ECDC subclass is a legacy class and can't be used anymore as the ECDC is not publishing daily updated data since December 2020.  \n","\n","The [blog post in this link](http://mb.cmbt.de/covid-19-analysis/data-source-comparison/) compares the quality of the different datasets.\n","\n","**ATTENTION**  \n","These classes have been modified compared to the files on GitHub to be able\n","to be executed in colab and they store the data in **/content/data/**  \n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8736,"status":"ok","timestamp":1655282898455,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"b-IFuGaRZR01"},"outputs":[],"source":["class CovidCases(ABC):\n","    \"\"\"This abstract base class will expose data attributes in form of a DataFrame. It also provides methods to process \n","    the data which will end up in additional columns in the DataFrame.  \n","    These are the names of seven columns that have to be generated by ALL subclasses.\n","\n","    Date\n","    The date of the data \n","    \n","    GeoID\n","    The ISO-3166-alpha_3 GeoID of the area such as 'FR' for France or 'DE' for Germany\n","\n","    GeoName\n","    The name of the area such as 'England' or 'Italy'\n","\n","    Population\n","    The population of the country\n","\n","    Continent\n","    E.g. The continent of the country. But it also may be grouping value for e.g. the states of a federal republic such as Bavaria\n","    \n","    DailyCases\n","    The number of new cases on a given day\n","\n","    DailyDeaths\n","    The number of new deaths on the given date\n","\n","    Beside these fields a subclass might also define additional columns such as 'Continent'\n","    Based on the six mandatory columns the class will generate the following additional columns (attributes):\n","    \n","    Cases\n","    The accumulated number of cases since the 31.12.2019\n","\n","    Deaths\n","    The accumulated number of deaths since the 31.12.2019\n","\n","    CasesPerMillionPopulation\n","    The number of cumulative cases divided by the population of the country in million\n","\n","    DeathsPerMillionPopulation\n","    The number of cumulative deaths divided by the population of the country in million\n","\n","    PercentDeaths\n","    The number of deaths in % of the cases. This is the Case Fatality Rate (CFR), an approximation for the\n","    Infection Fatality Rate (IFR) that includes also 'hidden' cases.\n","\n","    Incidence7DayPer100Kpopulation\n","    The accumulated 7 day incidence. That is the sum of daily cases of the last 7 days divided by the \n","    population in 100000\n","\n","    DoublingTime\n","    The number of days in which the number of cases will be doubled\n","\n","    R0\n","    This is an estimation of the reproduction number R0. As the calculation takes some time it is \n","    generated on demand by calling add_r0 method.\n","\n","    Beside that sub-class may add additional attributes. Please refer to the documentation of the \n","    specific sub-class that you want to use.\n","\n","    Returns:\n","        You can't create an instance of this class. Instead create an instance of a subclass\n","    \"\"\"\n","    def __init__(self, df, filenameCache = '', cacheLevel = 0):\n","        \"\"\"The constructor takes a dataframe loaded by any sub-class containing the data published by the\n","        website that is handled in the sub-classes individually.  \n","        To retrieve the data for an individual country you can use the public methods\n","        GetCountryDataByGeoID or GetCountryDataByCountryName. These functions take ISO 3166 alpha_2 \n","        (2 characters long) GeoIDs.\n","\n","        Args:\n","            df (dataframe): The dataframe containing information about individual countries such as\n","                            GeoID, CountryName, Cases and Deaths. \n","            filenameCache (str, optional): the filename of the cache. Defaults to \"\"\n","            cacheLevel (int, optional): the amount of data to be calculated for the cache. Defaults to 2.\n","                refer to __build_cache for more information of the different cache levels\n","        \"\"\"\n","        # load the geo information for the world via GitHub\n","        targetFilename = 'https://raw.githubusercontent.com/1c3t3a/Covid-19-analysis/master/data/GeoInformationWorld.csv'\n","        self.__dfGeoInformationWorld = pd.read_csv(targetFilename)\n","        # build a cache if wanted and keep it\n","        if (filenameCache != '' and cacheLevel > 0):\n","            self.__df = self.__build_cache(df, filenameCache, cacheLevel)  \n","        else:\n","            self.__df = df\n","\n","    @staticmethod\n","    def __compute_doubling_time(dfSingleCountry):\n","        \"\"\"Computes the doubling time for everyday day with the formula:\n","                ln(2) / ln(Conf[n] / Conf[n - 1])\n","        \n","        Args:\n","            dfSingleCountry (DataFrame): A dataframe holding only one country\n","\n","        Returns:\n","            DataFrame: A data frame holding only one column to be appended to another data frame\n","        \"\"\"\n","        result = []\n","        quotient = []\n","        for index, value in dfSingleCountry['Cases'].iteritems():\n","            #  calculating the quotient conf[n] / conf[n-1]\n","            if index > 0 and index - 1 != 0:\n","                quotient.append(value / dfSingleCountry['Cases'][index - 1])\n","            else:\n","                quotient.append(math.nan)\n","            # calculates the doubling time (can't be calculated when there's \n","            # no change from one day to the other)\n","            if quotient[index] != 1 and quotient[index] != math.nan and quotient[index] != 0:\n","                result.append(math.log(2) / math.log(quotient[index]))\n","            else:\n","                result.append(math.nan)\n","        # return the dataframe\n","        return pd.DataFrame(np.asarray(result))\n","\n","    @staticmethod\n","    def create_combined_dataframe_by_geoid_string_list(dfList, geoIDs, lastNdays=0, sinceNcases=0): \n","        \"\"\"Creates a combined dataframe from a list of individual datafames. To avoid\n","        duplicate country names the method will add a '-DATASOURCE' string behind the \n","        country name (e.g. 'Germany-OWID'). \n","\n","        Args:\n","            dfList (tuple of DataFrame objects): A list of data frames\n","            geoIDs (str): A string of comma separated GeoIds that have to be included in all given data frames\n","            lastNdays (int, optional): Get the data only for the last N days. Defaults to 0.\n","            sinceNcases (int, optional): Get the data since the Nth. case has been exceeded. Defaults to 0.\n","\n","        Returns:\n","            [DataFrame]: The combined data frame\n","        \"\"\"         \n","        # a final array of dataframes containing all three data\n","        dfs = []\n","        # loop through all classes / geoIDs\n","        for obj in dfList:\n","            # get the data frame\n","            df = obj.get_data_by_geoid_string_list(geoIDs, lastNdays, sinceNcases)\n","            # rename the country and add the source info to the name\n","            for name in df['GeoName'].unique():\n","                df.replace(name, name + '-' + obj.get_data_source_info()[1], inplace=True)\n","            # add it to the list\n","            dfs.append(df)  \n","        # finally concatenate all dfs together\n","        df = pd.concat(dfs)  \n","        # ...and return it\n","        return df\n","\n","    def __build_cache(self, df, filenameCache, cacheLevel = 0):\n","        \"\"\"Builds a cache file for all countries (so far). A cache level defines how much data is generated\n","        for the cache. The higher the value the more data is created and the longer it takes to build the cache \n","\n","        Args:\n","            df (DataFrame): a dat frame holding the data to be processed to build the cache, that's typically\n","            the data frame of all countries\n","            filenameCache (str): the filename of the cache\n","            cacheLevel (int, optional): the amount of data to be calculated for the cache. Defaults to 2.\n","                0: there is no cache generated at all\n","                1: the cache includes a call to add_additional_attributes to include the following attributes:\n","                    Cases, Deaths, PercentDeaths, CasesPerMillionPopulation, DeathsPerMillionPopulation, DoublingTime\n","                2: includes the attributes of cache level 1 plus DailyCases7 + DailyDeaths7\n","                3: includes the attributes of cache level 2 plus R0\n","                4: includes the attributes of cache level 2 plus R7\n","\n","        Returns:\n","            DataFrame: The data frame containing all additional attributes\n","        \"\"\"\n","        print('building cache...')\n","        # verify the cache level\n","        cacheLevel = max(0, cacheLevel)\n","        cacheLevel = min(4, cacheLevel)\n","        # some benchmarking\n","        start = time.time()\n","        # build the cache\n","        dfs = []\n","        # get data for each country\n","        for geoID in df['GeoID'].unique():\n","            # get the data for a country and add the additional rows\n","            dfSingle = df.loc[df['GeoID'] == geoID].copy()\n","            # reverse the data frame to the newest date in the bottom\n","            dfSingle = dfSingle.reindex(index=dfSingle.index[::-1])\n","            # cacheLevel 1\n","            dfSingle = self.__add_additional_attributes(dfSingle)\n","            if cacheLevel > 1:\n","                # add 7day incidence\n","                dfSingle = self.add_incidence_7day_per_100Kpopulation(dfSingle)\n","                # add lowpass filtered DailyCases\n","                dfSingle = self.add_lowpass_filter_for_attribute(dfSingle, 'DailyCases', 7)\n","                # add lowpass filtered DailyDeaths\n","                dfSingle = self.add_lowpass_filter_for_attribute(dfSingle, 'DailyDeaths', 7)\n","            if cacheLevel > 2:\n","                # add r0\n","                dfSingle = self.add_r0(dfSingle)\n","            if cacheLevel > 3:\n","                # add lowpass filtered R\n","                dfSingle = self.add_lowpass_filter_for_attribute(dfSingle, \"R\", 7)\n","            dfs.append(dfSingle)\n","        # concatenate dataframe\n","        dfCache = pd.concat(dfs)\n","        # save it\n","        dfCache.to_csv(filenameCache, index = False, na_rep = '0')        \n","        # some benchmarking\n","        end = time.time()\n","        print('building cache...done: ' + str(end - start) + 's')\n","        return dfCache\n","\n","    def __add_additional_attributes(self, dfSingleCountry):\n","        \"\"\"Adds additional attributes to a dataframe of a single country.  \n","\n","        Args:\n","            dfSingleCountry (DataFrame): A dataframe holding only one country\n","\n","        Returns:\n","            DataFrame: The modified data frame of the country\n","        \"\"\"\n","        if dfSingleCountry.empty == True:\n","            return\n","        # check if the attributes have been generated already\n","        for col in dfSingleCountry.columns:\n","            if col == 'PercentDeaths':\n","                return dfSingleCountry\n","        # reset the index on the dataframe (if the argument is just a slice)\n","        dfSingleCountry.reset_index(inplace=True, drop=True)\n","        # the cumulative cases\n","        dfSingleCountry['Cases'] = dfSingleCountry['DailyCases'].cumsum()\n","        # the cumulative cases\n","        dfSingleCountry['Deaths'] = dfSingleCountry['DailyDeaths'].cumsum()\n","        # the percentage of deaths of the cumulative cases\n","        dfSingleCountry['PercentDeaths'] = pd.DataFrame({'PercentDeaths': dfSingleCountry['Deaths'] * 100.0 / dfSingleCountry['Cases']})\n","        # the percentage of cumulative cases of the 1 million population\n","        dfSingleCountry['CasesPerMillionPopulation'] = pd.DataFrame({'CasesPerMillionPopulation': dfSingleCountry['Cases'].div(dfSingleCountry['Population'].iloc[0] / 1000000)})\n","        # the percentage of cumulative deaths of 1 million population\n","        dfSingleCountry['DeathsPerMillionPopulation'] = pd.DataFrame({'DeathsPerMillionPopulation': dfSingleCountry['Deaths'].div(dfSingleCountry['Population'].iloc[0] / 1000000)})\n","        \n","        if self.get_data_source_info()[1] == 'OWID':\n","            # the percantage of people that received the first vaccination dose\n","            dfSingleCountry['PercentPeopleReceivedFirstDose'] = pd.DataFrame({'PercentPeopleReceivedFirstDose': dfSingleCountry['PeopleReceivedFirstDose'] * 100 / dfSingleCountry['Population'].iloc[0]})\n","            # the percantage of people that are fully vaccinated\n","            dfSingleCountry['PercentPeopleReceivedAllDoses'] = pd.DataFrame({'PercentPeopleReceivedAllDoses': dfSingleCountry['PeopleReceivedAllDoses'] * 100 / dfSingleCountry['Population'].iloc[0]})\n","        \n","        # adds the extra attributes\n","        dfSingleCountry['DoublingTime'] = self.__compute_doubling_time(dfSingleCountry)\n","        # now apply the country names from our internal list\n","        dfInfo = self.__dfGeoInformationWorld\n","        # return the manipulated dataframe\n","        return dfSingleCountry\n","\n","    def __apply_lowpass_filter(self, dfAttribute, n):\n","        \"\"\"Returns a dataframe containing the lowpass filtered (with depth n)\n","        data of the given dataframe.\n","\n","        Args:\n","            dfAttribute (DataFrame): The data frame to be filtered\n","            n (int): Width of the lowpass filter\n","\n","        Returns:\n","            DataFrame: A data frame holding only one column to be appended to another data frame\n","        \"\"\"\n","        result = []\n","        # iterate the attribute\n","        for index, value in dfAttribute.iteritems():\n","            # if the dataframe contains NaN, leave it untouched\n","            if math.isnan(value):\n","                result.append(math.nan)\n","                continue\n","            if index == 0:\n","                result.append(value)\n","            # for all rows below the nth row, calculate the lowpass filter up to this point\n","            elif index < n:\n","                result.append(sum(dfAttribute[0:index + 1]) / (index + 1))\n","            else:\n","                start = index - n + 1\n","                result.append(sum(dfAttribute[start:start + n]) / n)\n","        # return the calculated data as an array\n","        return pd.DataFrame(np.asarray(result))\n","\n","    def add_lowpass_filter_for_attribute(self, df, attribute, n):\n","        \"\"\"Adds a attribute to the df of each country that is the lowpass filtered\n","        data of the given attribute. The width of the lowpass is given by then\n","        number n. The name of the newly created attribute is the given name\n","        with a tailing number n. E.g. 'DailyCases' with n = 7 will add to a newly\n","        added attribute named 'Cases7'.\n","        If the attribute already exists the function will return the given df.\n","\n","        Args:\n","            df (DataFrame): The data frame holding all countries and all columns\n","            attribute (str): The name of the column to be processed\n","            n (int): The width of the lowpass filter\n","\n","        Returns:\n","            DataFrame: A data frame that includes the newly generated column\n","        \"\"\" \n","        # check if the attribute already exists\n","        requestedAttribute = attribute + str(n)\n","        for col in df.columns:\n","            if col == requestedAttribute:\n","                return df\n","        # get all GeoIDs in the df\n","        geoIDs = df['GeoID'].unique()\n","        # our result data frame\n","        dfs = []\n","        for geoID in geoIDs:\n","            # get the country dataframe\n","            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n","            # reset the index to start from index = 0\n","            dfSingleCountry.reset_index(inplace=True, drop=True)\n","            # add the lowpass filtered attribute\n","            dfSingleCountry[requestedAttribute] = self.__apply_lowpass_filter(dfSingleCountry[attribute], 7)\n","            # add the country to the result\n","            dfs.append(dfSingleCountry)\n","        return pd.concat(dfs)\n","\n","    def __apply_r0(self, dfCases):\n","        \"\"\"Returns a dataframe containing an estimation for the reproduction\n","        number R0 of the dataframe given. The given dataframe has to contain\n","        'DailyCases'.\n","\n","        Args:\n","            dfCases (DataFrame): The data frame to be processed\n","            \n","        Returns:\n","            DataFrame: A data frame holding only one column to be appended to another data frame\n","        \"\"\"\n","        # add the r0 attribute\n","        result = []\n","        # we will create 2 blocks and sum the data of each block\n","        blockSize = 4\n","        # iterate the cases\n","        for index, value in dfCases.iteritems():\n","            if index < 2 * blockSize - 1:\n","                # fill it with 0, do not use math.nan because of the cache\n","                result.append(0)\n","            else:\n","                # the sum of block 0\n","                start = index - (2 * blockSize - 1)\n","                sum0 = sum(dfCases[start: start + blockSize])\n","                # the sum of block 1\n","                start = index - (blockSize - 1)\n","                sum1 = sum(dfCases[start: start + blockSize])\n","                # and R\n","                if sum0 == 0:\n","                    # fill it with 0\n","                    R = 0\n","                else:\n","                    R = sum1 / sum0\n","                result.append(R)\n","        # return the calculated data as an array\n","        return pd.DataFrame(np.asarray(result))\n","\n","    def add_r0(self, df):\n","        \"\"\"Adds a attribute to the df of each country that is an estimation of the\n","        reproduction number R0. Here the number is called 'R'. The returned\n","        dataframe should finally lowpassed filtered with a kernel size of 1x7.\n","        If the attribute already exists the function will return the given df.\n","        \n","        Args:\n","            df (DataFrame): The data frame holding all countries and all columns\n","\n","        Returns:\n","            DataFrame: A data frame that includes the newly generated column\n","        \"\"\" \n","        # check if the attribute already exists\n","        requestedAttribute = 'R'\n","        for col in df.columns:\n","            if col == requestedAttribute:\n","                return df\n","        # get all GeoIDs in the df\n","        geoIDs = df['GeoID'].unique()\n","        # our result data frame\n","        dfs = []\n","        for geoID in geoIDs:\n","            # get the country dataframe\n","            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n","            # reset the index to start from index = 0\n","            dfSingleCountry.reset_index(inplace=True, drop=True)\n","            # add the lowpass filtered attribute\n","            dfSingleCountry[requestedAttribute] = self.__apply_r0(dfSingleCountry['DailyCases'])\n","            # add the country to the result\n","            dfs.append(dfSingleCountry)\n","        return pd.concat(dfs)\n","\n","    def __apply_incidence_7day_per_100Kpopulation(self, dfAttribute, dfPopulation):\n","        \"\"\"Returns a dataframe containing the accumulated 7 day incidence\n","        of the given dataframe containing only one country.\n","        \n","        Args:\n","            dfAttribute (DataFrame): The data frame holding the daily ne cases\n","            dfPopulation (DataFrame): A data frame holding the population\n","            \n","        Returns:\n","            DataFrame: A data frame holding only one column to be appended to another data frame\n","        \"\"\"\n","        result = []\n","        # iterate the attribute\n","        for index, value in dfAttribute.iteritems():\n","            # for all rows below the nth row, calculate the lowpass filter up to this point\n","            if index < 7:\n","                daysSum7 = sum(dfAttribute[0:index + 1]) * 7 / (index + 1)\n","                result.append(daysSum7  / (dfPopulation[index] / 100000))\n","            else:\n","                start = index - 7 + 1\n","                daysSum7 = sum(dfAttribute[start:start + 7])\n","                result.append(daysSum7 / (dfPopulation[index] / 100000))\n","        # return the calculated data as an array\n","        return pd.DataFrame(np.asarray(result))\n","\n","    def add_incidence_7day_per_100Kpopulation(self, df):\n","        \"\"\"Adds a attribute to the df of each country that is representing the\n","        accumulated 7-day incidence. That is the sum of the daily cases of \n","        the last 7 days divided by the population in 100000 people.\n","        If the attribute already exists the function will return the given df.\n","        \n","        Args:\n","            df (DataFrame): The data frame holding all countries and all columns\n","\n","        Returns:\n","            DataFrame: A data frame that includes the newly generated column\n","        \"\"\" \n","        # check if the attribute exists\n","        requestedAttribute = 'Incidence7DayPer100Kpopulation'\n","        for col in df.columns:\n","            if col == requestedAttribute:\n","                return df\n","        # get all GeoIDs in the df\n","        geoIDs = df['GeoID'].unique()\n","        # our result data frame\n","        dfs = []\n","        for geoID in geoIDs:\n","            # get the country dataframe\n","            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n","            # reset the index to start from index = 0\n","            dfSingleCountry.reset_index(inplace=True, drop=True)\n","            # add the lowpass filtered attribute\n","            dfSingleCountry[requestedAttribute] = self.__apply_incidence_7day_per_100Kpopulation(dfSingleCountry['DailyCases'], dfSingleCountry['Population'])\n","            # add the country to the result\n","            dfs.append(dfSingleCountry)\n","        return pd.concat(dfs)\n","\n","    def save_df_to_csv(self, df, filename):\n","        \"\"\"Saves a df to a CSV file\n","\n","        Args:\n","            df (DataFrame): The data frame holding all countries and all columns\n","            filename (str): The name of the output file\n","        \"\"\"       \n","        df.to_csv(filename)\n","\n","    def get_data_by_geoid_list(self, geoIDs, lastNdays=0, sinceNcases=0):\n","        \"\"\"Return the dataframe by a list of geoIDs. Refer to the CSV\n","        file for a list of available GeoIDs and CountryNames.\n","\n","        Args:\n","            geoIDs (list): A list of strings holding the GeoIds\n","            lastNdays (int, optional): Get the data only for the last N days. Defaults to 0.\n","            sinceNcases (int, optional): Get the data since the Nth. case has been exceeded. Defaults to 0.\n","\n","        Raises:\n","            ValueError: In case that both optional arguments have been used (>0) \n","\n","        Returns:\n","            DataFrame: A data frame holding the information of the selected countries\n","        \"\"\"\n","        # correct potentially incorrect lists\n","        geoIDs = self.review_geoid_list(geoIDs)\n","        # check if only one optional parameter is used\n","        if lastNdays > 0 and sinceNcases > 0:\n","            raise ValueError(\"Only one optional parameter allowed!\")\n","        # our result data frame\n","        dfs = []\n","        # get data for each country\n","        for geoID in geoIDs:\n","            # get the data for a country and add the additional rows\n","            df = self.__df.loc[self.__df['GeoID'] == geoID].copy()\n","            # reverse the data frame to the newest date in the bottom\n","            df = df.reindex(index=df.index[::-1])\n","            df.head()\n","            df = self.__add_additional_attributes(df)\n","            # if lastNdays is specified just return these last n days\n","            if lastNdays > 0:\n","                df = df.tail(lastNdays)\n","            # if sinceNcases is specified calculate the start index\n","            if sinceNcases > 0:\n","                start = -1\n","                for index, val in df['Cases'].iteritems():\n","                    if val >= sinceNcases:\n","                        start = index\n","                        break\n","                # an illegal input will cause an exception\n","                if start == -1:\n","                    raise ValueError(\"Number of cases wasn't that high!\")\n","                # copy the data\n","                df = df.iloc[start:].copy()\n","                # reset the index on the remaining data points so that they\n","                # start at zero\n","                df.reset_index(inplace=True, drop=True)\n","            # append this dataframe to our result\n","            dfs.append(df)\n","        # return the concatenated dataframe\n","        return pd.concat(dfs)\n","\n","    def get_data_by_geoid_string_list(self, geoIDstringList, lastNdays=0, sinceNcases=0):\n","        \"\"\"Return the dataframe by a comma separated list of geoIDs. Refer to the CSV\n","        file for a list of available GeoIDs and CountryNames.\n","\n","        Args:\n","            geoIDs (str): A string of comma separated GeoIds\n","            lastNdays (int, optional): Get the data only for the last N days. Defaults to 0.\n","            sinceNcases (int, optional): Get the data since the Nth. case has been exceeded. Defaults to 0.\n","\n","        Raises:\n","            ValueError: In case that both optional arguments have been used (>0) \n","\n","        Returns:\n","            DataFrame: A data frame holding the information of the selected countries\n","        \"\"\"\n","        # split the string\n","        geoIDs = re.split(r',\\s*', geoIDstringList.upper())\n","        # return the concatenated dataframe\n","        return self.get_data_by_geoid_list(geoIDs, lastNdays, sinceNcases)\n","\n","    def get_all_data(self):\n","        \"\"\"Return the dataframe of all countries in the database.\n","        \n","        Returns:\n","            DataFrame: A data frame holding the information of all countries in the file\n","        \"\"\"\n","        # return all countries, but first add the extra columns\n","        return self.get_data_by_geoid_list(self.__df['GeoID'].unique())\n","\n","    @abstractmethod\n","    def get_available_GeoID_list(self):\n","        \"\"\"\n","        Returns a dataframe having just two columns for the GeoID and region/country or whatever name.  \n","        Needs to be implemented by all sub-classes derived from this.\n","\n","        Returns:\n","            Dataframe: A dataframe having two columns: The country name and GeoID\n","        \"\"\"\n","        pass \n","\n","    @abstractmethod\n","    def get_data_source_info(self):\n","        \"\"\"\n","        Returns a dataframe containing information about the data source. The dataframe holds 3 columns:\n","        InfoFullName: The full name of the data source\n","        InfoShortName: A shortname for the data source\n","        InfoLink: The link to get the data\n","\n","        Returns:\n","            Dataframe: A dataframe holding the information\n","        \"\"\"\n","        pass \n","\n","    @abstractmethod\n","    def review_geoid_list(self, geoIDs):\n","        \"\"\"\n","        Returns a corrected version of the given geoID list to ensure that cases of mismatches like UK-GB are corrected by the sub-class.  \n","        geoIDs: The list holding the geoIDs as requested such as ['DE', 'UK']\n","\n","        Returns:\n","            list: A corrected list such as ['DE', 'GB'] that translates incorrect country codes to corrected codes \n","        \"\"\"\n","        pass \n","\n","    \n","class CovidCasesWHO(CovidCases):\n","    \"\"\"The class will expose data attributes in form of a DataFrame. Its base class also provides methods to process \n","    the data which will end up in additional columns in the DataFrame. These are the name sof the columns\n","    that are generated. Notice: The 'Continent' column is additionally and specific to this sub class.\n","\n","    ATTENTION: The CovidCasesWHOv1 class is a older version of this class and it will load 50% slower. Both classes\n","               produce the same results\n","\n","    Date\n","    The date of the data \n","    \n","    GeoID\n","    The GeoID of the country such as FR for France or DE for Germany\n","\n","    GeoName\n","    The name of the country\n","\n","    Continent\n","    The continent of the country\n","\n","    Population\n","    The population of the country\n","\n","    DailyCases\n","    The number of new cases on a given day\n","\n","    DailyDeaths\n","    The number of new deaths on the given date\n","\n","    Continent\n","    The continent of the country as an additional column\n","    \n","    Returns:\n","        CovidCasesWHO: A class to provide access to some data based on the WHO file.\n","    \"\"\"\n","\n","    def __init__(self, filename, cacheLevel = 0):\n","        \"\"\"The constructor takes a string containing the full filename of a CSV\n","        database you can download from the WHO website:\n","        https://covid19.who.int/WHO-COVID-19-global-data.csv\n","        The database will be loaded and kept as a private member. If there is a cache\n","        file containing pre-calculated attributes it will be loaded instead of the \n","        downloaded WHO file. If there is no cache file available it may force the base \n","        class to build such a cache at the given cache level. A cache file is detected \n","        by having and tailing '-cache.csv'.\n","        To retrieve the data for an individual country you can use the public methods\n","        GetCountryDataByGeoID or GetCountryDataByCountryName. These functions take \n","        ISO 3166 alpha_2 (2 characters long) GeoIDs.\n","\n","        Args:\n","            filename (str): The full path and name of the csv file. \n","            cacheLevel (int, optional): the amount of data to be calculated for the cache. Defaults to 0.\n","                refer to CovidCase.__build_cache for more information of the different cache levels\n","        \"\"\"\n","        # some benchmarking\n","        start = time.time()\n","        # use a cache if it exists\n","        filenameCache = os.path.splitext(filename)[0] + '-cache.csv'\n","        if os.path.exists(filenameCache):\n","            print('using cache file: ' + filenameCache)\n","             # open the file\n","            self.__df = pd.read_csv(filenameCache, keep_default_na=False)\n","            # change the type of the 'date' field to a pandas date\n","            self.__df['Date'] = pd.to_datetime(self.__df['Date'],\n","                                               format='%Y-%m-%d')\n","            # now ensure the data layout\n","            dfs = []\n","            for geoID in self.__df['GeoID'].unique():\n","                # get the data for a country\n","                dfSingleCountry = self.__df.loc[self.__df['GeoID'] == geoID].copy()\n","                # reset the index\n","                dfSingleCountry.reset_index(inplace=True, drop=True)\n","                dfSingleCountry.head()\n","                # re-order it from newest to olders (top-bottom)\n","                dfSingleCountry = dfSingleCountry.reindex(index=dfSingleCountry.index[::-1])\n","                # append this dataframe to our result\n","                dfs.append(dfSingleCountry)\n","            # keep the concatenated dataframe\n","            self.__df = pd.concat(dfs)\n","            # some benchmarking\n","            end = time.time()\n","            print('Pandas loading the cached WHO CSV: ' + str(end - start) + 's')\n","            # pass the dataframe to the base class\n","            super().__init__(self.__df)\n","            return\n","        # open the file\n","        self.__df = pd.read_csv(filename, keep_default_na=False)\n","        # drop some columns\n","        self.__df = self.__df.drop(columns=['WHO_region',\n","                                            'Cumulative_cases',\n","                                            'Cumulative_deaths'])\n","        # rename the columns to be more readable\n","        self.__df.columns = ['Date',\n","                             'GeoID',\n","                             'GeoName',\n","                             'DailyCases',\n","                             'DailyDeaths']\n","        \n","        # now apply the country names from our internal list\n","        giw = GeoInformationWorld()\n","        # get all country info\n","        dfInfo = giw.get_geo_information_world()\n","        # our result data frame\n","        dfs = []\n","        for geoID in self.__df['GeoID'].unique():\n","            # 'other' fix\n","            if geoID == ' ':\n","                continue\n","            # 'Saba' fix\n","            if geoID == 'XC':\n","                continue\n","            # Sint Eustatius\n","            if geoID == 'XB':\n","                continue\n","            # American Samoa\n","            if geoID == 'AS':\n","                continue\n","            # Korea, People's Republic\n","            if geoID == 'KP':\n","                continue\n","            # French Guinea\n","            if geoID == 'GF':\n","                continue\n","            # Guadeloupe\n","            if geoID == 'GP':\n","                continue\n","            # Kiribati\n","            if geoID == 'KI':\n","                continue\n","            # Martinique\n","            if geoID == 'MQ':\n","                continue\n","            # Mayotte\n","            if geoID == 'YT':\n","                continue\n","            # Micronesia \n","            if geoID == 'FM':\n","                continue\n","            # Nauru\n","            if geoID == 'NR':\n","                continue\n","            # Niue\n","            if geoID == 'NU':\n","                continue\n","            # Palau\n","            if geoID == 'PW':\n","                continue\n","            # Pitcairn Islands\n","            if geoID == 'PN':\n","                continue\n","            # Réunion\n","            if geoID == 'RE':\n","                continue\n","            # Saint Barthélemy\n","            if geoID == 'BL':\n","                continue\n","            # Saint Helena\n","            if geoID == 'SH':\n","                continue\n","            # Saint Martin\n","            if geoID == 'MF':\n","                continue\n","            # Saint Pierre and Miquelon\n","            if geoID == 'PM':\n","                continue\n","            # Turkmenistan\n","            if geoID == 'TM':\n","                continue\n","            # Tokelau\n","            if geoID == 'TK':\n","                continue\n","            # Tonga\n","            if geoID == 'TO':\n","                continue\n","            # Tuvalu\n","            if geoID == 'TV':\n","                continue\n","\n","            # get the data for a country and add the additional rows\n","            dfSingleCountry = self.__df.loc[self.__df['GeoID'] == geoID].copy()\n","            # reset the index\n","            dfSingleCountry.reset_index(inplace=True, drop=True)\n","            dfSingleCountry.head()\n","            # Bonaire workaround\n","            if geoID == 'XA':\n","                geoID = 'BQ'\n","                dfSingleCountry['GeoID'] = [geoID for _ in range(0, len(dfSingleCountry['GeoID']))]    \n","            # get the geoName for this geoID from our internal list\n","            geoName = giw.geo_name_from_geoid(geoID)\n","            # the current name         \n","            curName = dfSingleCountry['GeoName'][0]\n","            # replace it if necessary\n","            if geoName != curName:\n","                dfSingleCountry['GeoName'] = [geoName for _ in range(0, len(dfSingleCountry['GeoID']))]\n","            # get the continent for this geoID from our internal list\n","            continent = giw.continent_from_geoid(geoID)\n","            # apply it to this country\n","            dfSingleCountry['Continent'] = [continent for _ in range(0, len(dfSingleCountry['GeoID']))]\n","            # get the population for this geoID from our internal list\n","            population = giw.population_from_geoid(geoID)\n","            # apply it to this country\n","            dfSingleCountry['Population'] = [population for _ in range(0, len(dfSingleCountry['GeoID']))]\n","            # re-order it from newest to olders (top-bottom)\n","            dfSingleCountry = dfSingleCountry.reindex(index=dfSingleCountry.index[::-1])\n","            # append this dataframe to our result\n","            dfs.append(dfSingleCountry)\n","        # keep the concatenated dataframe\n","        self.__df = pd.concat(dfs)\n","        # re-order the columns to be similar for all sub-classes                                   \n","        self.__df = self.__df[['Date', \n","                              'GeoName', \n","                              'GeoID', \n","                              'Population', \n","                              'Continent', \n","                              'DailyCases',\n","                              'DailyDeaths']]\n","        # change the type of the 'date' field to a pandas date\n","        self.__df['Date'] = pd.to_datetime(self.__df['Date'],\n","                                           format='%Y-%m-%d')\n","        # some benchmarking\n","        end = time.time()\n","        print('Pandas loading the WHO CSV: ' + str(end - start) + 's')\n","        # pass the dataframe to the base class\n","        if ((filenameCache != '') and (cacheLevel > 0)):\n","            # force the base class to build and save the cache\n","            super().__init__(self.__df, filenameCache, cacheLevel)\n","        else:\n","            super().__init__(self.__df)\n","\n","    @staticmethod\n","    def download_CSV_file():\n","        \"\"\"automatically downloads the database file if it doesn't exists. Need\n","        to be called in a try-catch block as it may throw FileNotFoundError or\n","        IOError errors\n","\n","        Raises:\n","            FileNotFoundError: In case it couldn't download the file\n","\n","        Returns:\n","            str: The filename of the database wether it has been downloaded or not.\n","        \"\"\"\n","        # todays date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-WHO\"\n","        # the target file\n","        targetFilename = '/content/data/' + preFix + '-db.csv'\n","        # check if it exist already\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","        else:\n","            # download the file from the who server\n","            url = 'https://covid19.who.int/WHO-COVID-19-global-data.csv'\n","            r = requests.get(url, timeout=1.0)\n","            if r.status_code == requests.codes.ok:\n","                with open(targetFilename, 'wb') as f:\n","                    f.write(r.content)\n","            else:\n","                raise FileNotFoundError('Error getting CSV file. Error code: ' + str(r.status_code))\n","        return targetFilename\n","\n","    def get_available_GeoID_list(self):\n","        \"\"\"Returns a dataframe having just two columns for the GeoID and Country name\n","\n","        Returns:\n","            Dataframe: A dataframe having two columns: The country name and GeoID\n","        \"\"\" \n","        # the list of GeoIDs in the dataframe\n","        geoIDs = self.__df['GeoID'].unique()\n","        # the list of country names in the dataframe\n","        countries = self.__df['GeoName'].unique()\n","        # merge them together\n","        list_of_tuples = list(zip(geoIDs, countries))\n","        # create a dataframe out of the list\n","        dfResult = pd.DataFrame(list_of_tuples, columns=['GeoID', 'GeoName'])\n","        return dfResult\n","\n","    def get_data_source_info(self):\n","        \"\"\"\n","        Returns a list containing information about the data source. The list holds 3 strings:\n","        InfoFullName: The full name of the data source\n","        InfoShortName: A shortname for the data source\n","        InfoLink: The link to get the data\n","\n","        Returns:\n","            Dataframe: A dataframe holding the information\n","        \"\"\"\n","        info = [\"World Health Organization\", \n","                \"WHO\",\n","                \"https://covid19.who.int/WHO-COVID-19-global-data.csv\"]\n","        return info\n","\n","    def review_geoid_list(self, geoIDs):\n","        \"\"\"\n","        Returns a corrected version of the given geoID list to ensure that cases of mismatches like UK-GB are corrected by the sub-class.  \n","        geoIDs: The list holding the geoIDs as requested such as ['DE', 'UK']\n","\n","        Returns:\n","            list: A corrected list such as ['DE', 'GB'] that translates incorrect country codes to corrected codes \n","        \"\"\"\n","        # fix the ECDC mistakes and map e.g. UK to GB \n","        corrected = []\n","        for geoID in geoIDs:\n","            if geoID == 'UK':\n","                corrected.append('GB')\n","            elif geoID == 'EL':\n","                corrected.append('GR')\n","            elif geoID == 'TW':\n","                corrected.append('CN')\n","            else:\n","                corrected.append(geoID)\n","        return corrected\n","\n","    @staticmethod\n","    def get_pygal_european_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of European countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesWHO.get_pygal_european_geoid_string_list().upper())\n","        return geoIDs\n","\n","    @staticmethod\n","    def get_pygal_european_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of European countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main european countries for a map, pygal doesn't contain e.g. \n","        # Andorra, Kosovo (XK)\n","        geoIdList = 'AM, AL, AZ, AT, BA, BE, BG, BY, CH, CY, CZ, ' + \\\n","                    'DE, DK, EE, GR, ES, FI, FR, GE, GL, '  + \\\n","                    'HU, HR, IE, IS, IT, LV, LI, LT, ' + \\\n","                    'MD, ME, MK, MT, NL, NO, PL, PT, ' + \\\n","                    'RU, SE, SI, SK, RO, UA, GB, RS'\n","        return geoIdList\n","\n","    @staticmethod\n","    def get_pygal_american_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of American countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesWHO.get_pygal_american_geoid_string_list().upper())\n","        return geoIDs\n","        \n","    @staticmethod\n","    def get_pygal_american_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of American countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main american countries for a map, pygal doesn't contain e.g. \n","        # Bahamas (BS), Barbados (BB), Bermuda (BM), Falkland Island (FK)\n","        # 2022-01-22 added BZ\n","        geoIdList = 'AR, BB, BM, BO, BR, BS, CA, CL, CO, ' + \\\n","                    'CR, CU, DO, EC, SV, GT, GY, HN, HT, ' + \\\n","                    'JM, MX, NI, PA, PE, PR, PY, SR, US, UY, VE, BZ'\n","        return geoIdList\n","\n","    @staticmethod\n","    def get_pygal_asian_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of Asian countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesWHO.get_pygal_asian_geoid_string_list().upper())\n","        return geoIDs\n","        \n","    @staticmethod\n","    def get_pygal_asian_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of Asian countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main asian countries for a map, pygal doesn't contain e.g. \n","        # Qatar (QA)\n","        geoIdList = 'AF, BH, BD, BT, BN, KH, CN, IR, IQ, IL, JP, JO, '  + \\\n","                    'KZ, KW, KG, LA, LB, MY, MV, MN, MM, NP, OM, PK, PS, PH, '  + \\\n","                    'QA, SA, SG, KR, LK, SY, TJ, TH, TL, TR, AE, UZ, VN, YE, IN, ID'\n","        return geoIdList\n","    \n","    @staticmethod\n","    def get_pygal_african_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of African countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesWHO.get_pygal_african_geoid_string_list().upper())\n","        return geoIDs\n","        \n","    @staticmethod\n","    def get_pygal_african_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of African countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main african countries for a map, pygal doesn't contain e.g. \n","        # Comoros (KM)\n","        # 2022-01-22 added NA\n","        geoIdList = 'DZ, AO, BJ, BW, BF, BI, CM, CV, CF, TD, KM, CG, CI, CD, '  + \\\n","                    'DJ, EG, GQ, ER, SZ, ET, GA, GM, GH, GN, GW, KE, LS, LR, '  + \\\n","                    'LY, MG, MW, ML, MR, MU, MA, MZ, NE, NG, RW, ST, SN, SC, '  + \\\n","                    'SL, SO, ZA, SS, SD, TG, TN, UG, TZ, EH, ZM, ZW, NA'\n","        return geoIdList\n","\n","    @staticmethod\n","    def get_pygal_oceania_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of Oceanian countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesWHO.get_pygal_oceania_geoid_string_list().upper())\n","        return geoIDs\n","\n","    @staticmethod\n","    def get_pygal_oceania_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of Oceanian countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main oceania countries for a map, pygal doesn't contain e.g. \n","        # Comoros (KM)\n","        geoIdList = 'AU, FJ, PF, GU, NC, NZ, MP, PG'\n","        return geoIdList\n","\n","    \n","class CovidCasesOWID(CovidCases):\n","    \"\"\"The class will expose data attributes in form of a DataFrame. Its base class also provides methods to process \n","    the data which will end up in additional columns in the DataFrame. These are the name sof the columns\n","    that are generated. Notice: The 'Continent' column is additionally and specific to this sub class\n","\n","    Date\n","    The date of the data \n","    \n","    GeoID\n","    The GeoID of the country such as FR for France or DE for Germany\n","\n","    GeoName\n","    The name of the country\n","\n","    Population\n","    The population of the country\n","\n","    Continent\n","    The continent of the country\n","\n","    DailyCases\n","    The number of new cases on a given day\n","\n","    DailyDeaths\n","    The number of new deaths on the given date\n","\n","    DailyVaccineDosesAdministered7DayAverage\n","    New COVID-19 vaccination doses administered (7-day smoothed). For countries that \n","    don't report vaccination data on a daily basis, we assume that vaccination \n","    changed equally on a daily basis over any periods in which no data was reported. \n","    This produces a complete series of daily figures, which is then averaged over a \n","    rolling 7-day window. \n","    In OWID words this is the new_vaccinations_smoothed value.\n","                              \n","    PeopleReceivedFirstDose\n","    Total number of people who received at least one vaccine dose.\n","    In OWID words this is the people_vaccinated value.\n","\n","    PeopleReceivedAllDoses\n","    Total number of people who received all doses prescribed by the vaccination protocol.\n","    In OWID words this is the people_fully_vaccinated value.\n","\n","    VaccineDosesAdministered\n","    Total number of COVID-19 vaccination doses administered. It's the sum of \n","    PeopleReceivedFirstDose and PeopleReceivedAllDoses.\n","    In OWID words this is the total_vaccinations value.\n","\n","    Continent\n","    The continent of the country as an additional column.\n","    \n","    Returns:\n","        CovidCasesOWID: A class to provide access to some data based on the OWID file.\n","    \"\"\"\n","\n","    def __init__(self, filename):\n","        \"\"\"The constructor takes a string containing the full filename of a CSV\n","        database you can download from the OWID website:\n","        https://covid.ourworldindata.org/data/owid-covid-data.csv\n","        The database will be loaded and kept as a private member. To retrieve the\n","        data for an individual country you can use the public methods\n","        GetCountryDataByGeoID or GetCountryDataByCountryName. These functions take \n","        ISO 3166 alpha_2 (2 characters long) GeoIDs.\n","\n","        Args:\n","            filename (str): The full path and name of the csv file. \n","        \"\"\"\n","        # some benchmarking\n","        start = time.time()\n","        # open the file\n","        self.__df = pd.read_csv(filename)\n","        # remove columns that we don't need\n","        self.__df = self.__df.drop(columns=['total_cases', \n","                                            'new_cases_smoothed', \n","                                            'total_deaths', \n","                                            'new_deaths_smoothed', \n","                                            'total_cases_per_million',\n","                                            'new_cases_per_million',\n","                                            'new_cases_smoothed_per_million',\n","                                            'total_deaths_per_million',\n","                                            'new_deaths_per_million',\n","                                            'new_deaths_smoothed_per_million',\n","                                            'reproduction_rate',\n","                                            'icu_patients',\n","                                            'icu_patients_per_million',\n","                                            'hosp_patients',\n","                                            'hosp_patients_per_million',\n","                                            'weekly_icu_admissions',\n","                                            'weekly_icu_admissions_per_million',\n","                                            'weekly_hosp_admissions',\n","                                            'weekly_hosp_admissions_per_million',\n","                                            'new_tests',\n","                                            'total_tests',\n","                                            'total_tests_per_thousand',\n","                                            'new_tests_per_thousand',\n","                                            'new_tests_smoothed',\n","                                            'new_tests_smoothed_per_thousand',\n","                                            'positive_rate',\n","                                            'tests_per_case',\n","                                            'tests_units',\n","                                            #'total_vaccinations',\n","                                            'total_vaccinations_per_hundred',\n","                                            'stringency_index',\n","                                            'population_density',\n","                                            'median_age',\n","                                            'aged_65_older',\n","                                            'aged_70_older',\n","                                            'gdp_per_capita',\n","                                            'extreme_poverty',\n","                                            'cardiovasc_death_rate',\n","                                            'diabetes_prevalence',\n","                                            'female_smokers',\n","                                            'male_smokers',\n","                                            'handwashing_facilities',\n","                                            'hospital_beds_per_thousand',\n","                                            'life_expectancy',\n","                                            'human_development_index',\n","                                            # three more columns have been introduced\n","                                            'new_vaccinations',\n","                                            #'new_vaccinations_smoothed',\n","                                            'new_vaccinations_smoothed_per_million',\n","                                            #'people_fully_vaccinated',\n","                                            'people_fully_vaccinated_per_hundred',\n","                                            #'people_vaccinated',\n","                                            'people_vaccinated_per_hundred',\n","                                            # again a new field\n","                                            'excess_mortality',\n","                                            # and of course some new fields\n","                                            'total_boosters',\n","                                            'total_boosters_per_hundred',\n","                                            # some more\n","                                            'excess_mortality_cumulative_absolute',\n","                                            'excess_mortality_cumulative',\n","                                            'excess_mortality_cumulative_per_million',\n","                                            'excess_mortality',\n","                                            'new_people_vaccinated_smoothed',\n","                                            'new_people_vaccinated_smoothed_per_hundred'])\n","        if self.__df.columns.size != 11:\n","            # oops, there are some new columns in the csv\n","            print('Detecting new cols in OWID CSV: ' + self.__df.columns)\n","            # add the new cols to a list\n","            cols = [self.__df.columns[col] for col in range (11, self.__df.columns.size)]\n","            # ...and drop them\n","            self.__df = self.__df.drop(columns=cols)\n","            print('Accepting cols in OWID CSV: ' + self.__df.columns)\n","        # rename the columns to be more readable\n","        self.__df.columns = ['GeoID',\n","                             'Continent',\n","                             'GeoName',\n","                             'Date',\n","                             'DailyCases',\n","                             'DailyDeaths',\n","                             'VaccineDosesAdministered',\n","                             'PeopleReceivedFirstDose',\n","                             'PeopleReceivedAllDoses',\n","                             'DailyVaccineDosesAdministered7DayAverage',\n","                             'Population']\n","        #print(self.__df.columns)\n","        # change the type of the 'date' field to a pandas date\n","        self.__df['Date'] = pd.to_datetime(self.__df['Date'],\n","                                           format='%Y/%m/%d')\n","        # re-order the columns to be similar for all sub-classes                                   \n","        self.__df = self.__df[['Date', \n","                              'GeoName', \n","                              'GeoID', \n","                              'Population', \n","                              'Continent', \n","                              'DailyCases',\n","                              'DailyDeaths',\n","                              'DailyVaccineDosesAdministered7DayAverage',\n","                              'PeopleReceivedFirstDose',\n","                              'PeopleReceivedAllDoses',\n","                              'VaccineDosesAdministered']]\n","        #print(self.__df)\n","        df = self.__df\n","        # to apply the country names from our internal list\n","        giw = GeoInformationWorld()\n","        # get all country info\n","        dfInfo = giw.get_geo_information_world()\n","        # we need the newest date being on top, get all GeoIDs in the df\n","        geoIDs = df['GeoID'].unique()\n","        # our result data frame\n","        dfs = []\n","        for geoID in geoIDs:\n","            # 'nan' workaround\n","            if str(geoID) == 'nan':\n","                # nothing else worked to detect this nan (it's the 'international' line in the file that doesn't have any GeoIds)\n","                continue\n","            # get the country dataframe\n","            dfSingleCountry = df.loc[df['GeoID'] == geoID].copy()\n","            # reset the index to start from index = 0\n","            dfSingleCountry.reset_index(inplace=True, drop=True)\n","            dfSingleCountry = dfSingleCountry.reindex(index=dfSingleCountry.index[::-1])  \n","            # 'Kosovo' workaround\n","            if geoID == 'OWID_KOS':\n","                geoID = 'KOS'\n","            # 'OWID World' workaround\n","            if geoID == 'OWID_WRL':\n","                continue\n","            # get the geoName for this geoID from our internal list\n","            geoName = giw.geo_name_from_ISO3166_alpha_3(geoID)\n","            # get the alpha-2 geoID from the alpha-3 geoID\n","            geoID2 = giw.geoID_from_ISO3166_alpha_3(geoID)\n","            # the current name         \n","            curName = dfSingleCountry['GeoName'][0]\n","            # replace it if necessary\n","            if geoName != curName:\n","                dfSingleCountry['GeoName'] = [geoName for _ in range(0, len(dfSingleCountry['GeoID']))]\n","            # now overwrite the alpha-3 geoID with the alpha-2 geoID so all sublasses can use the same geoIDs\n","            dfSingleCountry['GeoID'] = [geoID2 for _ in range(0, len(dfSingleCountry['GeoID']))]    \n","            # add the country to the result\n","            dfs.append(dfSingleCountry)\n","        # done, keep the list\n","        self.__df = pd.concat(dfs)\n","        # some benchmarking\n","        end = time.time()\n","        print('Pandas loading the OWID CSV: ' + str(end - start) + 's')\n","        # pass the dataframe to the base class\n","        super().__init__(self.__df)\n","\n","    @staticmethod\n","    def download_CSV_file():\n","        \"\"\"automatically downloads the database file if it doesn't exists. Need\n","        to be called in a try-catch block as it may throw FileNotFoundError or\n","        IOError errors\n","\n","        Raises:\n","            FileNotFoundError: In case it couldn't download the file\n","\n","        Returns:\n","            str: The filename of the database wether it has been downloaded or not.\n","        \"\"\"\n","        # todays date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-OWID\"\n","        # the target file\n","        targetFilename = '/content/data/' + preFix + '-db.csv'\n","        # check if it exist already\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","        else:\n","            # download the file from the ecdc server\n","            url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'\n","            r = requests.get(url, timeout=1.0)\n","            if r.status_code == requests.codes.ok:\n","                with open(targetFilename, 'wb') as f:\n","                    f.write(r.content)\n","            else:\n","                raise FileNotFoundError('Error getting CSV file. Error code: ' + str(r.status_code))\n","        return targetFilename\n","\n","    def get_available_GeoID_list(self):\n","        \"\"\"Returns a dataframe having just two columns for the GeoID and Country name\n","\n","        Returns:\n","            Dataframe: A dataframe having two columns: The country name and GeoID\n","        \"\"\" \n","        # the list of GeoIDs in the dataframe\n","        geoIDs = self.__df['GeoID'].unique()\n","        # the list of country names in the dataframe\n","        countries = self.__df['GeoName'].unique()\n","        # merge them together\n","        list_of_tuples = list(zip(geoIDs, countries))\n","        # create a dataframe out of the list\n","        dfResult = pd.DataFrame(list_of_tuples, columns=['GeoID', 'GeoName'])\n","        return dfResult\n","\n","    def get_data_source_info(self):\n","        \"\"\"\n","        Returns a list containing information about the data source. The list holds 3 strings:\n","        InfoFullName: The full name of the data source\n","        InfoShortName: A shortname for the data source\n","        InfoLink: The link to get the data\n","\n","        Returns:\n","            Dataframe: A dataframe holding the information\n","        \"\"\"\n","        info = [\"Our World In Data\", \n","                \"OWID\",\n","                \"https://covid.ourworldindata.org/data/owid-covid-data.csv\"]\n","        return info\n","\n","    def review_geoid_list(self, geoIDs):\n","        \"\"\"\n","        Returns a corrected version of the given geoID list to ensure that cases of mismatches like UK-GB are corrected by the sub-class.  \n","        geoIDs: The list holding the geoIDs as requested such as ['DE', 'UK']\n","\n","        Returns:\n","            list: A corrected list such as ['DE', 'GB'] that translates incorrect country codes to corrected codes \n","        \"\"\"\n","        # fix the ECDC mistakes and map e.g. UK to GB \n","        corrected = []\n","        for geoID in geoIDs:\n","            if geoID == 'UK':\n","                corrected.append('GB')\n","            elif geoID == 'EL':\n","                corrected.append('GR')\n","            else:\n","                corrected.append(geoID)\n","        return corrected\n","\n","    @staticmethod\n","    def get_pygal_european_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of European countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesOWID.get_pygal_european_geoid_string_list().upper())\n","        return geoIDs\n","\n","    @staticmethod\n","    def get_pygal_european_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of European countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main european countries for a map, pygal doesn't contain e.g. \n","        # Andorra, Kosovo (XK)\n","        geoIdList = 'AM, AL, AZ, AT, BA, BE, BG, BY, CH, CY, CZ, ' + \\\n","                    'DE, DK, EE, GR, ES, FI, FR, GE, GL, '  + \\\n","                    'HU, HR, IE, IS, IT, LV, LI, LT, ' + \\\n","                    'MD, ME, MK, MT, NL, NO, PL, PT, ' + \\\n","                    'RU, SE, SI, SK, RO, UA, GB, RS'\n","        return geoIdList\n","\n","    @staticmethod\n","    def get_pygal_american_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of American countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesOWID.get_pygal_american_geoid_string_list().upper())\n","        return geoIDs\n","        \n","    @staticmethod\n","    def get_pygal_american_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of American countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main american countries for a map, pygal doesn't contain e.g. \n","        # Bahamas (BS), Barbados (BB), Bermuda (BM), Falkland Island (FK)\n","        geoIdList = 'AR, BB, BM, BO, BR, BS, CA, CL, CO, ' + \\\n","                    'CR, CU, DO, EC, SV, GT, GY, HN, HT, ' + \\\n","                    'JM, MX, NI, PA, PE, PR, PY, SR, US, UY, VE'\n","        return geoIdList\n","\n","    @staticmethod\n","    def get_pygal_asian_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of Asian countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesOWID.get_pygal_asian_geoid_string_list().upper())\n","        return geoIDs\n","        \n","    @staticmethod\n","    def get_pygal_asian_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of Asian countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main asian countries for a map, pygal doesn't contain e.g. \n","        # Qatar (QA)\n","        geoIdList = 'AF, BH, BD, BT, BN, KH, CN, IR, IQ, IL, JP, JO, '  + \\\n","                    'KZ, KW, KG, LA, LB, MY, MV, MN, MM, NP, OM, PK, PS, PH, '  + \\\n","                    'QA, SA, SG, KR, LK, SY, TW, TJ, TH, TL, TR, AE, UZ, VN, YE, IN, ID'\n","        return geoIdList\n","    \n","    @staticmethod\n","    def get_pygal_african_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of African countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesOWID.get_pygal_african_geoid_string_list().upper())\n","        return geoIDs\n","        \n","    @staticmethod\n","    def get_pygal_african_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of African countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main african countries for a map, pygal doesn't contain e.g. \n","        # Comoros (KM)\n","        # 2022-01-20 added NA\n","        geoIdList = 'DZ, AO, BJ, BW, BF, BI, CM, CV, CF, TD, KM, CG, CI, CD, '  + \\\n","                    'DJ, EG, GQ, ER, SZ, ET, GA, GM, GH, GN, GW, KE, LS, LR, '  + \\\n","                    'LY, MG, MW, ML, MR, MU, MA, MZ, NE, NG, RW, ST, SN, SC, '  + \\\n","                    'SL, SO, ZA, SS, SD, TG, TN, UG, TZ, EH, ZM, ZW, NA'\n","        return geoIdList\n","\n","    @staticmethod\n","    def get_pygal_oceania_geoid_list():\n","        \"\"\"Returns a list of GeoIDs of Oceanian countries that are available in PayGal and \n","        the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            list: List of strings of GeoID's\n","        \"\"\"\n","        # just the main countries for a map\n","        geoIDs = re.split(r',\\s*', CovidCasesOWID.get_pygal_oceania_geoid_string_list().upper())\n","        return geoIDs\n","\n","    @staticmethod\n","    def get_pygal_oceania_geoid_string_list():\n","        \"\"\"\n","        Returns a comma separated list of GeoIDs of Oceanian countries that are available in \n","        PayGal and the WHO data. \n","        Be aware:\n","        Not all countries of the WHO are available in PayGal and some names are different \n","        (GB in PyGal = UK in WHO, GR in PyGal = EL in WHO). PyGal uses lower case and WHO\n","        upper case. \n","\n","        Returns:\n","            str: A comma separate list of GeoID's\n","        \"\"\"\n","        # just the main oceania countries for a map, pygal doesn't contain e.g. \n","        # Comoros (KM)\n","        geoIdList = 'AU, FJ, PF, GU, NC, NZ, MP, PG'\n","        return geoIdList\n"]},{"cell_type":"markdown","metadata":{"id":"JqycnVSZYJ8U"},"source":["# CovidFoliumMap base class\n","\n","This abstract class acts as a base class for other classes that implement different folium maps based on different datasources. The class defines the interface to make use of the sub-classes \n","Here are some usefull links:\n","\n","- Geodata visualization   \n","  Folium: The documentation is available on https://python-visualization.github.io/folium/  \n","   Different basemaps are available on https://leaflet-extras.github.io/leaflet-providers/preview/\n","            "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17550,"status":"ok","timestamp":1655282916001,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"cnOZ7EdfYJ8U","outputId":"ab0a31cd-b340-41a0-c09e-190dc43b80f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting geopandas\n","  Downloading geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 15.6 MB/s \n","\u001b[?25hRequirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.2)\n","Collecting pyproj>=2.2.0\n","  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n","\u001b[K     |████████████████████████████████| 6.3 MB 43.3 MB/s \n","\u001b[?25hCollecting fiona>=1.8\n","  Downloading Fiona-1.8.21-cp37-cp37m-manylinux2014_x86_64.whl (16.7 MB)\n","\u001b[K     |████████████████████████████████| 16.7 MB 387 kB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.3.5)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2022.5.18.1)\n","Collecting click-plugins>=1.0\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n","Collecting cligj>=0.5\n","  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.4.0)\n","Collecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2022.1)\n","Installing collected packages: munch, cligj, click-plugins, pyproj, fiona, geopandas\n","Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.21 geopandas-0.10.2 munch-2.5.0 pyproj-3.2.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: folium in /usr/local/lib/python3.7/dist-packages (0.8.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from folium) (1.15.0)\n","Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from folium) (0.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from folium) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from folium) (1.21.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from folium) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->folium) (2.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->folium) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->folium) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->folium) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->folium) (2022.5.18.1)\n"]}],"source":["!pip install geopandas\n","!pip install folium\n","import pandas as pd\n","import numpy as np\n","import os\n","import geopandas as gpd\n","import folium\n","import requests\n","import json\n","import time\n","from abc import ABC, abstractmethod\n","from typing import List\n","from dataclasses import dataclass, field\n","from datetime import date, timedelta\n","from pathlib import Path\n","    \n","class CovidFoliumMap(ABC):\n","    \"\"\"\n","    This abstract base class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes. It does this  \n","    by providing access to a pandas geoJSON dataframe and a data dataframe. It also  provides methods to generate a default map.\n","    \"\"\"\n","    def __init__(self, dataDirectory):\n","        \"\"\"\n","        The constructor takes two dataframes. One containing geoJSON information and a second containing CoVid-19 data. \n","\n","        Args:\n","            dataDirectory (str): The path to a directory to store temporary data\n","        \"\"\"\n","        # keep the data directory\n","        self.__dataDirectory = dataDirectory\n","\n","    @dataclass\n","    class mapOptions:\n","        \"\"\" Somehow a struct holding information about the map such as location of the alias, date, center, etc.\n","        \"\"\"\n","        mapAlias: str = field(default_factory=lambda : '')\n","        \"\"\" An alias name of the map that can be used as a filename to save the map\n","        \"\"\"\n","        #ingredients: List = field(default_factory=lambda: ['dow', 'tomatoes'])\n","        tooltipAttributes: List = field(default_factory=lambda : [])\n","        \"\"\" A list of data attributes of the data df that should appear in the tooltip when moving the mouse over the map\n","        \"\"\"\n","        mapAttribute: str = field(default_factory=lambda : '')\n","        \"\"\" The string that should appear in the leaflet of the map\n","        \"\"\"\n","        mapLocation: List = field(default_factory=lambda : [])\n","        \"\"\" The initial center of the map \n","        \"\"\"\n","        mapZoom: int = 4\n","        \"\"\" The initial zoom level of the map\n","        \"\"\"\n","        mapDate: date = date.today\n","        \"\"\" The date of the data shown in the map\n","        \"\"\"\n","        bins: List[float] = field(default_factory=lambda : [])\n","        \"\"\" A list of values representing the colour bins (Folium supports up to 10 bins), or none to calculate default bins\n","\n","        Returns:\n","            mapOptions: The struct of options\n","        \"\"\"\n","\n","    def create_default_map(self, \n","                           basemap, \n","                           coloredAttribute = 'Incidence7DayPer100Kpopulation', \n","                           coloredAttributeAlias = '7-day incidence per 100.000 population'):\n","        \"\"\" Returns a default folium map\n","\n","        Args:\n","            basemap (str): The name of the basemap to be used. Can be one of the nice_basemaps or something different\n","            coloredAttribute (str, optional): [description]. Defaults to 'Incidence7DayPer100Kpopulation'.\n","            coloredAttributeAlias (str, optional): [description]. Defaults to '7-day incidence per 100.000 population'.\n","        \"\"\"\n","        # get the map options\n","        dfGeo = self.get_geo_df()\n","        dfData = self.get_data_df()\n","        mapOptions = self.get_default_map_options()\n","        # check if we have every<thing that we need\n","        if (dfGeo is None) or (dfData is None):\n","            return None\n","        # merge geo and data dfs. ensure merging to the geoDF to keep the result a geoPandas df\n","        combined = dfGeo.merge(dfData[[self.get_merge_UID()] + mapOptions.tooltipAttributes], \n","                               on=self.get_merge_UID(), \n","                               how='left')\n","        # create the map\n","        map = folium.Map(attr=mapOptions.mapAttribute, location=mapOptions.mapLocation, tiles=basemap, zoom_start=mapOptions.mapZoom)\n","        # the alias incl. the date\n","        coloredAttributeAlias = coloredAttributeAlias + ' as of ' + mapOptions.mapDate.strftime('%Y-%m-%d')\n","        # the bins for the colored values\n","        if (mapOptions.bins is None):\n","            mapOptions.bins = list(combined[coloredAttribute].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]))\n","        else:      \n","            # the minimum/maximum in the coloredAttribute column\n","            maximum = dfData[coloredAttribute].max()\n","            minimum = dfData[coloredAttribute].min()\n","            # ensure min/max value will fit in the bins\n","            mapOptions.bins[mapOptions.bins.count(0)-1] = max(maximum, mapOptions.bins[mapOptions.bins.count(0)-1])\n","            mapOptions.bins[0] = min(minimum, mapOptions.bins[0])\n","        # build the choropleth\n","        cp = folium.Choropleth (geo_data=combined,\n","                                data=combined,\n","                                #data=df,\n","                                columns=[self.get_merge_UID(), coloredAttribute],\n","                                key_on='feature.properties.' + self.get_merge_UID(),\n","                                fill_color='YlOrRd',\n","                                fill_opacity=0.4,\n","                                line_opacity=0.4,\n","                                nan_fill_color='#f5f5f3',\n","                                legend_name=coloredAttributeAlias,\n","                                bins=[float(x) for x in mapOptions.bins],\n","                                highlight=True,\n","                                smooth_factor = 0.1)\n","        # give it a name\n","        cp.layer_name = \"Covid-19 data\"  \n","        # add it to the map\n","        cp.add_to(map)\n","        # create a tooltip for hovering\n","        tt = folium.GeoJsonTooltip(fields= mapOptions.tooltipAttributes)\n","        # add it to the json\n","        tt.add_to(cp.geojson)\n","        # numbers and dates in the system local\n","        tt.localize = True\n","        # add a layer control to the map\n","        folium.LayerControl().add_to(map)\n","        # a legend\n","        #legend_html = '<div style=\"position: fixed; bottom: 75px; left: 50%; margin-left: -350px; width: 700px; height: 20px; z-index:9999; font-size:20px;\">&nbsp; ' + 'Generated on ' + date.today().strftime('%Y-%m-%d') + '<br></div>'\n","        #map.get_root().html.add_child(folium.Element(legend_html))\n","        # return the map\n","        return map\n","\n","    def get_data_directory(self):\n","        \"\"\"Returns the data directory as a string\n","        \n","        Args:\n","            -\n","\n","        Returns:\n","            DataDirectory: A string pointing to the absolute data directory path\n","        \"\"\"\n","        return self.__dataDirectory\n","    \n","    @abstractmethod\n","    def get_data_df(self):\n","        \"\"\" Returns the pandas data df\n","        \"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def get_geo_df(self):\n","        \"\"\" Returns the geoPandas df containing geometry information\n","        \"\"\"\n","        pass\n","    \n","    @abstractmethod\n","    def get_default_map_options(self):\n","        \"\"\" returns the options of the default map\n","        \"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def get_merge_UID(self):\n","        \"\"\"\n","        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n","\n","        Returns:\n","            string: A string holding the name of the unique ID of the data dataframe \n","        \"\"\"\n","        pass \n","\n","    @abstractmethod\n","    def get_nice_basemaps(self):\n","        \"\"\"\n","        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n","        the preferred basemap should be basemap[0]\n","\n","        Returns:\n","            string: A array of strings referring to nice basemaps \n","        \"\"\"\n","        pass \n"]},{"cell_type":"markdown","metadata":{"id":"M7F5ylrtYJ8V"},"source":["# CovidFoliumMapWHO classs\n","\n","This classes generates different folium maps based on the data of the WHO using the CovidCases, CovidCasesWHO and in case of the World and Asia maps also the CovidCasesOWID class to get numbers for Taiwan as well.  \n","The class inherits from the CovidFoliumMap class.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":345,"status":"ok","timestamp":1655282916341,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"HlNMzuDgYJ8V"},"outputs":[],"source":["from enum import Enum\n","\n","class Continents(Enum):\n","    \"\"\" an enum for the continents\n","\n","    Args:\n","        Enum (int): an enum for the continents\n","    \"\"\"\n","    # \n","    World = 0\n","    Europe = 1\n","    Africa = 2\n","    Asia = 3\n","    Oceania = 4\n","    America = 5\n","\n","    \n","class CovidFoliumMapWHO(CovidFoliumMap):\n","    \"\"\"\n","    This class implements different folium maps based on the data of the WHO using the CovidCases and\n","    CovidCasesWHO classes. In case of the World and Asia maps it also uses the CovidCasesOWID class \n","    to map the Taiwan cases as well.\n","    The class inherits from the CovidFoliumMap class \n","    \"\"\"\n","    def __init__(self, continent, dataDirectory = '../data', numDaysBefore = 0):\n","        \"\"\" Constructor\n","\n","        Args:\n","            continent (Continent): The continent to create the map for\n","            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n","        \"\"\"\n","        # init members\n","        self.__dataDirectory = dataDirectory + '/'\n","        self.__dfGeo = None\n","        self.__dfData = None\n","        self.__defaultMapOptions = CovidFoliumMapWHO.get_map_options_by_continent(continent)\n","        # ensure that the data directory exists, meaning to create it if it is not available\n","        self.__dataDirectory = ensure_path_exists(dataDirectory)\n","        # check if it really exists\n","        if self.__dataDirectory != '':\n","            # get the geoJSON data frame\n","            self.__dfGeo = self.__get_geo_data()\n","            # get the covid data for all countries in the continent\n","            if not self.__dfGeo is None:\n","                self.__dfData = self.__get_covid_data(continent, numDaysBefore)\n","        # init the base class\n","        super().__init__(self.__dataDirectory)\n","\n","    def __get_geo_data(self):\n","        \"\"\" Downloads the geoJSON file from the server if necessary and opens it to return a geoPandas dataframe. \n","        The function throws an exception in case of an error. \n","        Instead of using the highres geoJSON data downloaded from https://raw.githubusercontent.com/datasets/geo-countries \n","        you can use medium or low resolution files you can create on this website: https://geojson-maps.ash.ms. Just save \n","        the downloaded file in the data directory using 'WorldCountriesLowRes.geojson' or 'WorldCountriesMedRes.geojson'\n","        as the filename.\n","\n","        Returns:\n","            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n","        \"\"\"\n","        # init return\n","        geoDf = None\n","        # the filename of the geoJSON that is used\n","        targetFilename = self.__dataDirectory + '/' + 'WorldCountriesMedRes.geojson'\n","        # check if it exist already\n","        if not os.path.exists(targetFilename):\n","            # download the file\n","            print('Downloading data (WorldCountriesMedRes.geojson), that might take some time...')\n","            # this is the regular endpoint to download the high resolution geoJSON \n","            # endpoint = 'https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson'\n","            # the manual download link is\n","            # 'https://github.com/datasets/geo-countries/blob/master/data/countries.geojson'\n","            # but we will download the medium resolution file from our GitHb account\n","            endpoint = 'https://raw.githubusercontent.com/1c3t3a/Covid-19-analysis/master/data/WorldCountriesMedRes.geojson'\n","            try:\n","                # try to download the file \n","                download_JSON_file(endpoint, targetFilename)\n","                print('Download finished.')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e)    \n","        # now the file should exist\n","        if os.path.exists(targetFilename):\n","            # load the file\n","            geoDf = gpd.read_file(targetFilename)\n","        # adjust column names\n","        if 'iso_a3' in geoDf.columns:\n","            # the low and medium geoJSON contain many not required attributes\n","            geoDf.rename(columns = {'admin':'Name', 'iso_a3':'ISO-3166-alpha_3', 'iso_a2':'GeoID'}, inplace = True)\n","        else:   \n","            # the highres file contains only 4 attributes which nor to be renamed \n","            geoDf.columns = ['Name', 'ISO-3166-alpha_3', 'GeoID', 'geometry']\n","        # finally return the geo df\n","        return geoDf\n","\n","    def __get_covid_data(self, continent, numDaysBefore = 0):\n","        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n","        \n","        Args:\n","            continent (Continent): The continent to create the map for\n","            numDaysBefore (int): A delay in days to the current date. especially during holidays the reporting is delayed\n","\n","        Returns:\n","            covid dataframe: the covid data for the German states or None if it can't load the file\n","        \"\"\"\n","        # init the result\n","        df = None\n","        try:\n","            # get the latests database file as a CSV\n","            dataFile = CovidCasesWHO.download_CSV_file()\n","            # get the data for the countryList\n","            whoData = CovidCasesWHO(dataFile)\n","        except Exception as e:\n","            if hasattr(e, 'message'):\n","                print(e.message)\n","            else:\n","                print(e)  \n","            return df\n","        # g# the list of comma separated geoIDs for the continent\n","        if continent == Continents.World:\n","            countryList = whoData.get_pygal_asian_geoid_list()  + \\\n","                          whoData.get_pygal_european_geoid_list()  +  \\\n","                          whoData.get_pygal_american_geoid_list()  +  \\\n","                          whoData.get_pygal_african_geoid_list()  +  \\\n","                          whoData.get_pygal_oceania_geoid_list()\n","        elif continent == Continents.Europe:\n","            countryList = whoData.get_pygal_european_geoid_list()\n","        elif continent == Continents.Africa:\n","            countryList = whoData.get_pygal_african_geoid_list()\n","        elif continent == Continents.Asia:\n","            countryList = whoData.get_pygal_asian_geoid_list()\n","        elif continent == Continents.Oceania:\n","            countryList = whoData.get_pygal_oceania_geoid_list()\n","        elif continent == Continents.America:\n","            countryList = whoData.get_pygal_american_geoid_list()\n","        # since Omicron the WHO data for China seem to be incomplete\n","        if 'CN' in countryList:\n","            # remove china from the WHO list\n","            countryList.remove('CN')\n","        # get the data for the country list\n","        df = whoData.get_data_by_geoid_list(countryList)\n","        # add the incidence\n","        df = whoData.add_incidence_7day_per_100Kpopulation(df)\n","        if continent == Continents.Asia or continent == Continents.World:\n","            try:\n","                # get the OWID database as well\n","                dataFile = CovidCasesOWID.download_CSV_file()\n","                # get the OWID data\n","                owidData = CovidCasesOWID(dataFile)\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e)  \n","                return df\n","            # the taiwan, hongkong and china data\n","            dfTW = owidData.get_data_by_geoid_string_list('TW, HK, CN')\n","            # add the incidence\n","            dfTW = owidData.add_incidence_7day_per_100Kpopulation(dfTW)  \n","            # append it\n","            df = pd.concat([df, dfTW])  \n","        # get the data for last friday, on days reporting will not be good\n","        today = date.today() - datetime.timedelta(days=numDaysBefore)\n","        # take care of weekends as the data is often not available on weekends\n","        if (today.weekday() == 0) or (today.weekday() == 6):\n","            last_friday = this_or_last_weekday(date.today(), 4)\n","            self.__defaultMapOptions.mapDate = date(last_friday.year, last_friday.month, last_friday.day)\n","        else:\n","            self.__defaultMapOptions.mapDate = today - timedelta(1)\n","        # get the data for that date\n","        dfDate = df.loc[df['Date'] == pd.to_datetime(self.__defaultMapOptions.mapDate)]     \n","        # ...and return df\n","        return dfDate\n","\n","    def get_data_df(self):\n","        \"\"\" Returns the covid19 dataframe\n","\n","        Returns:\n","            [Dataframe]: The pandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfData\n","\n","    def get_geo_df(self):\n","        \"\"\" Returns the geoJSON dataframe\n","\n","        Returns:\n","            [Dataframe]: The geoPandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfGeo\n","\n","    def get_default_map_options(self):\n","        \"\"\" Returns the options for the default map\n","\n","        Returns:\n","            [mapOptions]: The map options such as the default location and zoom\n","        \"\"\"\n","        return self.__defaultMapOptions\n","\n","    def get_merge_UID(self):\n","        \"\"\"\n","        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n","\n","        Returns:\n","            string: A string holding the name of the unique ID of the data dataframe \n","        \"\"\"\n","        return 'GeoID'\n","\n","    def get_nice_basemaps(self):\n","        \"\"\"\n","        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n","        the preferred basemap should be basemap[0]\n","\n","        Returns:\n","            string: A array of strings referring to nice basemaps \n","        \"\"\"\n","        mapArray = ['cartodbpositron',\n","                    'https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n","                    'Stamen Terrain']\n","        return mapArray\n","\n","    @staticmethod\n","    def get_map_options_by_continent(continent):\n","        \"\"\" The function generates default options for the WHO maps for the different continents\n","\n","        Args:\n","            continent (Continents): the continent to generate the data for\n","        \n","        Returns:\n","            [mapOptions]: The options for the default map\n","        \"\"\"\n","        mo = CovidFoliumMap.mapOptions()\n","        # init everything that is somehow constant for WHO maps\n","        mo.mapDate = date.today()\n","        # this will use automatically generated bins\n","        mo.bins = None\n","        # the leaflet\n","        mo.mapAttribute = 'WHO data. Map generated by CMBT, 2022'\n","        # all WHO dataframes will include these attributes\n","        mo.tooltipAttributes = ['GeoName', \n","                                'Cases',\n","                                'Deaths', \n","                                'PercentDeaths',\n","                                'DailyCases', \n","                                'DailyDeaths', \n","                                'Incidence7DayPer100Kpopulation',\n","                                'CasesPerMillionPopulation',\n","                                'DeathsPerMillionPopulation']\n","        if continent == Continents.World:\n","            # defaults for the world map\n","            mo.mapAlias = 'MapWorld' \n","            mo.mapLocation = [15, 0]\n","            mo.mapZoom = 2\n","        elif continent == Continents.Europe:\n","            # defaults for the europe map\n","            mo.mapAlias = 'MapEurope' \n","            mo.mapLocation = [51.3, 10.5]\n","            mo.mapZoom = 4\n","        elif continent == Continents.Africa:\n","            # defaults for the africa map\n","            mo.mapAlias = 'MapAfrica' \n","            mo.mapLocation=[5, 19]\n","            mo.mapZoom = 4\n","        elif continent == Continents.Asia:\n","            # defaults for the asia map\n","            mo.mapAlias = 'MapAsia' \n","            mo.mapLocation=[23, 92]\n","            mo.mapZoom = 4\n","        elif continent == Continents.Oceania:\n","            # defaults for the oceania map\n","            mo.mapAlias = 'MapOceania' \n","            mo.mapLocation=[-26, 147]\n","            mo.mapZoom = 4\n","        elif continent == Continents.America:\n","            # defaults for the america map\n","            mo.mapAlias = 'MapAmerica' \n","            mo.mapLocation=[16, -86]\n","            mo.mapZoom = 3\n","        # return the options\n","        return mo\n"]},{"cell_type":"markdown","metadata":{"id":"yOlkwCX8nu5u"},"source":["# CovidFoliumMapRKIxxx classes\n","\n","This classes implement different folium maps based on the data of the Robert Koch Institute. There are (so far) two maps available. One showing the 7-day incidence data for German States, the other for German Cities and Counties.  \n","The class inherits from the CovidFoliumMap class.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":984,"status":"ok","timestamp":1655282917322,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"hQMMZ9mjoZiF"},"outputs":[],"source":["class CovidFoliumMapDEcounties(CovidFoliumMap):\n","    \"\"\"\n","    This class will expose an interface to deal with Choropleth maps to display Covid-19 data attributes for counties and cities in Germany. \n","    \"\"\"\n","    def __init__(self, dataDirectory = '../data'):\n","        \"\"\" Constructor\n","\n","        Args:\n","            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n","        \"\"\"\n","        # init members\n","        self.__dataDirectory = dataDirectory + '/'\n","        self.__dfGeo = None\n","        self.__dfData = None\n","        self.__defaultMapOptions = CovidFoliumMap.mapOptions(mapDate=date.today(),\n","                                                            mapAlias = 'MapDEcounty',\n","                                                            mapLocation = [51.3, 10.5],\n","                                                            mapZoom = 6,\n","                                                            bins = [5, 25, 50, 100, 200, 400, 800, 1200, 1600, 2600],\n","                                                            mapAttribute = 'Robert Koch-Institut (RKI), dl-de/by-2-0, CMBT 2022',\n","                                                            tooltipAttributes = ['GeoName', \n","                                                                                'Cases', \n","                                                                                'Deaths', \n","                                                                                'WeeklyCases', \n","                                                                                'WeeklyDeaths', \n","                                                                                'DailyCases', \n","                                                                                'DailyDeaths', \n","                                                                                'DailyRecovered', \n","                                                                                'Incidence7DayPer100Kpopulation'])\n","        # ensure that the data directory exists, meaning to create it if it is not available\n","        self.__dataDirectory = ensure_path_exists(dataDirectory)\n","        # check if it really exists\n","        if self.__dataDirectory != '':\n","            # get the geo JSON data frame\n","            self.__dfGeo = self.__get_geo_data()\n","            # get the covid data for all counties/cities in the geo dataframe\n","            if not self.get_geo_df is None:\n","                self.__dfData = self.__get_covid_data(self.__dfGeo)\n","        # init base class\n","        super().__init__(self.__dataDirectory)\n","\n","    def __get_geo_data(self):\n","        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n","        exception in case of an error\n","\n","        Returns:\n","            geo dataframe: the geo dataframe of the German counties and cities or None if it can't load the file\n","        \"\"\"\n","        # init return\n","        geoDf = None\n","        # the filename of the geoJSON that is used\n","        targetFilename = self.__dataDirectory + '/' + 'RKI_Corona_Landkreise.geojson'\n","        # check if it exist already\n","        if not os.path.exists(targetFilename):\n","            # download the file\n","            print('Downloading data (RKI_Corona_Landkreise.geojson), that might take some time...')\n","            endpoint = 'https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/RKI_Landkreisdaten/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json'\n","            # the manual download link is\n","            # 'https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/917fc37a709542548cc3be077a786c17_0/explore?location=51.282342%2C10.714458%2C6.71'\n","            try:\n","                # try to download the file \n","                download_JSON_file(endpoint, targetFilename)\n","                print('Download finished.')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e)    \n","        # now the file should exist\n","        if os.path.exists(targetFilename):\n","            # load the file\n","            geoDf = gpd.read_file(targetFilename)\n","            #print(geoDf.head())\n","        # finally return the geo df\n","        return geoDf\n","\n","    def __get_covid_data(self, geoDf):\n","        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n","        \n","        Returns:\n","            covid dataframe: the covid data for the German counties and cities or None if it can't load the file\n","        \"\"\"\n","        # init the result\n","        df = None\n","        # get the date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-RKIcounty\"\n","        # the target filename of the csv to be downloaded\n","        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n","        # check if it exist already\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","            # read the file\n","            df = pd.read_csv(targetFilename)\n","        else:\n","            print('Downloading data (yy-mm-dd--RKIcounty-db.csv), that might take some time...')\n","            # build a result df\n","            dfs = []\n","            for id in geoDf['RS']:\n","                try:\n","                    # get the data for the county\n","                    df = self.__get_county_data_from_web(id)\n","                    # add it to the list\n","                    dfs.append(df)\n","                except:\n","                    msg = 'Error getting the data for ' + str(id) + '!'\n","                    print(msg) \n","            try:\n","                # finally concatenate all dfs together\n","                df = pd.concat(dfs)  \n","                # save it to file\n","                df.to_csv(targetFilename)\n","                print('Download finished.')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e) \n","        # ensure RS length is 5\n","        if not df is None:\n","            df['RS'] = df['RS'].astype(str).str.zfill(5)\n","        # ...and return df\n","        return df\n","    \n","    def __get_county_data_from_web(self, county_ID):\n","        \"\"\" Downloads the covid-19 data for the given county-ID\n","\n","        Args:\n","            county_ID string: the county-ID for which we want the data\n","\n","        Raises:\n","            ValueError: In case the data is empty\n","\n","        Returns:\n","            dataframe: A dataframe of the county data\n","        \"\"\"\n","        # the endpoint of the request\n","        endpoint = 'https://api.corona-zahlen.org/districts/' + county_ID\n","        # contact the server\n","        res = requests.get(endpoint)\n","        # check if there was a response\n","        if res.ok:\n","            # get the json\n","            res = res.json()\n","        else:\n","            # raise an exception\n","            res.raise_for_status()\n","        # check if the data is not empty\n","        if not bool(res['data']):\n","            raise ValueError(\"Empty response! County ID might be invalid.\")\n","        df = pd.json_normalize(res['data'])\n","        df.columns = ['RS', \n","                    'GeoName', \n","                    'GeoID', \n","                    'State', \n","                    'Population', \n","                    'Cases',\n","                    'Deaths',\n","                    'WeeklyCases',\n","                    'WeeklyDeaths',\n","                    'StateID',\n","                    'Recovered',\n","                    'Incidence7DayPer100Kpopulation', \n","                    'CasesPer100kPopulation', \n","                    'DailyCases', \n","                    'DailyDeaths', \n","                    'DailyRecovered']\n","        return df\n","\n","    def get_data_df(self):\n","        \"\"\" Returns the covid19 dataframe\n","\n","        Returns:\n","            [Dataframe]: The pandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfData\n","\n","    def get_geo_df(self):\n","        \"\"\" Returns the geoJSON dataframe\n","\n","        Returns:\n","            [Dataframe]: The geoPandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfGeo\n","\n","    def get_default_map_options(self):\n","        \"\"\" Returns the options for the default map\n","\n","        Returns:\n","            [mapOptions]: The map options such as the default location and zoom\n","        \"\"\"\n","        return self.__defaultMapOptions\n","\n","    def get_merge_UID(self):\n","        \"\"\"\n","        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n","\n","        Returns:\n","            string: A string holding the name of the unique ID of the data dataframe \n","        \"\"\"\n","        return 'RS'\n","\n","    def get_nice_basemaps(self):\n","        \"\"\"\n","        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n","        the preferred basemap should be basemap[0]\n","\n","        Returns:\n","            string: A array of strings referring to nice basemaps \n","        \"\"\"\n","        mapArray = ['cartodbpositron',\n","                    'https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n","                    'Stamen Terrain']\n","        return mapArray\n","\n","class CovidFoliumMapDEstates(CovidFoliumMap):\n","    \"\"\"\n","    This class will generate Choropleth maps to display Covid-19 data attributes for German states. \n","    \"\"\"\n","    def __init__(self, dataDirectory = '../data'):\n","        \"\"\" Constructor\n","\n","        Args:\n","            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n","        \"\"\"\n","        # init members\n","        self.__dataDirectory = dataDirectory + '/'\n","        self.__dfGeo = None\n","        self.__dfData = None\n","        self.__defaultMapOptions = CovidFoliumMap.mapOptions(mapDate=date.today(),\n","                                                            mapAlias = 'MapDEstate',\n","                                                            mapLocation = [51.3, 10.5],\n","                                                            mapZoom = 6,\n","                                                            bins = [5, 25, 50, 100, 200, 400, 800, 1200, 1600, 2600],\n","                                                            mapAttribute = 'Robert Koch-Institut (RKI), dl-de/by-2-0, CMBT 2022',\n","                                                            tooltipAttributes = ['GeoName', \n","                                                                                'Cases', \n","                                                                                'Deaths', \n","                                                                                'WeeklyCases', \n","                                                                                'WeeklyDeaths', \n","                                                                                'DailyCases', \n","                                                                                'DailyDeaths', \n","                                                                                'DailyRecovered', \n","                                                                                'Incidence7DayPer100Kpopulation',\n","                                                                                'HospitalizationCases7'])\n","        # a list of German states\n","        self.__statelist = [['Schleswig-Holstein', 'SH'],\n","                            ['Hamburg', 'HH'],\n","                            ['Niedersachsen', 'NI'],\n","                            ['Bremen', 'HB'],\n","                            ['Nordrhein-Westfalen', 'NW'],\n","                            ['Hessen', 'HE'],\n","                            ['Rheinland-Pfalz', 'RP'],\n","                            ['Baden-Württemberg', 'BW'],\n","                            ['Bayern', 'BY'],\n","                            ['Saarland', 'SL'],\n","                            ['Berlin', 'BE'],\n","                            ['Brandenburg', 'BB'],\n","                            ['Mecklenburg-Vorpommern', 'MV'],\n","                            ['Sachsen', 'SN'],\n","                            ['Sachsen-Anhalt', 'ST'],\n","                            ['Thüringen', 'TH']]\n","        # ensure that the data directory exists, meaning to create it if it is not available\n","        self.__dataDirectory = ensure_path_exists(dataDirectory)\n","        # check if it really exists\n","        if self.__dataDirectory != '':\n","            # get the geo JSON data frame\n","            self.__dfGeo = self.__get_geo_data()\n","            # get the covid data for all counties/cities in the geo dataframe\n","            if not self.get_geo_df is None:\n","                self.__dfData = self.__get_covid_data()\n","        # init the base class\n","        super().__init__(self.__dataDirectory)\n","\n","    def __get_geo_data(self):\n","        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n","        exception in case of an error\n","\n","        Returns:\n","            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n","        \"\"\"\n","        # init return\n","        geoDf = None\n","        # the filename of the geoJSON that is used\n","        targetFilename = self.__dataDirectory + '/' + 'RKI_Corona_Bundeslaender.geojson'\n","        # check if it exist already\n","        if not os.path.exists(targetFilename):\n","            # download the file\n","            print('Downloading data (RKI_Corona_Bundeslaender.geojson), that might take some time...')\n","            endpoint = 'https://opendata.arcgis.com/api/v3/datasets/ef4b445a53c1406892257fe63129a8ea_0/downloads/data?format=geojson&spatialRefId=4326'\n","            # the manual download link is\n","            # 'https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/ef4b445a53c1406892257fe63129a8ea_0/explore'\n","            try:\n","                # try to download the file \n","                download_JSON_file(endpoint, targetFilename)\n","                print('Download finished.')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e)    \n","        # now the file should exist\n","        if os.path.exists(targetFilename):\n","            # load the file\n","            geoDf = gpd.read_file(targetFilename)\n","        # finally return the geo df\n","        return geoDf\n","\n","    def __get_covid_data(self):\n","        \"\"\" Downloads the covid-19 data from the RKI servers if necessary, caches them and opens a final csv to return a Pandas dataframe. \n","        \n","        Returns:\n","            covid dataframe: the covid data for the German states or None if it can't load the file\n","        \"\"\"\n","        # init the result\n","        df = None\n","        # get the date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-RKIstates\"\n","        # the target filename of the csv to be downloaded\n","        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n","        # check if it exist already\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","            # read the file\n","            df = pd.read_csv(targetFilename)\n","        else:\n","            print('Downloading data (yy-mm-dd-RKIstates-db.csv), that might take some time...')\n","            # build a result df\n","            dfs = []\n","            for id in self.__statelist:\n","                try:\n","                    # get the data for the county\n","                    df = self.__get_state_data_from_web(id[1])\n","                    # add it to the list\n","                    dfs.append(df)\n","                except:\n","                    msg = 'Error getting the data for ' + str(id) + '!'\n","                    print(msg) \n","            # finally concatenate all dfs together\n","            try:\n","                df = pd.concat(dfs)  \n","                # save it to file\n","                df.to_csv(targetFilename)\n","                print('Download finished.')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e) \n","            #print(df.head())\n","        # ensure AGS length is 2\n","        if not df is None:\n","            df['AGS_TXT'] = df['AGS_TXT'].astype(str).str.zfill(2)\n","        # ...and return df\n","        return df\n","    \n","    def __get_state_data_from_web(self, state_ID):\n","        \"\"\" Downloads the covid-19 data for the given county-ID\n","\n","        Args:\n","            county_ID string: the county-ID for which we want the data\n","\n","        Raises:\n","            ValueError: In case the data is empty\n","\n","        Returns:\n","            dataframe: A dataframe of the county data\n","        \"\"\"\n","        # the endpoint of the request\n","        endpoint = 'https://api.corona-zahlen.org/states/' + state_ID\n","        # contact the server\n","        res = requests.get(endpoint)\n","        # check if there was a response\n","        if res.ok:\n","            # get the json\n","            res = res.json()\n","        else:\n","            # raise an exception\n","            res.raise_for_status()\n","        # check if the data is not empty\n","        if not bool(res['data']):\n","            raise ValueError(\"Empty response! State ID might be invalid.\")\n","        df = pd.json_normalize(res['data'])\n","        # adjust column names\n","        df.columns = ['AGS_TXT', \n","                    'GeoName', \n","                    'Population', \n","                    'Cases',\n","                    'Deaths',\n","                    'WeeklyCases',\n","                    'WeeklyDeaths',\n","                    'Recovered',\n","                    'GeoID',\n","                    'Incidence7DayPer100Kpopulation', \n","                    'CasesPer100kPopulation', \n","                    'DailyCases', \n","                    'DailyDeaths', \n","                    'DailyRecovered',\n","                    'HospitalizationCases7',\n","                    'HospitalizationIncidence7',\n","                    'HospitalizationDate',\n","                    'HospitalizationUpdate']\n","        return df\n","\n","    def get_data_df(self):\n","        \"\"\" Returns the covid19 dataframe\n","\n","        Returns:\n","            [Dataframe]: The pandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfData\n","\n","    def get_geo_df(self):\n","        \"\"\" Returns the geoJSON dataframe\n","\n","        Returns:\n","            [Dataframe]: The geoPandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfGeo\n","\n","    def get_default_map_options(self):\n","        \"\"\" Returns the options for the default map\n","\n","        Returns:\n","            [mapOptions]: The map options such as the default location and zoom\n","        \"\"\"\n","        return self.__defaultMapOptions\n","\n","    def get_merge_UID(self):\n","        \"\"\"\n","        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n","\n","        Returns:\n","            string: A string holding the name of the unique ID of the data dataframe \n","        \"\"\"\n","        return 'AGS_TXT'\n","\n","    def get_nice_basemaps(self):\n","        \"\"\"\n","        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n","        the preferred basemap should be basemap[0]\n","\n","        Returns:\n","            string: A array of strings referring to nice basemaps \n","        \"\"\"\n","        mapArray = ['cartodbpositron',\n","                    'https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n","                    'Stamen Terrain']\n","        return mapArray\n"]},{"cell_type":"markdown","metadata":{"id":"TciX3aPQNkxc"},"source":["#CovidFoliumMapDEageAndGenderXXX and their helper classes\n","This classes generate different folium maps based on the age and gender specific data of the RKI using access to the RKI Covid-19 master file. There are (so far) two maps available. One showing the percentage of infections for kids younger than 15 years for German States, the other for German Cities and Counties.\n","The class inherits from the CovidFoliumMap class"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17080,"status":"ok","timestamp":1655282934397,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"MG1PtDG5Ooq-","outputId":"f8a2874a-d8a5-47b3-dcdf-8e849c848d6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting recordclass\n","  Downloading recordclass-0.17.2.tar.gz (446 kB)\n","\u001b[K     |████████████████████████████████| 446 kB 26.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: recordclass\n","  Building wheel for recordclass (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for recordclass: filename=recordclass-0.17.2-cp37-cp37m-linux_x86_64.whl size=288761 sha256=630cd31cc8a82b403d5496e37d2bc3041cf3d9e3e163d068c9275a25f9f3ab6d\n","  Stored in directory: /root/.cache/pip/wheels/7c/a7/f2/f1b5af34322f1cc71e5a877d03056a188728509fcadf683197\n","Successfully built recordclass\n","Installing collected packages: recordclass\n","Successfully installed recordclass-0.17.2\n"]}],"source":["!pip install recordclass\n","from recordclass import recordclass\n","\n","class DownloadAndPreprocessRKIdata():\n","    def __init__(self, dataDirectory = '../data'):\n","        \"\"\" Constructor\n","\n","        Args:\n","            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n","        \"\"\"\n","        # ensure that the data directory exists, meaning to create it if it is not available\n","        self.__dataDirectory = ensure_path_exists(dataDirectory)\n","        # init the result\n","        self.__df = None\n","        # a mutable, named tuple to hold columnnames, groupID, alias and a datframe\n","        Group = recordclass('group', 'column name alias df')\n","        self.__groups = []\n","        # build the list\n","        self.__groups.append(Group('Altersgruppe', 'A00-A04', 'age: 0-4', None))\n","        self.__groups.append(Group('Altersgruppe', 'A05-A14', 'age: 5-14', None))\n","        self.__groups.append(Group('Altersgruppe', 'A15-A34', 'age: 15-34', None))\n","        self.__groups.append(Group('Altersgruppe', 'A35-A59', 'age: 35-59', None))\n","        self.__groups.append(Group('Altersgruppe', 'A60-A79', 'age: 60-79', None))\n","        self.__groups.append(Group('Altersgruppe', 'A80+', 'age: 80+', None))\n","        self.__groups.append(Group('Geschlecht', 'W', 'gender: female', None))\n","        self.__groups.append(Group('Geschlecht', 'M', 'gender: male', None))\n","\n","    def __download_RKI_master_file(self):\n","        \"\"\" checks if the RKI master file of today exits already and downloads it if not\n","\n","        Returns:\n","            [bool]: True in case the file is available for pre-processing\n","        \"\"\"\n","        # check if we did all that stuff before\n","        if (self.__df is not None) and (self.__groups is not None):\n","           return True\n","        # get the date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-RKI_COVID19\"\n","        # the target filename of the csv to be downloaded\n","        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n","        # check if it exist already\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","        else:\n","            # download the file\n","            print('Downloading data (yy-mm-dd-RKI_COVID19-db.csv), that might take some time...')\n","            endpoint = 'https://www.arcgis.com/sharing/rest/content/items/f10774f1c63e40168479a1feb6c7ca74/data'\n","            # the manual download link is\n","            # https://www.arcgis.com/home/item.html?id=dd4580c810204019a7b8eb3e0b329dd6\n","            # or: https://www.arcgis.com/home/item.html?id=f10774f1c63e40168479a1feb6c7ca74  \n","            try:\n","                # try to download the file \n","                req = requests.get(endpoint)\n","                # get the content\n","                content = req.content\n","                # open the file\n","                csv = open(targetFilename, 'wb')\n","                # write the file\n","                csv.write(content)\n","                # close the file\n","                csv.close()\n","                print('Download finished...')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e)\n","                return False\n","        # now the file should exist\n","        if os.path.exists(targetFilename):\n","            df = pd.read_csv(targetFilename)\n","            # drop some columns that we don't need\n","            df = df.drop(columns=['NeuGenesen', \n","                                  'AnzahlGenesen', \n","                                  'IstErkrankungsbeginn', \n","                                  'Altersgruppe2', \n","                                  'Refdatum', \n","                                  'NeuerTodesfall', \n","                                  'Datenstand'])\n","        # set index to datetime of 'Meldedatum'\n","        df = df.set_index('Meldedatum')\n","        df.index = pd.to_datetime(df.index)\n","        # keep the dataframe\n","        self.__df = df\n","        # filter age groups\n","        print('Filtering by age/gender groups...')\n","        for group in self.__groups:\n","            group.df = df[df[group.column].str.match(group.name)]\n","        print('Filtering done...')\n","        return True\n","\n","    def get_age_and_gender_data_by_county(self):\n","        \"\"\" Pre-processes the RKI master file to generate a csv holding the data per county\n","\n","        Returns:\n","            [DataFrame]: The data frame holding the data per county or None in case something went wrong\n","        \"\"\"\n","        # get the date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-RKI_COVID19_age_gender_per_county\"\n","        # the target filename of the csv to be used/created\n","        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","            # read the file\n","            df = pd.read_csv(targetFilename)\n","            print(df.head())\n","            # ...and return it\n","            return df\n","        else:\n","            # ensure that we have downloaded the RKI master file\n","            if self.__df is None:\n","                if not self.__download_RKI_master_file():\n","                    return None\n","        # build the sum of cases\n","        print('Building groups and sums...')\n","        dfGroupSum = []\n","        for group in self.__groups:\n","            # getting the sum for the group\n","            tmp = self.__group_and_sum_colum(group.df)\n","            # change the column name\n","            tmp.columns = ['Cases by ' + group.alias]\n","            # append it to the list\n","            dfGroupSum.append(tmp)\n","        # concat them horizontally so that the become columns \n","        dfSums = pd.concat(dfGroupSum, axis=1)\n","        # create a data frame and reset its index\n","        dfAgeAndGender = pd.DataFrame(dfSums.copy()).reset_index()\n","        # convert NaN to 0\n","        dfAgeAndGender.fillna(value=0, inplace=True)\n","        # rename IDLandkreis to RS to match the column name of the geoJSON\n","        dfAgeAndGender.rename(columns={'IdLandkreis':'RS'}, inplace=True)\n","        print(dfAgeAndGender.head())\n","        print('Calculate percentages...')\n","        # get the county IDs\n","        IDs = dfAgeAndGender['RS'].unique()\n","        dfs = []\n","        for ID in IDs:\n","            # the county data\n","            dfSingle = dfAgeAndGender.loc[dfAgeAndGender['RS'] == ID].copy()\n","            # all cases of all age groups\n","            overallByAge = sum([dfSingle['Cases by age: 0-4'],\n","                                dfSingle['Cases by age: 5-14'],\n","                                dfSingle['Cases by age: 15-34'],\n","                                dfSingle['Cases by age: 35-59'],\n","                                dfSingle['Cases by age: 60-79'],\n","                                dfSingle['Cases by age: 80+']])\n","            # the percentages of some groups\n","            percentage = sum([dfSingle['Cases by age: 0-4'], dfSingle['Cases by age: 5-14']]) * 100 / overallByAge\n","            dfSingle['Percent cases by age: 0-14'] = percentage\n","\n","            percentage = sum([dfSingle['Cases by age: 15-34'], dfSingle['Cases by age: 35-59'], dfSingle['Cases by age: 60-79']]) * 100 / overallByAge\n","            dfSingle['Percent cases by age: 15-79'] = percentage\n","            \n","            percentage = dfSingle['Cases by age: 80+'] * 100 / overallByAge\n","            dfSingle['Percent cases by age: 80+'] = percentage\n","            # put the rows together\n","            dfs.append(dfSingle)\n","        # concat them\n","        df = pd.concat(dfs)\n","        # write the result to a csv\n","        df.to_csv(targetFilename)\n","        return df\n","\n","    def get_age_and_gender_data_by_state(self):\n","        \"\"\" Pre-processes the RKI master file to generate a csv holding the data per state\n","\n","        Returns:\n","            [DataFrame]: The data frame holding the data per state or None in case something went wrong\n","        \"\"\"\n","        # get the date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-RKI_COVID19_age_gender_per_state\"\n","        # the target filename of the csv to be used/created\n","        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","            # read the file\n","            df = pd.read_csv(targetFilename)\n","            print(df.head())\n","            # ...and return it\n","            return df\n","        else:\n","            # ensure that we have downloaded the RKI master file\n","            if self.__df is None:\n","                if not self.__download_RKI_master_file():\n","                    return None\n","        # build the sum of cases\n","        print('Building groups and sums...')\n","        dfGroupSum = []\n","        for group in self.__groups:\n","            # getting the sum for the group\n","            tmp = self.__group_and_sum_colum(group.df, groupColumn='IdBundesland')\n","            # change the column name\n","            tmp.columns = ['Cases by ' + group.alias]\n","            # append it to the list\n","            dfGroupSum.append(tmp)\n","        # concat them horizontally so that the become columns \n","        dfSums = pd.concat(dfGroupSum, axis=1)\n","        # create a data frame and reset its index\n","        dfAgeAndGender = pd.DataFrame(dfSums.copy()).reset_index()\n","        # convert NaN to 0\n","        dfAgeAndGender.fillna(value=0, inplace=True)\n","        # rename IdBundesland to AGS_TXT to match the column name of the geoJSON\n","        dfAgeAndGender.rename(columns={'IdBundesland':'AGS_TXT'}, inplace=True)\n","        print('Calculate percentages...')\n","        # get the county IDs\n","        IDs = dfAgeAndGender['AGS_TXT'].unique()\n","        dfs = []\n","        for ID in IDs:\n","            # the county data\n","            dfSingle = dfAgeAndGender.loc[dfAgeAndGender['AGS_TXT'] == ID].copy()\n","            # all cases of all age groups\n","            overallByAge = sum([dfSingle['Cases by age: 0-4'],\n","                                dfSingle['Cases by age: 5-14'],\n","                                dfSingle['Cases by age: 15-34'],\n","                                dfSingle['Cases by age: 35-59'],\n","                                dfSingle['Cases by age: 60-79'],\n","                                dfSingle['Cases by age: 80+']])\n","            # the percentages of some groups\n","            percentage = sum([dfSingle['Cases by age: 0-4'], dfSingle['Cases by age: 5-14']]) * 100 / overallByAge\n","            dfSingle['Percent cases by age: 0-14'] = percentage\n","\n","            percentage = sum([dfSingle['Cases by age: 15-34'], dfSingle['Cases by age: 35-59'], dfSingle['Cases by age: 60-79']]) * 100 / overallByAge\n","            dfSingle['Percent cases by age: 15-79'] = percentage\n","            \n","            percentage = dfSingle['Cases by age: 80+'] * 100 / overallByAge\n","            dfSingle['Percent cases by age: 80+'] = percentage\n","            # put the rows together\n","            dfs.append(dfSingle)\n","        # concat them\n","        df = pd.concat(dfs)\n","        # write the result to a csv\n","        df.to_csv(targetFilename)\n","        return df\n","\n","    def __group_and_sum_colum(self, df, groupColumn = 'IdLandkreis', sumColumn='AnzahlFall', flagColumn='NeuerFall'):\n","        \"\"\" Groups the data by the groupColumn and builds the sum of the sumColumn. The data in the sumColumn might be \n","        invalid depending on a flag in the flagColumn\n","\n","        Args:\n","            df (DataFrame): the huge df loaded from the RKI master file\n","            groupColumn (str, optional): The grouping column. Defaults to 'IdLandkreis', can be 'IdBundesland' as well.\n","            sumColumn (str, optional): The column to be summed up. Defaults to 'AnzahlFall'.\n","            flagColumn (str, optional): The column holding the flag if the data is valid or not. -1, 0 or 1 refer to valid \n","            data. Defaults to 'NeuerFall'.\n","\n","        Returns:\n","            [DataFrame]: A data frame holding the values of the group (e.g. IdLandkreis) vertically and the grouped \n","            sums horizontally\n","        \"\"\"\n","        # flag column must be in -1, 0, 1 to indicate valid numbers\n","        flag = df[flagColumn].isin((-1, 0, 1))\n","        # group the flagged rows and build the sum\n","        series = df[flag].groupby([groupColumn])[sumColumn].sum().to_frame(name = sumColumn).reset_index()\n","        # make an array \n","        #series = series[['IdLandkreis', 'AnzahlFall']]\n","        # the ID is a 5 digit string\n","        series[groupColumn] = series[groupColumn].astype(str).str.zfill(5)\n","        # set the index\n","        series = series.set_index(groupColumn)\n","        return series\n","\n","class CovidFoliumMapDEageAndGenderCounties(CovidFoliumMap):\n","    \"\"\"\n","    This class will generate Choropleth maps to display Covid-19 data attributes sorted by age and gender for counties and cities in Germany. \n","    \"\"\"\n","    def __init__(self, dataDirectory = '../data'):\n","        \"\"\" Constructor\n","\n","        Args:\n","            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n","        \"\"\"\n","        # ensure that the data directory exists, meaning to create it if it is not available\n","        self.__dataDirectory = ensure_path_exists(dataDirectory)\n","        # init members\n","        self.__dfGeo = None\n","        self.__dfData = None\n","        self.__defaultMapOptions = CovidFoliumMap.mapOptions(mapDate=date.today(),\n","                                                            mapAlias = 'MapDEageAndGenderCounty',\n","                                                            mapLocation = [51.3, 10.5],\n","                                                            mapZoom = 6,\n","                                                            bins = None,\n","                                                            mapAttribute = 'Robert Koch-Institut (RKI), dl-de/by-2-0, CMBT 2022',\n","                                                            tooltipAttributes = ['GeoName',\n","                                                                                'Cases by age: 0-4', \n","                                                                                'Cases by age: 5-14', \n","                                                                                'Cases by age: 15-34', \n","                                                                                'Cases by age: 35-59', \n","                                                                                'Cases by age: 60-79', \n","                                                                                'Cases by age: 80+', \n","                                                                                'Percent cases by age: 0-14',\n","                                                                                'Percent cases by age: 15-79',\n","                                                                                'Percent cases by age: 80+',\n","                                                                                'Cases by gender: female', \n","                                                                                'Cases by gender: male'])\n","        # ensure that the data directory exists, meaning to create it if it is not available\n","        self.__dataDirectory = ensure_path_exists(dataDirectory)\n","        # check if it really exists\n","        if self.__dataDirectory != '':\n","            # get the geo JSON data frame\n","            self.__dfGeo = self.__get_geo_data()\n","            # get the covid data for all counties/cities in the geo dataframe\n","            if not self.get_geo_df is None:\n","                self.__dfData = self.__get_covid_data(self.__dfGeo)\n","        # init base class\n","        super().__init__(self.__dataDirectory)\n","\n","    def __get_geo_data(self):\n","        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n","        exception in case of an error\n","\n","        Returns:\n","            geo dataframe: the geo dataframe of the German counties and cities or None if it can't load the file\n","        \"\"\"\n","        # init return\n","        geoDf = None\n","        # the filename of the geoJSON that is used\n","        targetFilename = self.__dataDirectory + '/' + 'RKI_Corona_Landkreise.geojson'\n","        # check if it exist already\n","        if not os.path.exists(targetFilename):\n","            # download the file\n","            print('Downloading data (RKI_Corona_Landkreise.geojson), that might take some time...')\n","            endpoint = 'https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/RKI_Landkreisdaten/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json'\n","            # the manual download link is\n","            # 'https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/917fc37a709542548cc3be077a786c17_0/explore?location=51.282342%2C10.714458%2C6.71'\n","            try:\n","                # try to download the file \n","                download_JSON_file(endpoint, targetFilename)\n","                print('Download finished.')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e)    \n","        # now the file should exist\n","        if os.path.exists(targetFilename):\n","            # load the file\n","            geoDf = gpd.read_file(targetFilename)\n","        # sort it by RS (LandreisID), there is a mismatch because of the RKI Berlin approach of having separate data for the districts \n","        # in one table they are sorted into the table, in the other added at the end\n","        geoDf = geoDf.sort_values('RS').reset_index()\n","        return geoDf\n","\n","    def __get_covid_data(self, dfGeo):\n","        \"\"\" Reads the pre-processed covid-19 data from a csv file generated by the DownloadAndPreprocessRKIdata class. If the file \n","        doesn't exist it will ensure that they will be created. Finally it will return a Pandas dataframe. \n","        \n","        Returns:\n","            covid dataframe: the covid data for the German counties and cities or None if it can't load the file\n","        \"\"\"\n","        # init the result\n","        df = None\n","        # get the date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-RKI_COVID19_age_gender_per_county\"\n","        # the target filename of the csv to be used/created\n","        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n","        # check if it exist already\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","        else:\n","            # download and preprocess RKI data for both states and counties\n","            ps = DownloadAndPreprocessRKIdata(self.__dataDirectory)\n","            # preprocess by state\n","            if ps.get_age_and_gender_data_by_state() is None:\n","                print(\"Preprocessing by state failed!\")\n","                return df\n","            # preprocess by county\n","            if ps.get_age_and_gender_data_by_county() is None:\n","                print(\"Preprocessing by county failed!\")\n","                return df\n","        # now the file should exist, read it\n","        df = pd.read_csv(targetFilename)\n","        if df is None:\n","            return df\n","        # ensure RS length is 5\n","        df['RS'] = df['RS'].astype(str).str.zfill(5)\n","        # sort it by RS (LandkreisID), there is a mismatch because of the RKI Berlin approach of having separate data for the districts \n","        # in one table they are sorted into the table, in the other added at the end\n","        df = df.sort_values('RS').reset_index()\n","        # the county name is in the dfgeo\n","        dfTmp = pd.DataFrame(dfGeo['county'])\n","        dfTmp.columns = ['GeoName']\n","        combined = pd.concat([df, dfTmp], axis=1)\n","        # some how it should work with a merge but that fails (maybe because the two tables haven't been sorted) TODO\n","        # result = df.merge(geoDf['RS', 'county'], on='RS', how='left')\n","        # ...and return df\n","        return combined\n","    \n","    def get_data_df(self):\n","        \"\"\" Returns the covid19 dataframe\n","\n","        Returns:\n","            [Dataframe]: The pandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfData\n","\n","    def get_geo_df(self):\n","        \"\"\" Returns the geoJSON dataframe\n","\n","        Returns:\n","            [Dataframe]: The geoPandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfGeo\n","\n","    def get_default_map_options(self):\n","        \"\"\" Returns the options for the default map\n","\n","        Returns:\n","            [mapOptions]: The map options such as the default location and zoom\n","        \"\"\"\n","        return self.__defaultMapOptions\n","\n","    def get_merge_UID(self):\n","        \"\"\"\n","        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n","\n","        Returns:\n","            string: A string holding the name of the unique ID of the data dataframe \n","        \"\"\"\n","        return 'RS'\n","\n","    def get_nice_basemaps(self):\n","        \"\"\"\n","        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n","        the preferred basemap should be basemap[0]\n","\n","        Returns:\n","            string: A array of strings referring to nice basemaps \n","        \"\"\"\n","        mapArray = ['cartodbpositron',\n","                    'https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n","                    'Stamen Terrain']\n","        return mapArray\n","\n","class CovidFoliumMapDEageAndGenderStates(CovidFoliumMap):\n","    \"\"\"\n","    This class will generate Choropleth maps to display Covid-19 data attributes sorted by age and gender for German states.  \n","    \"\"\"\n","    def __init__(self, dataDirectory = '../data'):\n","        \"\"\" Constructor\n","\n","        Args:\n","            dataDirectory (str, optional): The data directory to be used for cached data. Defaults to '../data'.\n","        \"\"\"\n","        # ensure that the data directory exists, meaning to create it if it is not available\n","        self.__dataDirectory = ensure_path_exists(dataDirectory)\n","        # init members\n","        self.__dfGeo = None\n","        self.__dfData = None\n","        self.__defaultMapOptions = CovidFoliumMap.mapOptions(mapDate=date.today(),\n","                                                            mapAlias = 'MapDEageAndGenderState',\n","                                                            mapLocation = [51.3, 10.5],\n","                                                            mapZoom = 6,\n","                                                            bins = None,\n","                                                            mapAttribute = 'Robert Koch-Institut (RKI), dl-de/by-2-0, CMBT 2022',\n","                                                            tooltipAttributes = ['GeoName',\n","                                                                                'Cases by age: 0-4', \n","                                                                                'Cases by age: 5-14', \n","                                                                                'Cases by age: 15-34', \n","                                                                                'Cases by age: 35-59', \n","                                                                                'Cases by age: 60-79', \n","                                                                                'Cases by age: 80+', \n","                                                                                'Percent cases by age: 0-14',\n","                                                                                'Percent cases by age: 15-79',\n","                                                                                'Percent cases by age: 80+',\n","                                                                                'Cases by gender: female', \n","                                                                                'Cases by gender: male'])\n","        \n","        # check if it really exists\n","        if self.__dataDirectory != '':\n","            # get the geo JSON data frame\n","            self.__dfGeo = self.__get_geo_data()\n","            # get the covid data for all counties/cities in the geo dataframe\n","            if not self.get_geo_df is None:\n","                self.__dfData = self.__get_covid_data(self.__dfGeo)\n","        # init the base class\n","        super().__init__(self.__dataDirectory)\n","\n","    def __get_geo_data(self):\n","        \"\"\" Downloads the JSON file from the RKI server if necessary and opens it to return a geoPandas dataframe. The function throws an\n","        exception in case of an error\n","\n","        Returns:\n","            geo dataframe: the geo dataframe of the German states or None if it can't load the file\n","        \"\"\"\n","        # init return\n","        geoDf = None\n","        # the filename of the geoJSON that is used\n","        targetFilename = self.__dataDirectory + '/' + 'RKI_Corona_Bundeslaender.geojson'\n","        # check if it exist already\n","        if not os.path.exists(targetFilename):\n","            # download the file\n","            print('Downloading data (RKI_Corona_Bundeslaender.geojson), that might take some time...')\n","            endpoint = 'https://opendata.arcgis.com/api/v3/datasets/ef4b445a53c1406892257fe63129a8ea_0/downloads/data?format=geojson&spatialRefId=4326'\n","            # the manual download link is\n","            # 'https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/ef4b445a53c1406892257fe63129a8ea_0/explore'\n","            try:\n","                # try to download the file \n","                download_JSON_file(endpoint, targetFilename)\n","                print('Download finished.')\n","            except Exception as e:\n","                if hasattr(e, 'message'):\n","                    print(e.message)\n","                else:\n","                    print(e)    \n","        # now the file should exist\n","        if os.path.exists(targetFilename):\n","            # load the file\n","            geoDf = gpd.read_file(targetFilename)\n","        # finally return the geo df\n","        return geoDf\n","\n","    def __get_covid_data(self, dfGeo):\n","        \"\"\" Reads the pre-processed covid-19 data from a csv file generated by the DownloadAndPreprocessRKIdata class. If the file \n","        doesn't exist it will ensure that they will be created. Finally it will return a Pandas dataframe. \n","        \n","        Returns:\n","            covid dataframe: the covid data for the German counties and cities or None if it can't load the file\n","        \"\"\"\n","        # init the result\n","        df = None\n","        # get the date\n","        today = date.today()\n","        # the prefix of the CSV file is Y-m-d\n","        preFix = today.strftime('%Y-%m-%d') + \"-RKI_COVID19_age_gender_per_state\"\n","        # the target filename of the csv to be used/created\n","        targetFilename = self.__dataDirectory + '/' + preFix + '-db.csv'\n","        # check if it exist already\n","        if os.path.exists(targetFilename):\n","            print('using existing file: ' + targetFilename)\n","        else:\n","            # download and preprocess RKI data for both states and counties\n","            ps = DownloadAndPreprocessRKIdata(self.__dataDirectory)\n","            # preprocess by state\n","            if ps.get_age_and_gender_data_by_state() is None:\n","                print(\"Preprocessing by state failed!\")\n","                return df\n","            # preprocess by county\n","            if ps.get_age_and_gender_data_by_county() is None:\n","                print(\"Preprocessing by county failed!\")\n","                return df\n","        # now the file should exist, read it\n","        df = pd.read_csv(targetFilename)\n","        if df is None:\n","            return df\n","        # ensure AGS length is 2\n","        df['AGS_TXT'] = df['AGS_TXT'].astype(str).str.zfill(2)\n","        \n","        # get the state names from the dfGeo\n","        dfTmp = pd.DataFrame(dfGeo['LAN_ew_GEN'])\n","        # rename them to fit our wording \n","        dfTmp.columns = ['GeoName']\n","        # put them together\n","        combined = pd.concat([df, dfTmp], axis=1)\n","        # ...and return them\n","        return combined\n","   \n","    def get_data_df(self):\n","        \"\"\" Returns the covid19 dataframe\n","\n","        Returns:\n","            [Dataframe]: The pandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfData\n","\n","    def get_geo_df(self):\n","        \"\"\" Returns the geoJSON dataframe\n","\n","        Returns:\n","            [Dataframe]: The geoPandas data frame with all data for the countries\n","        \"\"\"\n","        return self.__dfGeo\n","\n","    def get_default_map_options(self):\n","        \"\"\" Returns the options for the default map\n","\n","        Returns:\n","            [mapOptions]: The map options such as the default location and zoom\n","        \"\"\"\n","        return self.__defaultMapOptions\n","\n","    def get_merge_UID(self):\n","        \"\"\"\n","        Returns the string holding the name of the unique ID of the data and the geo dataframe that can be used to merge the two\n","\n","        Returns:\n","            string: A string holding the name of the unique ID of the data dataframe \n","        \"\"\"\n","        return 'AGS_TXT'\n","\n","    def get_nice_basemaps(self):\n","        \"\"\"\n","        Returns an array of strings referring to nice basemaps for the specific region. At least one basemaps should be given and \n","        the preferred basemap should be basemap[0]\n","\n","        Returns:\n","            string: A array of strings referring to nice basemaps \n","        \"\"\"\n","        mapArray = ['cartodbpositron',\n","                    'https://server.arcgisonline.com/arcgis/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}',\n","                    'Stamen Terrain']\n","        return mapArray\n"]},{"cell_type":"markdown","metadata":{"id":"lghJh3eIYJ8a"},"source":["# The main function\n","\n","Here we will generate all the maps. To so we first have to download the neccessary data and then process them. That will take 6 minutes running for the first time. If you execute the function again it will make use of the cached data files and execute in roughly 2 minutes.  \n","After execution you will find all maps (html files) as well as all data files in the /content/data directory. If you right-click on of the files you can download it to view it on your local system using any webbrowser.\n","The maps are also updated daily between 9:00 and 12:00 CET and available online on http://mb.cmbt.de/interactive-data-maps/"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295398,"status":"ok","timestamp":1655283229792,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"E2lCDJkmYJ8a","outputId":"bc27a0d3-475c-4847-e75a-f4993e1d3fb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on google colab. Using /content/data/ as the data directory\n","Starting at:  08:48:54\n","Downloading data (WorldCountriesMedRes.geojson), that might take some time...\n","Download finished.\n","Pandas loading the WHO CSV: 7.271298170089722s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in long_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in long_scalars\n"]},{"name":"stdout","output_type":"stream","text":["Pandas loading the OWID CSV: 4.833786725997925s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in double_scalars\n"]},{"name":"stdout","output_type":"stream","text":["using existing file: /content/data/2022-06-15-WHO-db.csv\n","Pandas loading the WHO CSV: 4.0359206199646s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in long_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in long_scalars\n"]},{"name":"stdout","output_type":"stream","text":["using existing file: /content/data/2022-06-15-WHO-db.csv\n","Pandas loading the WHO CSV: 4.037375450134277s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in long_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in long_scalars\n"]},{"name":"stdout","output_type":"stream","text":["using existing file: /content/data/2022-06-15-WHO-db.csv\n","Pandas loading the WHO CSV: 4.0293800830841064s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in long_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in long_scalars\n"]},{"name":"stdout","output_type":"stream","text":["using existing file: /content/data/2022-06-15-WHO-db.csv\n","Pandas loading the WHO CSV: 4.07571005821228s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in long_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in long_scalars\n"]},{"name":"stdout","output_type":"stream","text":["using existing file: /content/data/2022-06-15-OWID-db.csv\n","Pandas loading the OWID CSV: 4.825725793838501s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in double_scalars\n"]},{"name":"stdout","output_type":"stream","text":["using existing file: /content/data/2022-06-15-WHO-db.csv\n","Pandas loading the WHO CSV: 4.093368768692017s\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in long_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in long_scalars\n"]},{"name":"stdout","output_type":"stream","text":["Downloading data (RKI_Corona_Bundeslaender.geojson), that might take some time...\n","Download finished.\n","Downloading data (yy-mm-dd-RKIstates-db.csv), that might take some time...\n","Download finished.\n","Downloading data (RKI_Corona_Landkreise.geojson), that might take some time...\n","Download finished.\n","Downloading data (yy-mm-dd--RKIcounty-db.csv), that might take some time...\n","Error getting the data for 14729!\n","Download finished.\n","Downloading data (yy-mm-dd-RKI_COVID19-db.csv), that might take some time...\n","Download finished...\n","Filtering by age/gender groups...\n","Filtering done...\n","Building groups and sums...\n","Calculate percentages...\n","Building groups and sums...\n","      RS  Cases by age: 0-4  Cases by age: 5-14  Cases by age: 15-34  \\\n","0  01001                987                4055                10743   \n","1  01002               1911                8840                27196   \n","2  01003               2081                8753                20757   \n","3  01004                688                3421                 7238   \n","4  01051               1176                5285                11575   \n","\n","   Cases by age: 35-59  Cases by age: 60-79  Cases by age: 80+  \\\n","0                 8807                 2251                659   \n","1                20985                 5254               1831   \n","2                21524                 5615               1934   \n","3                 7621                 2008                783   \n","4                12980                 4193               1292   \n","\n","   Cases by gender: female  Cases by gender: male  \n","0                    14105                  13298  \n","1                    33841                  32045  \n","2                    30735                  28525  \n","3                    11020                  10589  \n","4                    18839                  17541  \n","Calculate percentages...\n","using existing file: /content/data//2022-06-15-RKI_COVID19_age_gender_per_county-db.csv\n","Finished at:  08:53:49\n"]}],"source":["import os\n","from collections import namedtuple\n","\n","def main():\n","    # the directory for temp. data as well as for the output\n","    # check if this code is running in jupyter, either local or in colab\n","    try:\n","        # this will generate an exception if not executed in jupyter\n","        if 'google.colab' in str(get_ipython()):    \n","            outputDir = '/content/data/'\n","            print('Running on google colab. Using ' + outputDir + ' as the data directory')\n","        else:\n","            # the absolute directory of this python file\n","            absDirectory = os.path.dirname(os.path.abspath(os.path.abspath('')))\n","            # the target filename\n","            outputDir = os.path.join(absDirectory, './data/')\n","            print('Running on local jupyter server. Using ' + outputDir + ' as the data directory')\n","    except:\n","        # we are running not in jupyter\n","        outputDir = '../data'\n","        print('Running locally. Using ' + outputDir + ' as the data directory')\n","\n","    # print the start time\n","    now = datetime.datetime.now()\n","    currentTime = now.strftime(\"%H:%M:%S\")\n","    print(\"Starting at: \", currentTime)\n","\n","    # an array of instances\n","    mapObjects = []\n","    \n","    # a tuple to select maps to be generated\n","    ToGenerate = namedtuple('ToGenerate', 'WHO RKIrest RKIage')\n","    # you may change this to select the maps to be created\n","    generate = ToGenerate(True, True, True)\n","    \n","    # the WHO maps\n","    if generate.WHO == True:\n","        # world\n","        mapObjects.append(CovidFoliumMapWHO(Continents.World, outputDir, 0))\n","        # africa\n","        mapObjects.append(CovidFoliumMapWHO(Continents.Africa, outputDir))\n","        # oceania\n","        mapObjects.append(CovidFoliumMapWHO(Continents.Oceania, outputDir))\n","        # america\n","        mapObjects.append(CovidFoliumMapWHO(Continents.America, outputDir))\n","        # asia\n","        mapObjects.append(CovidFoliumMapWHO(Continents.Asia, outputDir, 0))\n","        # europe\n","        mapObjects.append(CovidFoliumMapWHO(Continents.Europe, outputDir))\n","    \n","    # the RKI maps via the REST api\n","    if generate.RKIrest == True:\n","        # de states\n","        mapObjects.append(CovidFoliumMapDEstates(outputDir))\n","        # de counties\n","        mapObjects.append(CovidFoliumMapDEcounties(outputDir))\n","\n","    # the RKI data via the huge csv\n","    if generate.RKIage == True:\n","        # de states per age\n","        mapObjects.append(CovidFoliumMapDEageAndGenderStates(outputDir))\n","        # de counties per age\n","        mapObjects.append(CovidFoliumMapDEageAndGenderCounties(outputDir))\n","    \n","    # process the maps\n","    for mapObject in mapObjects:\n","        # check if it is initialized\n","        if mapObject.get_geo_df() is None:\n","            return\n","        # get the data directory\n","        dir = mapObject.get_data_directory()\n","        # select a basemap\n","        basemap = mapObject.get_nice_basemaps()[0]\n","        # build the default map\n","        if mapObject.get_default_map_options().mapAlias.find('age') > 0:\n","            # the maps contaning age based information\n","            map = mapObject.create_default_map(basemap, \n","                                               coloredAttribute = 'Percent cases by age: 0-14', \n","                                               coloredAttributeAlias = 'Percent cases age 0-14')\n","        else:\n","            # standard incidence based maps \n","            map = mapObject.create_default_map(basemap)\n","        # the filename\n","        filename = mapObject.get_default_map_options().mapAlias\n","        # save the map\n","        if map is not None:\n","            map.save(dir + '/' + filename + '.html')  \n","        if mapObject.get_default_map_options().mapAlias.find('World') > 0:\n","            # the filename\n","            filename = mapObject.get_default_map_options().mapAlias\n","\n","            # reset the bins as we want to generate them again automatically\n","            mapObject.get_default_map_options().bins = None\n","            # build another map of the world\n","            map = mapObject.create_default_map(basemap, 'PercentDeaths', 'Case Fatality Rate (CFR)')\n","            # save that as well\n","            if map is not None:\n","                map.save(dir + '/' + filename + 'CFR.html')  \n","\n","            # reset the bins as we want to generate them again automatically\n","            mapObject.get_default_map_options().bins = None\n","            # build another map of the world \n","            map = mapObject.create_default_map(basemap, 'CasesPerMillionPopulation', 'Cases per million population')\n","            # save that as well\n","            if map is not None:\n","                map.save(dir + '/' + filename + 'CasesPerMillionPopulation.html')   \n","    # print finished time\n","    now = datetime.datetime.now()\n","    currentTime = now.strftime(\"%H:%M:%S\")\n","    print(\"Finished at: \", currentTime)\n","    return\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655283229793,"user":{"displayName":"Martin Kersting","userId":"05848390804132089229"},"user_tz":-120},"id":"7ufJEHchYJ8a"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["t0p31pajYJ8D","_zJqa4_YYJ8K","zlx1lJ-XZO44","JqycnVSZYJ8U","M7F5ylrtYJ8V","yOlkwCX8nu5u","TciX3aPQNkxc"],"name":"CovidFoliumMapGenerator.ipynb","provenance":[]},"interpreter":{"hash":"f111841a7ea3927433a19963a3c52141477c3f1fe61c68f7bdbc1c0e5222f9a1"},"kernelspec":{"display_name":"Python 3.7.7 64-bit ('3.7.7')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
